--- gcc/libgcc/config.host	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libgcc/config.host	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -959,7 +959,9 @@ sh-*-elf* | sh[12346l]*-*-elf*)
 		libic_invalidate_array_4-100.a \
 		libic_invalidate_array_4-200.a \
 		libic_invalidate_array_4a.a \
-		libgcc-Os-4-200.a libgcc-4-300.a"
+		libic_invalidate_4a.a \
+		libic_invalidate.a \
+		libgcc-4-200.a libgcc-Os-4-200.a libgcc-4-300.a"
 	case ${host} in sh64*-*-*)
 		tmake_file="$tmake_file sh/t-sh64"
 		;;
@@ -968,11 +970,18 @@ sh-*-elf* | sh[12346l]*-*-elf*)
 	sh*-superh-elf)
 		tmake_file="$tmake_file sh/t-superh"
 		extra_parts="$extra_parts crt1-mmu.o gcrt1-mmu.o gcrt1.o"
+		extra_parts="$extra_parts trap-handler.o"
  		;;
  	esac
+	case ${target_thread_file} in
+	  generic)
+	    tmake_file="${tmake_file} sh/t-generic"
+	    ;;
+	esac
 	;;
 sh-*-linux* | sh[2346lbe]*-*-linux*)
 	tmake_file="${tmake_file} sh/t-sh t-slibgcc-libgcc sh/t-linux t-fdpbit"
+	extra_parts="$extra_parts libgcc-4-200.a libgcc-Os-4-200.a libgcc-4-300.a"
 	case ${host} in sh64*-*-linux*)
 		tmake_file="$tmake_file sh/t-sh64"
 		;;
--- gcc/libgcc/configure.ac	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libgcc/configure.ac	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -376,6 +376,7 @@ case $target_thread_file in
     single)	thread_header=gthr-single.h ;;
     tpf)	thread_header=config/s390/gthr-tpf.h ;;
     vxworks)	thread_header=config/gthr-vxworks.h ;;
+    generic)	thread_header=gthr-generic.h ;;
     win32)	thread_header=config/i386/gthr-win32.h ;;
 esac
 
--- gcc/libgcc/gthr-generic.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/gthr-generic.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,181 @@
+/* Generic threads supplementary implementation. */
+/* Compile this one with gcc.  */
+/* Copyright (C) 1997, 1999, 2000, 2002, 2006 Free Software Foundation, Inc.
+   Copyright (c) 2006, 2012 STMicroelectronics.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 3, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+Under Section 7 of GPL version 3, you are granted additional
+permissions described in the GCC Runtime Library Exception, version
+3.1, as published by the Free Software Foundation.
+
+You should have received a copy of the GNU General Public License and
+a copy of the GCC Runtime Library Exception along with this program;
+see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+<http://www.gnu.org/licenses/>.  */
+
+#define __GTHR_WEAK __attribute__ ((weak))
+
+#include "tconfig.h"
+#include "gthr.h"
+
+#ifndef __gthr_generic_h
+#error "Generic thread support package not supported"
+#endif
+
+/* These are stub functions.  When threading is available, a suitable set of definitions should be linked in.  */
+
+/* Return 1 if thread system is active, 0 if not.  */
+int
+__generic_gxx_active_p (void)
+{
+  return 0;
+}
+
+/* The following functions should return zero on success or the error
+   number.  If the operation is not supported, -1 is returned.
+
+   __generic_gxx_once
+   __generic_gxx_key_create
+   __generic_gxx_key_delete
+   __generic_gxx_setspecific
+   __generic_gxx_mutex_lock
+   __generic_gxx_mutex_trylock
+   __generic_gxx_mutex_unlock
+   __generic_gxx_recursive_mutex_lock
+   __generic_gxx_recursive_mutex_trylock
+   __generic_gxx_recursive_mutex_unlock  */
+
+/* FUNC is a function that should be called without parameters.
+   *ONCE has been initialized to __GTHREAD_ONCE_INIT and is otherwise only
+   used in calls to __generic_gxx_once with FUNC as the second parameter.
+   If __generic_gxx_once succeeds, FUNC will have been called exactly once
+   since the initialization of ONCE through any number of calls of
+   __generic_gxx_once with this pair of ONCE and FUNC values.  */
+int
+__generic_gxx_once (__gthread_once_t *once ATTRIBUTE_UNUSED,
+		    void (*func)(void) ATTRIBUTE_UNUSED)
+{
+  return -1;
+}
+
+/* Assign a key to *KEY that can be used in calls to
+   __generic_gxx_setspecific / __generic_gxx_getspecific.
+   If DTOR is nonzero, and at thread exit the value associated with the key
+   is nonzero, DTOR will be called at thread exit with the value associated
+   with the key as its only argument.  */
+int
+__generic_gxx_key_create (__gthread_key_t *key ATTRIBUTE_UNUSED,
+			  void (*dtor)(void *) ATTRIBUTE_UNUSED)
+{
+  return -1;
+}
+
+/* KEY is a key previously allocated by __generic_gxx_key_create.
+   Remove it from the set of keys known for this thread.  */
+int
+__generic_gxx_key_delete (__gthread_key_t key ATTRIBUTE_UNUSED)
+{
+  return -1;
+}
+
+/* Return thread-specific data associated with KEY.  */
+void *
+__generic_gxx_getspecific (__gthread_key_t key ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* Set thread-specific data associated with KEY to PTR.  */
+int
+__generic_gxx_setspecific (__gthread_key_t key ATTRIBUTE_UNUSED,
+		      const void *ptr ATTRIBUTE_UNUSED)
+{
+  return -1;
+}
+
+/* Initialize *MUTEX.  */
+void
+__generic_gxx_mutex_init_function (__gthread_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+}
+
+/* Destroy *MUTEX.  */
+int
+__generic_gxx_mutex_destroy (__gthread_mutex_t *__mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* Acquire a lock on *MUTEX.  The behaviour is undefined if a lock on *MUTEX
+   has already been acquired by the same thread.  */
+int
+__generic_gxx_mutex_lock (__gthread_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* Try to acquire a lock on *MUTEX.  If a lock on *MUTEX already exists,
+   return an error code.  */
+int
+__generic_gxx_mutex_trylock (__gthread_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* A lock on *MUTEX has previously been acquired with __generic_gxx_mutex_lock
+   or __generic_gxx_mutex_trylock.  Release the lock.  */
+int
+__generic_gxx_mutex_unlock (__gthread_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* Initialize *MUTEX.  */
+void
+__generic_gxx_recursive_mutex_init_function (__gthread_recursive_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+}
+
+/* Destroy *MUTEX.  */
+int
+__generic_gxx_recursive_mutex_destroy (__gthread_recursive_mutex_t *__mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* Acquire a lock on *MUTEX.  If a lock on *MUTEX has already been acquired by
+   the same thread, succeed.  */
+int
+__generic_gxx_recursive_mutex_lock (__gthread_recursive_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* Try to acquire a lock on *MUTEX.  If a lock on *MUTEX has already been
+   acquired by the same thread, succeed.  If any other lock on *MUTEX
+   already exists, return an error code.  */
+int
+__generic_gxx_recursive_mutex_trylock (__gthread_recursive_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* A lock on *MUTEX has previously been acquired with
+   __generic_gxx_recursive_mutex_lock or
+   __generic_gxx_recursive_mutex_trylock.  Release the lock.  */
+int
+__generic_gxx_recursive_mutex_unlock (__gthread_recursive_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
--- gcc/libgcc/gthr-generic.h	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/gthr-generic.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,385 @@
+/* Generic threads compatibility routines for libgcc2 and libobjc. */
+/* Compile this one with gcc.  */
+/* Copyright (C) 1997, 1999, 2000, 2002, 2006 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 3, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+Under Section 7 of GPL version 3, you are granted additional
+permissions described in the GCC Runtime Library Exception, version
+3.1, as published by the Free Software Foundation.
+
+You should have received a copy of the GNU General Public License and
+a copy of the GCC Runtime Library Exception along with this program;
+see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+<http://www.gnu.org/licenses/>.  */
+
+#ifndef __gthr_generic_h
+#define __gthr_generic_h
+
+#define __GTHREADS 1
+
+#define __GTHREAD_ONCE_INIT 0
+#define __GTHREAD_MUTEX_INIT_FUNCTION __gthread_mutex_init_function
+#define __GTHREAD_RECURSIVE_MUTEX_INIT_FUNCTION __gthread_recursive_mutex_init_function
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/* Avoid depedency on specific headers.
+   The general idea is that you dynamically allocate the required data
+   structures, and a void * is used to point to this dynamically allocated
+   data.  If your implementation can put all the required information in
+   the void * itself, that's fine, too, of course.
+   libstdc++ inherits from the mutex types, whcih is why they need to be
+   wrapped up as structs.  */
+typedef void *__gthread_key_t;
+typedef void *__gthread_once_t;
+typedef struct __gthread_mutex_s { void *__p; } __gthread_mutex_t;
+typedef struct __gthread_recursive_mutex_s { void *__p; } __gthread_recursive_mutex_t;
+
+/* We should always link with at least one definition, so we want strong
+   references.  The stub definitions are weak so that they can be overriden.  */
+#ifndef __GTHR_WEAK
+#define __GTHR_WEAK
+#endif
+
+extern int __generic_gxx_active_p (void) __GTHR_WEAK;
+
+extern int __generic_gxx_once (__gthread_once_t *, void (*)(void)) __GTHR_WEAK;
+
+extern int __generic_gxx_key_create (__gthread_key_t *,
+				     void (*)(void *)) __GTHR_WEAK;
+
+extern int __generic_gxx_key_delete (__gthread_key_t key) __GTHR_WEAK;
+
+extern void *__generic_gxx_getspecific (__gthread_key_t key) __GTHR_WEAK;
+
+extern int __generic_gxx_setspecific (__gthread_key_t, const void *) __GTHR_WEAK;
+
+extern void __generic_gxx_mutex_init_function (__gthread_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_mutex_destroy (__gthread_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_mutex_lock (__gthread_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_mutex_trylock (__gthread_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_mutex_unlock (__gthread_mutex_t *) __GTHR_WEAK;
+
+extern void __generic_gxx_recursive_mutex_init_function (__gthread_recursive_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_recursive_mutex_destroy (__gthread_recursive_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_recursive_mutex_lock (__gthread_recursive_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_recursive_mutex_trylock (__gthread_recursive_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_recursive_mutex_unlock (__gthread_recursive_mutex_t *) __GTHR_WEAK;
+
+#ifdef __cplusplus
+}
+#endif
+
+#ifdef _LIBOBJC
+
+extern int __generic_gxx_objc_init_thread_system (void) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_close_thread_system (void) __GTHR_WEAK;
+
+extern objc_thread_t __generic_gxx_objc_thread_detach (void (*)(void *), void *) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_thread_set_priority (int priority) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_thread_get_priority (void) __GTHR_WEAK;
+
+extern void __generic_gxx_objc_thread_yield (void) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_thread_exit (void) __GTHR_WEAK;
+
+extern objc_thread_t __generic_gxx_objc_thread_id (void) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_thread_set_data (void *value) __GTHR_WEAK;
+
+extern void *__generic_gxx_objc_thread_get_data (void) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_mutex_allocate (objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_mutex_deallocate (objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_mutex_lock (objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_mutex_trylock (objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_mutex_unlock (objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_condition_allocate (objc_condition_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_condition_deallocate (objc_condition_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_condition_wait (objc_condition_t, objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_condition_broadcast (objc_condition_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_condition_signal (objc_condition_t) __GTHR_WEAK;
+
+/* Backend initialization functions */
+
+/* Initialize the threads subsystem.  */
+static inline int
+__gthread_objc_init_thread_system (void)
+{
+  return __generic_gxx_objc_init_thread_system ();
+}
+
+/* Close the threads subsystem.  */
+static inline int
+__gthread_objc_close_thread_system (void)
+{
+  return __generic_gxx_objc_close_thread_system ();
+}
+
+/* Backend thread functions */
+
+/* Create a new thread of execution.  */
+static inline objc_thread_t
+__gthread_objc_thread_detach (void (* func)(void *), void * arg)
+{
+  return __generic_gxx_objc_thread_detach (func, arg);
+}
+
+/* Set the current thread's priority.  */
+static inline int
+__gthread_objc_thread_set_priority (int priority)
+{
+  return __generic_gxx_objc_thread_set_priority (priority);
+}
+
+/* Return the current thread's priority.  */
+static inline int
+__gthread_objc_thread_get_priority (void)
+{
+  return __generic_gxx_objc_thread_get_priority ();
+}
+
+/* Yield our process time to another thread.  */
+static inline void
+__gthread_objc_thread_yield (void)
+{
+  __generic_gxx_objc_thread_yield ();
+}
+
+/* Terminate the current thread.  */
+static inline int
+__gthread_objc_thread_exit (void)
+{
+  return __generic_gxx_objc_thread_exit ();
+}
+
+/* Returns an integer value which uniquely describes a thread.  */
+static inline objc_thread_t
+__gthread_objc_thread_id (void)
+{
+  return __generic_gxx_objc_thread_id ();
+}
+
+/* Sets the thread's local storage pointer.  */
+static inline int
+__gthread_objc_thread_set_data (void *value)
+{
+  return __generic_gxx_objc_thread_set_data (value);
+}
+
+/* Returns the thread's local storage pointer.  */
+static inline void *
+__gthread_objc_thread_get_data (void)
+{
+  return __generic_gxx_objc_thread_get_data ();
+}
+
+/* Backend mutex functions */
+
+/* Allocate a mutex.  */
+static inline int
+__gthread_objc_mutex_allocate (objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_mutex_allocate (mutex);
+}
+
+/* Deallocate a mutex.  */
+static inline int
+__gthread_objc_mutex_deallocate (objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_mutex_deallocate (mutex);
+}
+
+/* Grab a lock on a mutex.  */
+static inline int
+__gthread_objc_mutex_lock (objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_mutex_lock (mutex);
+}
+
+/* Try to grab a lock on a mutex.  */
+static inline int
+__gthread_objc_mutex_trylock (objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_mutex_trylock (mutex);
+}
+
+/* Unlock the mutex */
+static inline int
+__gthread_objc_mutex_unlock (objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_mutex_unlock (mutex);
+}
+
+/* Backend condition mutex functions */
+
+/* Allocate a condition.  */
+static inline int
+__gthread_objc_condition_allocate (objc_condition_t condition)
+{
+  return __generic_gxx_objc_condition_allocate (condition);
+}
+
+/* Deallocate a condition.  */
+static inline int
+__gthread_objc_condition_deallocate (objc_condition_t condition)
+{
+  return __generic_gxx_objc_condition_deallocate (condition);
+}
+
+/* Wait on the condition */
+static inline int
+__gthread_objc_condition_wait (objc_condition_t condition, objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_condition_wait (condition, mutex);
+}
+
+/* Wake up all threads waiting on this condition.  */
+static inline int
+__gthread_objc_condition_broadcast (objc_condition_t condition)
+{
+  return __generic_gxx_objc_condition_broadcast ( condition);
+}
+
+/* Wake up one thread waiting on this condition.  */
+static inline int
+__gthread_objc_condition_signal (objc_condition_t condition)
+{
+  return __generic_gxx_objc_condition_signal (condition);
+}
+
+#else /* !_LIBOBJC */
+
+static inline int
+__gthread_active_p (void)
+{
+  return __generic_gxx_active_p ();
+}
+
+static inline int
+__gthread_once (__gthread_once_t *once, void (*func)(void))
+{
+  return __generic_gxx_once (once, func);
+}
+
+static inline int
+__gthread_key_create (__gthread_key_t *key, void (*dtor)(void *))
+{
+  return __generic_gxx_key_create (key, dtor);
+}
+
+static inline int
+__gthread_key_delete (__gthread_key_t key)
+{
+  return __generic_gxx_key_delete (key);
+}
+
+static inline void *
+__gthread_getspecific (__gthread_key_t key)
+{
+  return __generic_gxx_getspecific (key);
+}
+
+static inline int
+__gthread_setspecific (__gthread_key_t key, const void *ptr)
+{
+  return __generic_gxx_setspecific (key, ptr);
+}
+
+static inline void
+__gthread_mutex_init_function (__gthread_mutex_t *mutex)
+{
+  __generic_gxx_mutex_init_function (mutex);
+}
+
+static inline int
+__gthread_mutex_destroy (__gthread_mutex_t * mutex)
+{
+  return __generic_gxx_mutex_destroy (mutex);
+}
+
+static inline int
+__gthread_mutex_lock (__gthread_mutex_t * mutex)
+{
+  return __generic_gxx_mutex_lock (mutex);
+}
+
+static inline int
+__gthread_mutex_trylock (__gthread_mutex_t * mutex)
+{
+  return __generic_gxx_mutex_trylock (mutex);
+}
+
+static inline int
+__gthread_mutex_unlock (__gthread_mutex_t * mutex)
+{
+  return __generic_gxx_mutex_unlock (mutex);
+}
+
+static inline void
+__gthread_recursive_mutex_init_function (__gthread_recursive_mutex_t *mutex)
+{
+  __generic_gxx_recursive_mutex_init_function (mutex);
+}
+
+static inline int
+__gthread_recursive_mutex_destroy (__gthread_recursive_mutex_t * mutex)
+{
+  return __generic_gxx_recursive_mutex_destroy (mutex);
+}
+
+static inline int
+__gthread_recursive_mutex_lock (__gthread_recursive_mutex_t * mutex)
+{
+  return __generic_gxx_recursive_mutex_lock (mutex);
+}
+
+static inline int
+__gthread_recursive_mutex_trylock (__gthread_recursive_mutex_t * mutex)
+{
+  return __generic_gxx_recursive_mutex_trylock (mutex);
+}
+
+static inline int
+__gthread_recursive_mutex_unlock (__gthread_recursive_mutex_t * mutex)
+{
+  return __generic_gxx_recursive_mutex_unlock (mutex);
+}
+
+#endif /* _LIBOBJC */
+
+#endif /* __gthr_generic_h */
--- gcc/libgcc/longlong.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libgcc/longlong.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1048,12 +1048,36 @@ UDItype __umulsidi3 (USItype, USItype);
 /* This is the same algorithm as __udiv_qrnnd_c.  */
 #define UDIV_NEEDS_NORMALIZATION 1
 
+#ifdef	DB_ST40300_BUG_WORKAROUND
 #define udiv_qrnnd(q, r, n1, n0, d) \
   do {									\
     extern UWtype __udiv_qrnnd_16 (UWtype, UWtype)			\
                         __attribute__ ((visibility ("hidden")));	\
     /* r0: rn r1: qn */ /* r0: n1 r4: n0 r5: d r6: d1 */ /* r2: __m */	\
     __asm__ (								\
+"	.align	2\n"                                                    \
+"       mov%M4 %4,r5\n"						        \
+"	swap.w %3,r4\n"							\
+"	swap.w r5,r6\n"							\
+"       nop\n"                                                          \
+"	jsr @%5\n"							\
+"	shll16 r6\n"							\
+"	swap.w r4,r4\n"							\
+"       nop\n"                                                          \
+"	jsr @%5\n"							\
+"	swap.w r1,%0\n"							\
+"	or r1,%0"							\
+	: "=r" (q), "=&z" (r)						\
+	: "1" (n1), "r" (n0), "rm" (d), "r" (&__udiv_qrnnd_16)		\
+        : "r1", "r2", "r4", "r5", "r6", "pr", "t");			\
+  } while (0)
+#else
+#define udiv_qrnnd(q, r, n1, n0, d) \
+  do {									\
+    extern UWtype __udiv_qrnnd_16 (UWtype, UWtype)			\
+                        __attribute__ ((visibility ("hidden")));	\
+    /* r0: rn r1: qn */ /* r0: n1 r4: n0 r5: d r6: d1 */ /* r2: __m */	\
+    __asm__ (								\
 	"mov%M4 %4,r5\n"						\
 "	swap.w %3,r4\n"							\
 "	swap.w r5,r6\n"							\
@@ -1067,6 +1091,7 @@ UDItype __umulsidi3 (USItype, USItype);
 	: "1" (n1), "r" (n0), "rm" (d), "r" (&__udiv_qrnnd_16)		\
 	: "r1", "r2", "r4", "r5", "r6", "pr", "t");			\
   } while (0)
+#endif
 
 #define UDIV_TIME 80
 
--- gcc/libgcc/config/sh/IEEE-754/divsf3.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/divsf3.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,404 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!divides two single precision floating point 
+
+! Author: Aanchal Khanna
+
+! Arguments: Dividend is in r4, divisor in r5
+! Result: r0
+
+! r4 and r5 are referred as op1 and op2 resp.
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align	5
+	.global	GLOBAL (divsf3)
+	FUNC (GLOBAL (divsf3))
+
+GLOBAL (divsf3):
+	mov.l	.L_mask_sign,r1
+	mov	r4,r3
+
+	xor	r5,r3
+	shll	r4
+
+	shlr	r4
+	mov.l	.L_inf,r2
+
+	and	r3,r1		!r1=resultant sign
+	mov	r4,r6
+
+	shll	r5
+	mov	#0,r0		
+
+	shlr	r5
+	and	r2,r6
+
+	cmp/eq	r2,r6
+	mov	r5,r7
+
+	and     r2,r7
+	bt	.L_op1_inv
+
+	cmp/eq	r2,r7
+	mov	#-23,r3
+
+	bt	.L_op2_inv
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLR23 (r6)
+	SHLR23 (r7)
+#else
+	shld	r3,r6
+	shld	r3,r7
+#endif
+
+	cmp/eq	r0,r4
+
+	bt	.L_op1_zero		!dividend=0
+	cmp/eq	r0,r6
+
+	mov.l   .L_imp_bit,r3
+	bt	.L_norm_op1		!normalize dividend
+.L_chk_op2:
+	cmp/eq	r0,r5
+	bt	.L_op2_zero		!divisor=0
+
+	cmp/eq	r0,r7
+	bt	.L_norm_op2		!normalize divisor
+
+.L_div1:
+	sub	r7,r6
+	add	#127,r6			!r6=resultant exponent
+
+	mov     r3,r7
+	mov.l	.L_mask_mant,r3
+
+	and	r3,r4
+	!chk exponent for overflow
+        mov.l   .L_255,r2
+
+	and     r3,r5
+	or	r7,r4
+
+	cmp/ge  r2,r6
+	or	r7,r5
+
+	bt	.L_return_inf
+	mov	r0,r2
+
+	cmp/eq  r4,r5
+	bf      .L_den_one
+
+	cmp/ge	r6,r0
+	!numerator=denominator, quotient=1, remainder=0
+	mov	r7,r2			
+
+	mov     r0,r4
+	!chk exponent for underflow
+	bt	.L_underflow
+        bra     .L_pack
+        nop
+
+.L_den_one:
+	!denominator=1, result=numerator
+
+	cmp/eq  r7,r5
+        bf      .L_divide
+
+	!chk exponent for underflow
+	cmp/ge  r6,r0
+        mov    r4,r2           
+
+        SL(bt,    .L_underflow,
+	 mov	r0,r4)
+	bra     .L_pack
+	nop
+
+.L_divide:
+	!dividing the mantissas r4<-dividend, r5<-divisor
+
+	cmp/hi	r4,r5
+	bf	.L_loop
+
+	shll	r4		! if mantissa(op1)< mantissa(op2)
+	add     #-1,r6		! shift left the numerator and decrease the exponent.
+
+.L_loop:
+	!division loop
+
+	cmp/ge	r5,r4
+	bf	.L_skip
+
+	or	r7,r2
+	sub	r5,r4
+
+.L_skip:
+	shlr	r7
+	shll	r4
+
+	cmp/eq	r0,r7
+	bf	.L_loop
+
+	!chk the exponent for underflow
+	cmp/ge  r6,r0
+	bt      .L_underflow
+	
+	!apply rounding
+	cmp/gt	r5,r4
+	bt	.L_round1
+
+	cmp/eq	r4,r5
+	bt	.L_round2
+
+.L_pack:
+	!pack the result, r1=sign, r2=quotient, r6=exponent
+
+	mov    #23,r4
+	and     r3,r2
+
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLL23 (r6)
+#else
+	shld	r4,r6
+#endif
+	or	r2,r1
+
+	or	r6,r1
+	mov	r1,r0	
+	
+	rts
+	nop
+
+.L_round1:
+	!Apply proper rounding
+
+        bra     .L_pack
+        add     #1,r2
+
+.L_round2:
+	!Apply proper rounding
+
+        mov.l   .L_comp_1,r5
+        bra     .L_pack
+        and     r5,r2
+
+.L_op1_inv:
+	!chk if op1 is Inf or NaN
+
+	mov.l	.L_mask_mant,r3
+	mov	r4,r6
+
+	and	r3,r6
+	cmp/hi	r0,r6
+
+	bt	.L_ret_op1
+	cmp/eq	r2,r7
+
+	SL(bf,	.L_ret_op1,
+	 mov	r1,r0)
+
+	rts
+	mov	#-1,r0	! 0/0, return NaN
+	
+.L_op2_inv:
+	!chk if op2 is Inf or NaN
+
+	mov.l	.L_mask_mant,r3
+	mov	r5,r7
+	
+	and	r3,r7
+	cmp/hi	r0,r7
+
+	bt	.L_ret_op2
+	mov	r1,r0
+	
+	rts
+	nop
+
+.L_op1_zero:
+	!op1 is zero. If op2 is zero, return NaN, else return zero
+
+	cmp/eq	r0,r5
+
+	bf	.L_ret_op1	
+
+	rts
+	mov	#-1,r0
+
+.L_op2_zero:
+	!B is zero,return Inf
+
+	rts
+	or	r2,r0
+
+.L_return_inf:
+	mov.l	.L_inf,r0
+	
+	rts
+	or	r1,r0
+
+.L_norm_op1:
+	!normalize dividend
+
+	shll	r4
+	tst	r2,r4
+	
+	add     #-1,r6
+	bt	.L_norm_op1
+
+	bra	.L_chk_op2
+	add	#1,r6
+
+.L_norm_op2:
+	!normalize divisor
+
+	shll	r5
+	tst	r2,r5
+	
+	add	#-1,r7
+	bt	.L_norm_op2
+
+	bra	.L_div1
+	add	#1,r7
+
+.L_underflow:
+	!denormalize the result
+
+	add	#1,r6
+	mov	#-24,r7
+
+	cmp/gt	r6,r7
+	mov	r2,r5
+
+	bt	.L_return_zero
+	add     #-1,r6
+
+	mov	#32,r3
+	neg	r6,r7
+
+	add	#1,r7
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r6,r2
+#else
+	cmp/ge	r0,r6
+	bf	.L_mov_right
+
+.L_mov_left:
+	cmp/eq	r0,r6
+	bt	.L_out
+
+	shll	r2
+	bra	.L_mov_left
+	add	#-1,r6
+
+.L_mov_right:
+	cmp/eq	r0,r6
+	bt	.L_out
+
+	add	#1,r6
+	bra	.L_mov_right
+	shlr	r2
+	
+.L_out:
+#endif
+	sub	r7,r3
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r3,r5
+#else
+	cmp/ge	r0,r3
+	bf	.L_mov_right_1
+
+.L_mov_left_1:
+	shll	r5
+	add	#-1,r3
+
+	cmp/eq	r0,r3
+	bf	.L_mov_left_1
+
+	bt	.L_out_1
+
+.L_mov_right_1:
+	cmp/eq	r0,r3
+	bt	.L_out_1
+
+	add	#1,r3
+	bra	.L_mov_right_1
+	shlr	r5
+
+.L_out_1:
+#endif
+	shlr	r2
+	addc	r0,r2
+
+	cmp/eq	r4,r0		!r4 contains the remainder
+	mov      r2,r0
+
+	mov.l	.L_mask_sign,r7
+	bf	.L_return
+
+	mov.l   .L_comp_1,r2
+	cmp/eq	r7,r5
+
+	bf	.L_return
+	and	r2,r0
+
+.L_return:
+	rts
+	or     r1,r0
+	
+.L_ret_op1:
+	rts
+	or	r4,r0
+
+.L_ret_op2:
+	rts
+	or	r5,r0
+
+.L_return_zero:
+	rts
+	or	r1,r0
+
+
+
+	.align	2
+.L_inf:
+	.long	0x7f800000
+.L_mask_sign:
+	.long	0x80000000
+.L_mask_mant:
+	.long	0x007fffff
+.L_imp_bit:
+	.long	0x00800000
+.L_comp_1:
+	.long	0xfffffffe
+.L_255:
+	.long	255
+
+ENDFUNC (GLOBAL (divsf3))
--- gcc/libgcc/config/sh/IEEE-754/m3/divsf3.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/m3/divsf3.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,375 @@
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! divsf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+
+! long 0th..3rd significant byte
+#ifdef __LITTLE_ENDIAN__
+#define L0SB	3
+#define L1SB	2
+#define L2SB	1
+#define L3SB	0
+#else
+#define L0SB	0
+#define L1SB	1
+#define L2SB	2
+#define L3SB	3
+#endif
+
+! clobbered: r0,r1,r2,r3,r6,r7,T (and for sh.md's purposes PR)
+!
+! Note: When the divisor is larger than the divident, we have to adjust the
+! exponent down by one.  We do this automatically when subtracting the entire
+! exponent/fraction bitstring as an integer, by means of the borrow from
+! bit 23 to bit 24.
+! Note: non-denormal rounding of a division result cannot cause fraction
+! overflow / exponent change. (r4 > r5 : fraction must stay in (2..1] interval;
+! r4 < r5: having an extra bit of precision available, even the smallest
+! possible difference of the result from one is rounded in all rounding modes
+! to a fraction smaller than one.)
+! sh4-200: 59 cycles
+! sh4-300: 44 cycles
+! tab indent: exponent / sign computations
+! tab+space indent: fraction computation
+FUNC(GLOBAL(divsf3))
+	.global GLOBAL(divsf3)
+	.balign	4
+GLOBAL(divsf3):
+	mov.l	LOCAL(x7f800000),r3
+	mov	#1,r2
+	mov	r4,r6
+	 shll8	 r6
+	mov	r5,r7
+	 shll8	 r7
+	rotr	r2
+	tst	r3,r4
+	or	r2,r6
+	bt/s	LOCAL(denorm_arg0)
+	or	r2,r7
+	tst	r3,r5
+	bt	LOCAL(denorm_arg1)
+	mov.l	LOCAL(x3f000000),r3	! bias minus explict leading 1
+	 div0u
+LOCAL(denorm_done):
+!	mov.l	LOCAL(x3f000000),r3	! bias minus explict leading 1
+	cmp/hs	r7,r6
+	mov.l	r8,@-r15
+	mov.l	LOCAL(xff800000),r8
+	bt		LOCAL(no_norm)
+	add  	r8,r3
+LOCAL(no_norm):
+	shlr	 r6
+	 div1	 r7,r6
+	 bt	 0f
+	 div1	r7,r6
+0:	mov.l	r9,@-r15
+	 div1	 r7,r6
+	mov	r4,r1
+	and	r8,r1
+	add	r1,r3
+	 div1	 r7,r6
+	and	r5,r8
+	sub	r8,r3	! result sign/exponent minus 1 if no overflow/underflow
+	 div1	 r7,r6
+	or	r3,r2
+	 div1	 r7,r6
+	mov.w	LOCAL(xff00),r9
+	 div1	 r7,r6
+	mov.l	r2,@-r15 ! L0SB is 0xff iff denorm / infinity exp is computed
+	 div1	 r7,r6
+	mov.w	LOCAL(m23),r2
+	 div1	 r7,r6
+	mov	r4,r0
+	 div1	 r7,r6
+	 extu.b	 r6,r1
+	 and	 r9,r6
+	 swap.w	 r1,r1	! first 8 bits of result fraction in bit 23..16
+	 div1	 r7,r6
+	shld	r2,r0
+	 div1	 r7,r6
+	mov.b	r0,@(L3SB,r15)	! 0xff iff divident was infinity / nan
+	 div1	 r7,r6
+	mov	r5,r0
+	 div1	 r7,r6
+	shld	r2,r0
+	 div1	 r7,r6
+	mov.b	r0,@(L2SB,r15)	! 0xff iff divisor was infinity / nan
+	 div1	 r7,r6
+	mov	r4,r0
+	 div1	 r7,r6
+	mov.w	LOCAL(m31),r2
+	 div1	 r7,r6
+	 extu.b	 r6,r8	! second 8 bits of result fraction in bit 7..0
+	 and	 r9,r6
+	mov.l	LOCAL(xff800000),r9
+	 div1	 r7,r6
+	xor	r5,r0	! msb := correct result sign
+	 div1	 r7,r6
+	xor	r3,r0	! xor with sign of result sign/exponent word
+	 div1	 r7,r6
+	shad	r2,r0
+	 div1	 r7,r6
+	mov.b	r0,@(L1SB,r15)	! 0xff	iff exponent over/underflows
+	and	r9,r3	! isolate sign / exponent
+	 div1	 r7,r6
+	 swap.b	r8,r0	! second 8 bits of result fraction in bit 15..8
+	 div1	 r7,r6
+	 or	r1,r0	! first 16 bits of result fraction in bit 23..8
+	 div1	 r7,r6
+	mov.w	LOCAL(m1),r9
+	 div1	 r7,r6
+	mov.l	@r15+,r8 ! load encoding of unusal exponent conditions
+	 extu.b	 r6,r1
+	 or	 r1,r0	! 24 bit result fraction with explicit leading 1
+	addc	r3,r0	! add in exponent / sign
+	cmp/str	r9,r8
+	! (no stall *here* for SH4-100 / SH4-200)
+	bt/s	LOCAL(inf_nan_denorm_zero)
+	mov.l	@r15+,r9
+	rts
+	mov.l	@r15+,r8
+
+/* The exponennt adjustment for denormal numbers is done by leaving an
+   adjusted value in r3; r4/r5 are not changed.  */
+	.balign	4
+LOCAL(denorm_arg0):
+	mov.w	LOCAL(xff00),r1
+	sub	r2,r6	! 0x800000000 : remove implict 1
+	tst	r6,r6
+	bt	LOCAL(div_zero)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	mov	r6,r0
+	shld	r0,r6
+	tst	r3,r5
+	mov.l	LOCAL(x3f800000),r3	! bias - 1 + 1
+	mov	#23,r1
+	shld	r1,r0
+	bt/s	LOCAL(denorm_arg1_2)
+	sub	r0,r3
+	bra	LOCAL(denorm_done)
+	 div0u
+
+LOCAL(denorm_arg1):
+	mov.l	LOCAL(x3f000000),r3	! bias - 1
+LOCAL(denorm_arg1_2):
+	sub	r2,r7	! 0x800000000 : remove implict 1
+	mov.w	LOCAL(xff00),r1
+	tst	r7,r7
+	bt	LOCAL(div_by_zero)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	mov	r7,r0
+	shld	r0,r7
+	add	#-1,r0
+	mov	#23,r1
+	shld	r1,r0
+	add	r0,r3
+	bra	LOCAL(denorm_done)
+	 div0u
+
+	.balign	4
+LOCAL(inf_nan_denorm_zero):
+! r0 has the rounded result, r6 has the non-rounded lowest bits & rest.
+! the bit just below the LSB of r6 is available as ~Q
+
+! Alternative way to get at ~Q:
+! if rounding took place, ~Q must be set.
+! if the rest appears to be zero, ~Q must be set.
+! if the rest appears to be nonzero, but rounding didn't take place,
+! ~Q must be clear;  the apparent rest will then require adjusting to test if 
+! the actual rest is nonzero.
+	mov	r0,r2
+	not	r8,r0
+	tst	#0xff,r0
+	shlr8	r0
+	mov.l	@r15+,r8
+	bt/s	LOCAL(div_inf_or_nan)
+	tst	#0xff,r0
+	mov	r4,r0
+	bt	LOCAL(div_by_inf_or_nan)
+	add	r0,r0
+	mov	r5,r1
+	add	r1,r1
+	cmp/hi	r1,r0
+	mov	r6,r0
+	bt	LOCAL(overflow)
+	sub	r2,r0
+	exts.b	r0,r0	! -1 if rounding took place
+	shlr8	r6	! isolate div1-mangled rest
+	addc	r2,r0	! generate carry if rounding took place
+	shlr8	r7
+	mov.l	LOCAL(xffffff),r1
+	sub	r3,r0	! pre-rounding fraction
+	bt	0f ! going directly to denorm_sticky would cause mispredicts
+	tst	r6,r6	! rest can only be zero if lost bit was set
+0:	add	r7,r6	! (T ? corrupt : reconstruct) actual rest
+	bt	0f
+	and r1,r6
+	cmp/pl	r6
+0:	mov.w	LOCAL(m24),r1
+	addc	r0,r0	! put in sticky bit
+	add	#-1,r3
+	mov.l	LOCAL(x80000000),r6
+	add	r3,r3
+	mov	r0,r2
+	shad	r1,r3	! exponent ; s32.0
+	!
+	cmp/pl	r3
+	bt/s	LOCAL(zero_nan)	! return zero
+	clrt
+	shld	r3,r0
+	add	#31,r3
+	cmp/pl	r3
+	shld	r3,r2
+	bf	LOCAL(zero_nan)	! return zero
+	rotl	r2
+	cmp/hi	r6,r2
+	mov	#0,r7
+	addc	r7,r0
+	shll	r0
+	div0s	r4,r5
+	rts
+	rotcr	r0
+	
+! ????
+! undo normal rounding (lowest bits still in r6). then do denormal rounding.
+	
+LOCAL(overflow):
+	mov.l	LOCAL(xff000000),r0
+	div0s	r4,r5
+	rts
+	rotcr	r0
+	
+LOCAL(div_inf_or_nan):
+	mov	r4,r0
+	bra	LOCAL(nan_if_t)
+	add	r0,r0
+	
+LOCAL(div_by_inf_or_nan):
+	mov.l	LOCAL(xff000000),r1
+	mov	#0,r0
+	mov	r5,r2
+	add	r2,r2
+	bra	LOCAL(nan_if_t)
+	cmp/hi	r1,r2
+
+
+
+! still need to check for divide by zero or divide by nan
+! r3: 0x7f800000
+	.balign	4
+LOCAL(div_zero):
+	mov	r5,r1
+	add	r1,r1
+	tst	r1,r1	! 0 / 0 -> nan
+	bt	LOCAL(nan)
+	add	r3,r3
+	cmp/hi	r3,r1	! 0 / nan -> nan (but 0 / inf -> 0)
+LOCAL(zero_nan):
+	mov	#0,r0
+LOCAL(nan_if_t):
+	bf	0f:
+LOCAL(nan):
+	mov	#-1,r0
+0:	div0s	r4,r5	! compute sign
+	rts
+	rotcr	r0	! insert sign
+
+LOCAL(div_by_zero):
+	mov.l	LOCAL(xff000000),r0
+	mov	r4,r2
+	add	r2,r2
+	bra	LOCAL(nan_if_t)
+	cmp/hi	r0,r2
+	
+	.balign	4
+LOCAL(clz):
+	mov.l	r8,@-r15
+	extu.w	r0,r8
+	mov.l	r9,@-r15
+	cmp/eq	r0,r8
+	bt/s	0f
+	mov	#32,r9
+	shlr16	r0
+	extu.w	r0,r8
+	add	#-16,r9
+0:	tst	r1,r8	! 0xff00
+	mov.l	LOCAL(c_clz_tab),r0
+	bt	0f
+	shlr8	r8
+0:	bt	0f
+	add	#-8,r9
+0:
+#ifdef	__PIC__
+	add	r0,r8
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r8),r8
+	mov	r9,r0
+	mov.l	@r15+,r9
+	!
+	!
+	!
+	sub	r8,r0
+	mov.l	@r15+,r8
+	rts
+	lds.l	@r15+,pr
+
+!	We encode even some words as pc-relative that would fit as immediate
+!	in the instruction in order to avoid some pipeline stalls on
+!	SH4-100 / SH4-200.
+LOCAL(m23):	.word -23
+LOCAL(m24):	.word -24
+LOCAL(m31):	.word -31
+LOCAL(xff01):	.word 0xff01
+	.balign	4
+LOCAL(xff000000): .long 0xff000000
+#ifdef __LITTLE_ENDIAN__
+LOCAL(xff00):	.word 0xff00
+LOCAL(m1):	.word -1
+#else
+LOCAL(m1):	.word -1
+LOCAL(xff00):	.word 0xff00
+#endif
+LOCAL(xffffff): .long 0xffffff
+LOCAL(x7f800000): .long 0x7f800000
+LOCAL(x3f000000): .long 0x3f000000
+LOCAL(x3f800000): .long 0x3f800000
+LOCAL(xff800000): .long 0xff800000
+LOCAL(x40000000): .long 0x40000000
+LOCAL(x80000000): .long 0x80000000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(divsf3))
--- gcc/libgcc/config/sh/IEEE-754/m3/divdf3.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/m3/divdf3.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,668 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! divdf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke joern.rennecke@st.com
+
+/* y = 1/x  ; x (- [1,2)
+   y0 = 1.5 - x/2 - tab[(1-x)*64] = y + d ; abs(d)/y <= 0x1.0c/256
+
+   y1 = y0 - ((y0) * x - 1) * y0  =  y-x*d^2
+   y2 = y1 - ((y1) * x - 1) * y1 =~= y-x^3*d^4
+
+   z0 = y2*a ;  a1 = a - z0*x /# 32 * 64 -> 64 bit #/
+   z1 = y2*a1 (round to nearest odd 0.5 ulp);
+   a2 = a1 - z1*x /# 32 * 64 -> 64 bit #/
+
+   z = a/x = z0 + z1 - 0.5 ulp + (a2 > 0) * ulp
+
+   Unless stated otherwise, multiplies can be done in 32 * 32 bit or less
+   with suitable scaling and/or top truncation.
+   We use a slightly modified algorithm here that checks if the lower
+   bits in z1 are sufficient to determine the outcome of rounding - in that
+   case a2 is not computed.
+   -z1 is computed in units of 1/128 ulp, with an error in the range
+   -0x3.e/128 .. +0 ulp.
+   Thus, after adding three, the result can be safely rounded for normal
+   numbers if any of the bits 5..2 is set, or if the highest guard bit
+   (bit 6 if y <1, otherwise bit 7) is set.
+   (Because of the way truncation works, we would be fine for an open
+    error interval of (-4/128..+1/128) ulp )
+   For denormal numbers, the rounding point lies higher, but it would be
+   quite cumbersome to calculate where exactly; it is sufficient if any
+   of the bits 7..3 is set.
+   x truncated to 20 bits is sufficient to calculate y0 or even y1.
+   Table entries are adjusted by about +128 to use full signed byte range.
+   This adjustment has been perturbed slightly to allow cse with the
+   shift count constant -26.
+   The threshold point for the shift adjust before rounding is found by
+   comparing the fractions, which is exact, unlike the top bit of y2.
+   Therefore, the top bit of y2 becomes slightly random after the adjustment
+   shift, but that's OK because this can happen only at the boundaries of
+   the interval, and the biasing of the error means that it can in fact happen
+   only at the bottom end.  And there, the carry propagation will make sure
+   that in the end we will have in effect an implicit 1 (or two whem rounding
+   up...)  */
+/* If an exact result exists, it can have no more bits than the divident.
+   Hence, we don't need to bother with the round-to-even tie breaker
+   unless the result is denormalized.  */
+/* 64 cycles through main path for sh4-300 (about 93.7% of normalized numbers),
+   82 for the path for rounding tie-breaking for normalized numbers
+   (including one branch mispredict).
+   Some cycles might be saved by more careful register allocation.  */
+
+#define x_h r12
+#define yn  r3
+
+FUNC(GLOBAL(divdf3))
+ .global GLOBAL(divdf3)
+
+/* Adjust arg0 now, too.  We still have to come back to denorm_arg1_done,
+   since we heven't done any of the work yet that we do till the denorm_arg0
+   entry point.  We know that neither of the arguments is inf/nan, but
+   arg0 might be zero.  Check for that first to avoid having to establish an
+   rts return address.  */
+LOCAL(both_denorm):
+	mov.l	r9,@-r15
+	mov	DBL0H,r1
+	mov.l	r0,@-r15
+	shll2	r1
+	mov.w LOCAL(both_denorm_cleanup_off),r9
+	or	DBL0L,r1
+	tst	r1,r1
+	mov	DBL0H,r0
+	bf/s	LOCAL(zero_denorm_arg0_1)
+	shll2	r0
+	mov.l	@(4,r15),r9
+	add	#8,r15
+	bra	LOCAL(ret_inf_nan_0)
+	mov	r1,DBLRH
+
+LOCAL(both_denorm_cleanup):
+	mov.l	@r15+,r0
+	!
+	mov.l	@r15+,r9
+ mov	#12,r3
+ mov.l	LOCAL(xfffe2006),r1	! yn := (-1. << 17) + (0x80 << 6) ; shift #-26
+	bra	LOCAL(denorm_arg1_done)
+	!
+	add	r0,DBL0H
+
+/* Denorm handling leaves the incoming denorm argument with an exponent of +1
+   (implicit 1).  To leave the result exponent unaltered, the other
+   argument's exponent is adjusted by the the shift count.  */
+
+	.balign 4
+LOCAL(arg0_tiny):
+	bsr	LOCAL(clz)
+	mov	DBL0L,r0
+	shll	DBL0H
+	add	#1,r0
+	mov	DBL0L,DBL0H
+	shld	r0,DBL0H
+	rotcr	DBL0H
+	
+	cmp/pl r0
+	bf/s	LOCAL(a0t_dpt_neg)
+	tst	DBL0L,DBL0L	/* Check for divide of zero.  */
+	add	#-32,r0
+	shld	r0,DBL0L
+	bf/s	LOCAL(adjust_arg1_exp)
+	add	#63,r0
+	bra 	LOCAL(return_0)
+	nop
+
+LOCAL(a0t_dpt_neg):
+	add 	#31,r0
+	bf/s	LOCAL(adjust_arg1_exp)
+	shld	r0,DBL0L
+		  
+LOCAL(return_0): /* Return 0 with appropriate sign.  */
+	mov.l	@r15+,r10
+	mov	#0,DBLRH
+	mov.l	@r15+,r9
+	bra	LOCAL(ret_inf_nan_0)
+	mov.l	@r15+,r8
+
+	.balign 4
+LOCAL(arg1_tiny):
+	bsr	LOCAL(clz)
+	mov	DBL1L,r0
+	shll	DBL1H
+	add	#1,r0
+	mov	DBL1L,DBL1H
+	shld	r0,DBL1H
+	rotcr	DBL1H
+
+	cmp/pl r0
+	bf/s	LOCAL(a1t_dpt_neg)
+	tst	DBL1L,DBL1L	/* Check for divide by zero.  */
+	add	#-32,r0
+	shld	r0,DBL1L
+	bf/s	LOCAL(adjust_arg0_exp)
+	add	#63,r0
+	bra 	LOCAL(a1t_end)
+	nop
+
+LOCAL(a1t_dpt_neg):
+	add 	#31,r0
+	bf/s	LOCAL(adjust_arg0_exp)
+	shld	r0,DBL1L
+	
+LOCAL(a1t_end):		  		  
+	mov	DBL0H,r0
+	add	r0,r0
+	tst	r0,r0	! 0 / 0 ?
+	mov	#-1,DBLRH
+	bf	LOCAL(return_inf)
+	!
+	tst   DBL0L,DBL0L
+	bf	LOCAL(return_inf)
+	bt	LOCAL(ret_inf_nan_0)
+	!
+
+	.balign 4
+LOCAL(zero_denorm_arg1):
+	not	DBL0H,r3
+	mov	DBL1H,r0
+	tst	r2,r3
+	shll2	r0
+	bt	LOCAL(early_inf_nan_arg0)
+	tst	r0,r0
+	mov.w	LOCAL(xff00),r12
+	bt/s	LOCAL(arg1_tiny)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	shlr2	r0
+	!
+	shll	DBL1H
+	mov	DBL1L,r3
+	shld	r0,DBL1H
+	shld	r0,DBL1L
+	rotcr	DBL1H
+	add	#-32,r0
+	shld	r0,r3
+	add	#32,r0
+	or	r3,DBL1H
+LOCAL(adjust_arg0_exp):
+	tst	r2,DBL0H
+	mov	#20,r3
+	shld	r3,r0
+	bt	LOCAL(both_denorm)
+	add	DBL0H,r0
+	div0s	r0,DBL0H	! Check for obvious overflow.  */
+	not	r0,r3		! Check for more subtle overflow - lest
+	bt	LOCAL(return_inf)
+	mov	r0,DBL0H
+	tst	r2,r3		! we mistake it for NaN later
+	mov	#12,r3
+	bf	LOCAL(denorm_arg1_done)
+LOCAL(return_inf): /* Return infinity with appropriate sign.  */
+	mov	#20,r3
+	mov	#-2,DBLRH
+	bra	LOCAL(ret_inf_nan_0)
+	shad	r3,DBLRH
+
+/* inf/n -> inf; inf/0 -> inf; inf/inf -> nan; inf/nan->nan  nan/x -> nan */
+LOCAL(inf_nan_arg0):
+	mov.l	@r15+,r10
+	mov.l	@r15+,r9
+	mov.l	@r15+,r8
+LOCAL(early_inf_nan_arg0):
+	not	DBL1H,r3
+	mov	DBL0H,DBLRH
+	tst	r2,r3	! both inf/nan?
+	add	DBLRH,DBLRH
+	bf	LOCAL(ret_inf_nan_0)
+	mov	#-1,DBLRH
+LOCAL(ret_inf_nan_0):
+	mov	#0,DBLRL
+	mov.l	@r15+,r12
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+	
+/* Already handled: inf/x, nan/x .  Thus: x/inf -> 0; x/nan -> nan */
+	.balign	4
+LOCAL(inf_nan_arg1):
+	mov	DBL1H,r2
+	mov	#12,r1
+	shld	r1,r2
+	mov.l	@r15+,r10
+	mov	#0,DBLRL
+	mov.l	@r15+,r9
+	or	DBL1L,r2
+	mov.l	@r15+,r8
+	cmp/hi	DBLRL,r2
+	mov.l	@r15+,r12
+	subc	DBLRH,DBLRH
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+	
+	.balign 4
+LOCAL(zero_denorm_arg0):
+	mov.w	LOCAL(denorm_arg0_done_off),r9
+	not	DBL1H,r1
+	mov	DBL0H,r0
+	tst	r2,r1
+	shll2	r0
+	bt	LOCAL(inf_nan_arg1)
+LOCAL(zero_denorm_arg0_1):
+	tst	r0,r0
+	mov.w	LOCAL(xff00),r12
+	bt/s	LOCAL(arg0_tiny)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	shlr2	r0
+	shll	DBL0H
+	mov	DBL0L,r12
+	shld	r0,DBL0H
+	shld	r0,DBL0L
+	rotcr	DBL0H
+	add	#-32,r0
+	shld	r0,r12
+	add	#32,r0
+	or	r12,DBL0H
+LOCAL(adjust_arg1_exp):
+	mov	#20,r12
+	shld	r12,r0
+	add	DBL1H,r0
+	div0s	r0,DBL1H	! Check for obvious underflow.  */
+	not	r0,r12		! Check for more subtle underflow - lest
+	bt	LOCAL(return_0)
+	mov	r0,DBL1H
+	tst	r2,r12		! we mistake it for NaN later
+	bt	LOCAL(return_0)
+	!
+	braf	r9
+	mov	#13,r0
+LOCAL(zero_denorm_arg1_dispatch):
+
+LOCAL(xff00):	.word 0xff00
+LOCAL(denorm_arg0_done_off):
+	.word LOCAL(denorm_arg0_done)-LOCAL(zero_denorm_arg1_dispatch)
+LOCAL(both_denorm_cleanup_off):
+	.word LOCAL(both_denorm_cleanup)-LOCAL(zero_denorm_arg1_dispatch)
+
+ .balign	8
+GLOBAL(divdf3):
+ mov.l	LOCAL(x7ff00000),r2
+ mov	#12,r3
+ mov.l	LOCAL(xfffe2006),r1	! yn := (-1. << 17) + (0x80 << 6) ; shift #-26
+ tst	r2,DBL1H
+ mov.l	r12,@-r15
+ bt	LOCAL(zero_denorm_arg1)
+
+LOCAL(denorm_arg1_done):
+ mov	DBL1H,x_h	! x_h live in r12
+ shld	r3,x_h	! x - 1 ; u0.20
+ mov	x_h,yn
+ mova	LOCAL(ytab),r0
+ mov.l	r8,@-r15
+ shld	r1,yn	! x-1 ; u26.6
+ mov.b	@(r0,yn),yn
+ mov	#6,r0
+ mov.l	r9,@-r15
+ mov	x_h,r8
+ mov.l	r10,@-r15
+ shlr16	x_h	! x - 1; u16.16	! x/2 - 0.5 ; u15.17
+ add	x_h,r1	! SH4-200 single-issues this insn
+ shld	r0,yn
+ sub	r1,yn	! yn := y0 ; u15.17
+ mov	DBL1L,r1
+ mov	#-20,r10
+ mul.l	yn,x_h	! r12 dead
+ swap.w	yn,r9
+ shld	r10,r1
+ sts	macl,r0	! y0 * (x-1) - n ; u-1.32
+ add	r9,r0	! y0 * x - 1     ; s-1.32
+ tst	r2,DBL0H
+ dmuls.l r0,yn
+ mov.w	LOCAL(d13),r0
+ or	r1,r8	! x  - 1; u0.32
+ add	yn,yn	! yn = y0 ; u14.18
+ bt	LOCAL(zero_denorm_arg0)
+
+LOCAL(denorm_arg0_done):
+ sts	mach,r1	!      d0 ; s14.18
+ sub	r1,yn	! yn = y1 ; u14.18 ; <= 0x3fffc
+ mov	DBL0L,r12
+ shld	r0,yn	! yn = y1 ; u1.31 ; <= 0x7fff8000
+ mov.w	LOCAL(d12),r9
+ dmulu.l yn,r8
+ shld	r10,r12
+ mov	yn,r0
+ mov	DBL0H,r8
+ add	yn,yn	! yn = y1 ; u0.32 ; <= 0xffff0000
+ sts	mach,r1	! y1 * (x-1); u1.31
+ add	r0,r1	! y1 * x    ; u1.31
+ dmulu.l yn,r1
+ not	DBL0H,r10
+ shld	r9,r8
+ tst	r2,r10
+ or	r8,r12	! a - 1; u0.32
+ bt	LOCAL(inf_nan_arg0)
+ sts	mach,r1	! d1+yn; u1.31
+ sett		! adjust y2 so that it can be interpreted as s1.31
+ not	DBL1H,r10
+ subc	r1,yn	! yn := y2 ; u1.31 ; can be 0x7fffffff
+ mov.l	LOCAL(x001fffff),r9
+ dmulu.l yn,r12
+ tst	r2,r10
+ or	DBL1H,r2
+ bt	LOCAL(inf_nan_arg1)
+ mov.l	r11,@-r15
+ sts	mach,r12	! y2*(a-1) ; u1.31
+ add	yn,r12		! z0       ; u1.31
+ dmulu.l r12,DBL1L
+ mov.l	LOCAL(x40000000),DBLRH ! bias + 1
+ and	r9,r2		! x ; u12.20
+ cmp/hi	DBL0L,DBL1L
+ sts	macl,r8
+ mov	#-24,r11
+ sts	mach,r9 	! r9:r8 := z0 * DBL1L; u-19.64
+ 
+ subc	DBL1H,DBLRH
+ mul.l	r12,r2  	! (r9+macl):r8 == z0*x; u-19.64
+ shll	r8
+ add	DBL0H,DBLRH	! result sign/exponent + 1
+ mov	r8,r10
+ sts	macl,DBLRL
+ add	DBLRL,r9
+ rotcl	r9		! r9:r8 := z*x; u-20.63
+ shld	r11,r10
+
+! mov.l	LOCAL(xfff00000),DBLRL
+! mov DBL1H,r11
+! and DBLRL,r11
+! subc r11,DBLRH
+! add DBL0H,DBLRH
+		  
+ mov.l	LOCAL(x7fe00000),DBLRL
+ sub	DBL0L,r9	! r9:r8 := -a ; u-20.63
+ cmp/pz	r9		! In corner cases this shift can loose ..
+ shll8	r9		!  .. the sign, so check it first.
+ mov.l	LOCAL(x00200000),r11
+ !mov.l	LOCAL(x00100000),r11
+ or	r10,r9	! -a1 ; s-28.32
+ mov.l	LOCAL(x00100000),r10
+ dmulu.l r9,yn	! sign for r9 is in T
+ xor	DBL0H,DBL1H	! calculate expected sign & bit20
+ mov.w	LOCAL(d120),DBL0H ! to test bits 6..4
+ xor	DBLRH,DBL1H
+ !
+ sts	mach,DBL0L	! -z1 ; s-27.32
+ bt 0f
+ sub	yn,DBL0L	! multiply adjust for -a1 negative; r3 dies here
+0:tst	r10,DBL1H		! set T if a >= x
+ mov.l LOCAL(xfff00000),r3
+ bt	0f
+ add	DBL0L,DBL0L	! z1 ; s-27.32 / s-28.32
+0:bt 0f
+ add	r12,r12	! z0 ; u1.31 / u0.31
+0:add	#6-64,DBL0L
+ and	r3,DBLRH	! isolate sign / exponent
+ tst	DBL0H,DBL0L
+ bf/s	LOCAL(exact)	! make the hot path taken for best branch prediction
+ cmp/pz	DBL1H
+
+! Unless we follow the next branch, we need to test which way the rounding
+! should go.
+! For normal numbers, we know that the result is not exact, so the sign
+! of the rest will be conclusive.
+! We generate a number that looks safely rounded so that denorm handling
+! can safely test the number twice.
+! r10:r8 == 0 will indicate if the number was exact, which can happen
+! when we come here for denormals to check a number that is close or
+! equal to a result in whole ulps.
+ bf	LOCAL(ret_denorm_inf)	! denorm or infinity, DBLRH has inverted sign
+ add	#64,DBL0L
+LOCAL(find_adjust): tst	r10,DBL1H ! set T if a >= x
+ mov	#-2,r10
+ addc	r10,r10
+ mov	DBL0L,DBLRL	! z1 ; s-27.32 / s-28.32 ; lower 4 bits unsafe.
+ shad	r10,DBLRL	! tentatively rounded z1 ; s-24.32
+ shll8	r8		! r9:r8 := -a1 ; s-28.64
+ clrt
+ dmuls.l DBLRL,DBL1L	! DBLRL signed, DBL1L unsigned
+ mov	r8,r10
+ shll16	r8		! r8  := lowpart  of -a1 ; s-44.48
+ xtrct	r9,r10		! r10 := highpart of -a1 ; s-44.48
+ !
+ sts	macl,r3
+ subc	r3,r8
+ sts	mach,r3
+ subc	r3,r10
+ cmp/pz	DBL1L
+ mul.l	DBLRL,r2
+ bt	0f
+ sub	DBLRL,r10	! adjust for signed/unsigned multiply
+0: mov.l	LOCAL(x7fe00000),DBLRL
+ mov	#-26,r2
+ sts	macl,r9
+ sub	r9,r10		! r10:r8 := -a2
+ add	#-64+16,DBL0L	! the denorm code negates this adj. for exact results
+ shld	r2,r10		! convert sign into adjustment in the range 32..63
+ sub	r10,DBL0L
+ cmp/pz	DBL1H
+
+ .balign 4
+LOCAL(exact):
+ bf	LOCAL(ret_denorm_inf)	! denorm or infinity, DBLRH has inverted sign
+ tst	DBLRL,DBLRH
+ bt	LOCAL(ret_denorm_inf)	! denorm, DBLRH has correct sign
+ mov	#-7,DBL1H
+ cmp/pz	DBL0L		! T is sign extension of z1
+ not	DBL0L,DBLRL
+ subc	r11,DBLRH	! calculate sign / exponent minus implicit 1 minus T
+ mov.l	@r15+,r11
+ mov.l	@r15+,r10
+ shad	DBL1H,DBLRL
+ mov.l	@r15+,r9
+ mov	#-11,DBL1H
+ mov	r12,r8		! z0 contributes to DBLRH and DBLRL
+ shld	DBL1H,r12
+ mov	#21,DBL1H
+ clrt
+ shld	DBL1H,r8
+ addc	r8,DBLRL
+ mov.l	@r15+,r8
+ addc	r12,DBLRH
+ rts
+ mov.l	@r15+,r12
+
+!	sign in DBLRH ^ DBL1H
+! If the last 7 bits are in the range 64..64+7, we might have an exact
+! value in the preceding bits - or we might not. For denorms, we need to
+! find out.
+! if r10:r8 is zero, we just have found out that there is an exact value.
+	.balign	4
+LOCAL(ret_denorm_inf):
+	mov	DBLRH,r3
+	add	r3,r3
+	div0s	DBL1H,r3
+!	mov	#248,DBLRL
+	mov	#120,DBLRL
+	bt	LOCAL(ret_inf_late)
+	add	#64,DBL0L
+	tst	DBLRL,DBL0L
+	mov	#-21,DBLRL
+	bt	LOCAL(find_adjust)
+	or	r10,r8
+!	add	#-64,DBL0L
+	tst	r8,r8		! check if find_adjust found an exact value.
+	shad	DBLRL,r3
+	bf	0f
+	add	#-16,DBL0L	! if yes, cancel adjustment
+0:	mov	#-8,DBLRL	! remove the three lowest (inexact) bits
+	and	DBLRL,DBL0L
+	add	#-2-11,r3	! shift count for denorm generation
+	neg 	DBL0L,DBL0L
+	mov	#-28,r2
+	mov	DBL0L,DBLRL
+	mov.l	@r15+,r11
+	mov.l	@r15+,r10
+	shll2	DBLRL
+	mov.l	@r15+,r9
+	shad	r2,DBL0L
+	mov.l	@r15+,r8
+	mov	#-31,r2
+	cmp/ge	r2,r3
+	shll2	DBLRL
+	bt/s	0f
+	add	DBL0L,r12	! fraction in r12:DBLRL ; u1.63
+	mov	#0,r2
+	cmp/hi r2,DBLRL
+	mov	#-33,r2
+	add	#31,r3
+	mov	r12,DBLRL
+	rotcl	DBLRL		! put in sticky bit
+	movt	r12
+	cmp/ge	r3,r2
+	bt	LOCAL(test1)
+0:	div0s	DBL1H,DBLRH	! calculate sign
+	mov	r12,DBLRH
+	shld	r3,DBLRH
+	mov	DBLRL,r2
+	shld	r3,DBLRL
+	add	#32,r3
+	add	DBLRH,DBLRH
+	mov.l	LOCAL(x80000000),DBL1H
+	shld	r3,r12
+	rotcr	DBLRH		! combine sign with highpart
+	add	#-1,r3
+	shld	r3,r2
+	mov	#0,r3
+	rotl	r2
+	cmp/hi	DBL1H,r2
+	addc	r12,DBLRL
+	mov.l	@r15+,r12
+	rts
+	addc	r3,DBLRH
+
+LOCAL(test1):
+	cmp/ge	r2,r3
+	bf/s	LOCAL(return_0_late)
+	div0s	DBL1H,DBLRH
+	mov #0,DBLRH
+	mov	DBLRL,r2
+	mov #0,DBLRL
+	rotcr	DBLRH		! combine sign with highpart
+	mov	#0,r3
+	rotl	r2
+	cmp/hi	r3,r2
+	addc	r3,DBLRL
+	mov.l	@r15+,r12
+	rts
+	addc	r3,DBLRH
+		  
+		  
+LOCAL(ret_inf_late):
+	mov.l	@r15+,r11
+	mov.l	@r15+,r10
+	mov	DBLRH,DBL0H
+	mov.l	@r15+,r9
+	bra	LOCAL(return_inf)
+	mov.l	@r15+,r8
+
+LOCAL(return_0_late):
+	div0s	DBLRH,DBL1H
+	mov.l	@r15+,r12
+	mov	#0,DBLRH
+	mov	#0,DBLRL
+	rts
+	rotcr	DBLRH
+
+	
+	
+	.balign	4
+LOCAL(clz):
+	mov.l	r8,@-r15
+	extu.w	r0,r8
+	mov.l	r9,@-r15
+	cmp/eq	r0,r8
+	bt/s	0f
+	mov	#21,r9
+	shlr16	r0
+	extu.w	r0,r8
+	add	#-16,r9
+0:	tst	r12,r8	! 0xff00
+	mov.l	LOCAL(c_clz_tab),r0
+	bt	0f
+	shlr8	r8
+0:	bt	0f
+	add	#-8,r9
+0:
+#ifdef	__PIC__
+	add	r0,r8
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r8),r8
+	mov	r9,r0
+	mov.l	@r15+,r9
+	!
+	!
+	!
+	sub	r8,r0
+	mov.l	@r15+,r8
+	rts
+	lds.l	@r15+,pr
+
+!	We encode even some words as pc-relative that would fit as immediate
+!	in the instruction in order to avoid some pipeline stalls on
+!	SH4-100 / SH4-200.
+LOCAL(d1):	.word 1
+LOCAL(d12):	.word 12
+LOCAL(d13):	.word 13
+LOCAL(d120):	.word 120
+
+	.balign 4
+LOCAL(x7ff00000): .long 0x7ff00000
+LOCAL(xfffe2006): .long 0xfffe2006
+LOCAL(x001fffff): .long 0x001fffff
+LOCAL(x40000000): .long 0x40000000
+LOCAL(x7fe00000): .long 0x7fe00000
+LOCAL(x00100000): .long 0x00100000
+LOCAL(x00200000): .long 0x00200000
+LOCAL(xfff00000): .long 0xfff00000
+LOCAL(x80000000): .long 0x80000000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+LOCAL(ytab):
+        .byte   120, 105,  91,  78,  66,  54,  43,  33
+        .byte    24,  15,   8,   0,  -5, -12, -17, -22
+        .byte   -27, -31, -34, -37, -40, -42, -44, -45
+        .byte   -46, -46, -47, -46, -46, -45, -44, -42
+        .byte   -41, -39, -36, -34, -31, -28, -24, -20
+        .byte   -17, -12,  -8,  -4,   0,   5,  10,  16
+        .byte    21,  27,  33,  39,  45,  52,  58,  65
+        .byte    72,  79,  86,  93, 101, 109, 116, 124
+ENDFUNC(GLOBAL(divdf3))
--- gcc/libgcc/config/sh/IEEE-754/m3/floatunssisf.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/m3/floatunssisf.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,94 @@
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! floatsisf for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+
+FUNC(GLOBAL(floatunsisf))
+	.global GLOBAL(floatunsisf)
+	.balign	4
+GLOBAL(floatunsisf):
+	mov.l	LOCAL(c_clz_tab),r0
+	extu.w	r4,r1
+	mov.w	LOCAL(xff00),r3
+	cmp/eq	r4,r1
+	mov	#24,r2
+	bt	0f
+	mov	r4,r1
+	shlr16	r1
+	add	#-16,r2
+0:	tst	r3,r1	! 0xff00
+	bt	0f
+	shlr8	r1
+0:	bt	0f
+	add	#-8,r2
+0:
+#ifdef	__PIC__
+	add	r0,r1
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r1),r1
+	mov	r4,r0
+	mov.l	LOCAL(x4a800000),r3	! bias + 23 - implicit 1
+	tst	r4,r4
+	bt	LOCAL(ret0)
+	!
+	sub	r1,r2
+	mov.l	LOCAL(x80000000),r1
+	shld	r2,r0
+	cmp/pz	r2
+	add	r3,r0
+	bt	LOCAL(noround)
+	add	#31,r2
+	shld	r2,r4
+	rotl	r4
+	add	#-31,r2
+	cmp/hi	r1,r4
+	mov	#0,r3
+	addc	r3,r0
+LOCAL(noround):
+	mov	#23,r1
+	shld	r1,r2
+	rts
+	sub	r2,r0
+LOCAL(ret0):
+	rts
+	nop
+
+LOCAL(xff00):	.word 0xff00
+	.balign	4
+LOCAL(x4a800000): .long 0x4a800000
+LOCAL(x80000000): .long 0x80000000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(floatunsisf))
--- gcc/libgcc/config/sh/IEEE-754/m3/floatunssidf.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/m3/floatunssidf.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,96 @@
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! floatunssidf for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+
+FUNC(GLOBAL(floatunsidf))
+	.global GLOBAL(floatunsidf)
+	.balign	4
+GLOBAL(floatunsidf):
+	mov.l	LOCAL(c_clz_tab),r0
+	extu.w	r4,r1
+	mov.w	LOCAL(0xff00),r3
+	cmp/eq	r4,r1
+	mov	#21,r2
+	bt	0f
+	mov	r4,r1
+	shlr16	r1
+	add	#-16,r2
+0:	tst	r3,r1	! 0xff00
+	bt	0f
+	shlr8	r1
+0:	bt	0f
+	add	#-8,r2
+0:
+#ifdef	__PIC__
+	add	r0,r1
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r1),r5
+	mov	r4,DBLRL
+	mov.l	LOCAL(x41200000),r3	! bias + 20 - implicit 1
+	tst	r4,r4
+	mov	r4,DBLRH
+	bt	LOCAL(ret0)
+	sub	r5,r2
+	mov	r2,r5
+	shld	r2,DBLRH
+	cmp/pz	r2
+	add	r3,DBLRH
+	add	#32,r2
+	shld	r2,DBLRL
+	bf	0f
+	mov.w	LOCAL(d0),DBLRL
+0:	mov	#20,r2
+	shld	r2,r5
+	rts
+	sub	r5,DBLRH
+LOCAL(ret0):
+	mov	r4,DBLRL
+	rts
+	mov	r4,DBLRH
+
+LOCAL(0xff00):	.word  0xff00
+	.balign	4
+LOCAL(x41200000):
+#ifdef __LITTLE_ENDIAN__
+LOCAL(d0):	  .word 0
+		  .word 0x4120
+#else
+		  .word 0x4120
+LOCAL(d0):	  .word 0
+#endif
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(floatunsidf))
--- gcc/libgcc/config/sh/IEEE-754/m3/divdf3-rt.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/m3/divdf3-rt.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,519 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! divdf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke joern.rennecke@st.com
+
+/* This version is not quite finshed, since I've found that I can
+   get better average performance with a slightly altered algorithm.
+   Still, if you want a version for hard real time, this version here might
+   be a good starting point, since it has effectively no conditional
+   branches in the path that deals with normal numbers
+   (branches with zero offset are effectively conditional execution),
+   and thus it has a uniform execution time in this path.  */
+
+/* y = 1/x  ; x (- [1,2)
+   y0 = 1.5 - x/2 - tab[(1-x)*64] = y + d ; abs(d)/y <= 0x1.0c/256
+
+   y1 = y0 - ((y0) * x - 1) * y0  =  y-x*d^2
+   y2 = y1 - ((y1) * x - 1) * y1 =~= y-x^3*d^4
+
+   z0 = y2*a ;  a1 = a - z0*x /# 32 * 64 -> 64 bit #/
+   z1 = y2*a1 (round to nearest odd 0.5 ulp);
+   a2 = a1 - z1*x /# 32 * 64 -> 64 bit #/
+
+   z = a/x = z0 + z1 - 0.5 ulp + (a2 > 0) * ulp
+
+   Unless stated otherwise, multiplies can be done in 32 * 32 bit or less
+   with suitable scaling and/or top truncation.
+   x truncated to 20 bits is sufficient to calculate y0 or even y1.
+   Table entries are adjusted by about +128 to use full signed byte range.
+   This adjustment has been perturbed slightly to allow cse with the
+   shift count constant -26.
+   The threshold point for the shift adjust before rounding is found by
+   comparing the fractions, which is exact, unlike the top bit of y2.
+   Therefore, the top bit of y2 becomes slightly random after the adjustment
+   shift, but that's OK because this can happen only at the boundaries of
+   the interval, and the baising of the error means that it can in fact happen
+   only at the bottom end.  And there, the carry propagation will make sure
+   that in the end we will have in effect an implicit 1 (or two whem rounding
+   up...)  */
+/* If an exact result exists, it can have no more bits than the divident.
+   Hence, we don't need to bother with the round-to-even tie breaker
+   unless the result is denormalized.  */
+/* 70 cycles through main path for sh4-300 .  Some cycles might be
+   saved by more careful register allocation.
+   122 cycles for sh4-200.  If execution time for sh4-200 is of concern,
+   a specially scheduled version makes sense.  */
+
+#define x_h r12
+#define yn  r3
+
+FUNC(GLOBAL(divdf3))
+ .global GLOBAL(divdf3)
+
+/* Adjust arg0 now, too.  We still have to come back to denorm_arg1_done,
+   since we heven't done any of the work yet that we do till the denorm_arg0
+   entry point.  We know that neither of the arguments is inf/nan, but
+   arg0 might be zero.  Check for that first to avoid having to establish an
+   rts return address.  */
+LOCAL(both_denorm):
+	mov.l	r9,@-r15
+	mov	DBL0H,r1
+	mov.l	r0,@-r15
+	shll2	r1
+	mov.w LOCAL(both_denorm_cleanup_off),r9
+	or	DBL0L,r1
+	tst	r1,r1
+	mov	DBL0H,r0
+	bf/s	LOCAL(zero_denorm_arg0_1)
+	shll2	r0
+	mov.l	@(4,r15),r9
+	add	#8,r15
+	bra	LOCAL(ret_inf_nan_0)
+	mov	r1,DBLRH
+
+LOCAL(both_denorm_cleanup):
+	mov.l	@r15+,r0
+	!
+	mov.l	@r15+,r9
+ mov	#12,r3
+ mov.l	LOCAL(xfffe2006),r1	! yn := (-1. << 17) + (0x80 << 6) ; shift #-26
+	bra	LOCAL(denorm_arg1_done)
+	!
+	add	r0,DBL0H
+
+/* Denorm handling leaves the incoming denorm argument with an exponent of +1
+   (implicit 1).  To leave the result exponent unaltered, the other
+   argument's exponent is adjusted by the the shift count.  */
+
+	.balign 4
+LOCAL(arg0_tiny):
+	bsr	LOCAL(clz)
+	mov	DBL0L,r0
+	shll	DBL0H
+	add	#1,r0
+	mov	DBL0L,DBL0H
+	shld	r0,DBL0H
+	rotcr	DBL0H
+	tst	DBL0L,DBL0L	/* Check for divide of zero.  */
+	add	#-33,r0
+	shld	r0,DBL0L
+	bf/s	LOCAL(adjust_arg1_exp)
+	add	#64,r0
+LOCAL(return_0): /* Return 0 with appropriate sign.  */
+	mov.l	@r15+,r10
+	mov	#0,DBLRH
+	mov.l	@r15+,r9
+	bra	LOCAL(ret_inf_nan_0)
+	mov.l	@r15+,r8
+
+	.balign 4
+LOCAL(arg1_tiny):
+	bsr	LOCAL(clz)
+	mov	DBL1L,r0
+	shll	DBL1H
+	add	#1,r0
+	mov	DBL1L,DBL1H
+	shld	r0,DBL1H
+	rotcr	DBL1H
+	tst	DBL1L,DBL1L	/* Check for divide by zero.  */
+	add	#-33,r0
+	shld	r0,DBL1L
+	bf/s	LOCAL(adjust_arg0_exp)
+	add	#64,r0
+	mov	DBL0H,r0
+	add	r0,r0
+	tst	r0,r0	! 0 / 0 ?
+	mov	#-1,DBLRH
+	bf	LOCAL(return_inf)
+	!
+	bt	LOCAL(ret_inf_nan_0)
+	!
+
+	.balign 4
+LOCAL(zero_denorm_arg1):
+	not	DBL0H,r3
+	mov	DBL1H,r0
+	tst	r2,r3
+	shll2	r0
+	bt	LOCAL(early_inf_nan_arg0)
+	tst	r0,r0
+	mov.w	LOCAL(xff00),r12
+	bt/s	LOCAL(arg1_tiny)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	shlr2	r0
+	!
+	shll	DBL1H
+	mov	DBL1L,r3
+	shld	r0,DBL1H
+	shld	r0,DBL1L
+	rotcr	DBL1H
+	add	#-32,r0
+	shld	r0,r3
+	add	#32,r0
+	or	r3,DBL1H
+LOCAL(adjust_arg0_exp):
+	tst	r2,DBL0H
+	mov	#20,r3
+	shld	r3,r0
+	bt	LOCAL(both_denorm)
+	add	DBL0H,r0
+	div0s	r0,DBL0H	! Check for obvious overflow.  */
+	not	r0,r3		! Check for more subtle overflow - lest
+	bt	LOCAL(return_inf)
+	mov	r0,DBL0H
+	tst	r2,r3		! we mistake it for NaN later
+	mov	#12,r3
+	bf	LOCAL(denorm_arg1_done)
+LOCAL(return_inf): /* Return infinity with appropriate sign.  */
+	mov	#20,r3
+	mov	#-2,DBLRH
+	bra	LOCAL(ret_inf_nan_0)
+	shad	r3,DBLRH
+
+/* inf/n -> inf; inf/0 -> inf; inf/inf -> nan; inf/nan->nan  nan/x -> nan */
+LOCAL(inf_nan_arg0):
+	mov.l	@r15+,r10
+	mov.l	@r15+,r9
+	mov.l	@r15+,r8
+LOCAL(early_inf_nan_arg0):
+	not	DBL1H,r3
+	mov	DBL0H,DBLRH
+	tst	r2,r3	! both inf/nan?
+	add	DBLRH,DBLRH
+	bf	LOCAL(ret_inf_nan_0)
+	mov	#-1,DBLRH
+LOCAL(ret_inf_nan_0):
+	mov	#0,DBLRL
+	mov.l	@r15+,r12
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+	
+/* Already handled: inf/x, nan/x .  Thus: x/inf -> 0; x/nan -> nan */
+	.balign	4
+LOCAL(inf_nan_arg1):
+	mov	DBL1H,r2
+	mov	#12,r1
+	shld	r1,r2
+	mov.l	@r15+,r10
+	mov	#0,DBLRL
+	mov.l	@r15+,r9
+	or	DBL1L,r2
+	mov.l	@r15+,r8
+	cmp/hi	DBLRL,r2
+	mov.l	@r15+,r12
+	subc	DBLRH,DBLRH
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+	
+	.balign 4
+LOCAL(zero_denorm_arg0):
+	mov.w	LOCAL(denorm_arg0_done_off),r9
+	not	DBL1H,r1
+	mov	DBL0H,r0
+	tst	r2,r1
+	shll2	r0
+	bt	LOCAL(inf_nan_arg1)
+LOCAL(zero_denorm_arg0_1):
+	tst	r0,r0
+	mov.w	LOCAL(xff00),r12
+	bt/s	LOCAL(arg0_tiny)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	shlr2	r0
+	shll	DBL0H
+	mov	DBL0L,r12
+	shld	r0,DBL0H
+	shld	r0,DBL0L
+	rotcr	DBL0H
+	add	#-32,r0
+	shld	r0,r12
+	add	#32,r0
+	or	r12,DBL0H
+LOCAL(adjust_arg1_exp):
+	mov	#20,r12
+	shld	r12,r0
+	add	DBL1H,r0
+	div0s	r0,DBL1H	! Check for obvious underflow.  */
+	not	r0,r12		! Check for more subtle underflow - lest
+	bt	LOCAL(return_0)
+	mov	r0,DBL1H
+	tst	r2,r12		! we mistake it for NaN later
+	bt	LOCAL(return_0)
+	!
+	braf	r9
+	mov	#13,r0
+LOCAL(zero_denorm_arg1_dispatch):
+
+LOCAL(xff00):	.word 0xff00
+LOCAL(denorm_arg0_done_off):
+	.word LOCAL(denorm_arg0_done)-LOCAL(zero_denorm_arg1_dispatch)
+LOCAL(both_denorm_cleanup_off):
+	.word LOCAL(both_denorm_cleanup)-LOCAL(zero_denorm_arg1_dispatch)
+
+ .balign	8
+GLOBAL(divdf3):
+ mov.l	LOCAL(x7ff00000),r2
+ mov	#12,r3
+ mov.l	LOCAL(xfffe2006),r1	! yn := (-1. << 17) + (0x80 << 6) ; shift #-26
+ tst	r2,DBL1H
+ mov.l	r12,@-r15
+ bt	LOCAL(zero_denorm_arg1)
+
+LOCAL(denorm_arg1_done):
+ mov	DBL1H,x_h	! x_h live in r12
+ shld	r3,x_h	! x - 1 ; u0.20
+ mov	x_h,yn
+ mova	LOCAL(ytab),r0
+ mov.l	r8,@-r15
+ shld	r1,yn	! x-1 ; u26.6
+ mov.b	@(r0,yn),yn
+ mov	#6,r0
+ mov.l	r9,@-r15
+ mov	x_h,r8
+ mov.l	r10,@-r15
+ shlr16	x_h	! x - 1; u16.16	! x/2 - 0.5 ; u15.17
+ add	x_h,r1	! SH4-200 single-issues this insn
+ shld	r0,yn
+ sub	r1,yn	! yn := y0 ; u15.17
+ mov	DBL1L,r1
+ mov	#-20,r10
+ mul.l	yn,x_h	! r12 dead
+ swap.w	yn,r9
+ shld	r10,r1
+ sts	macl,r0	! y0 * (x-1) - n ; u-1.32
+ add	r9,r0	! y0 * x - 1     ; s-1.32
+ tst	r2,DBL0H
+ dmuls.l r0,yn
+ mov.w	LOCAL(d13),r0
+ or	r1,r8	! x  - 1; u0.32
+ add	yn,yn	! yn = y0 ; u14.18
+ bt	LOCAL(zero_denorm_arg0)
+
+LOCAL(denorm_arg0_done):	! This label must stay aligned.
+ sts	mach,r1	!      d0 ; s14.18
+ sub	r1,yn	! yn = y1 ; u14.18 ; <= 0x3fffc
+ mov	DBL0L,r12
+ shld	r0,yn	! yn = y1 ; u1.31 ; <= 0x7fff8000
+ mov.w	LOCAL(d12),r9
+ dmulu.l yn,r8
+ shld	r10,r12
+ mov	yn,r0
+ mov	DBL0H,r8
+ add	yn,yn	! yn = y1 ; u0.32 ; <= 0xffff0000
+ sts	mach,r1	! y1 * (x-1); u1.31
+ add	r0,r1	! y1 * x    ; u1.31
+ dmulu.l yn,r1
+ not	DBL0H,r10
+ shld	r9,r8
+ tst	r2,r10
+ or	r8,r12	! a - 1; u0.32
+ bt	LOCAL(inf_nan_arg0)
+ sts	mach,r1	! d1+yn; u1.31
+ sett		! adjust y2 so that it can be interpreted as s1.31
+ not	DBL1H,r10
+ subc	r1,yn	! yn := y2 ; u1.31 ; can be 0x7fffffff
+ mov.l	LOCAL(x001fffff),r9
+ dmulu.l yn,r12
+ tst	r2,r10
+ or	DBL1H,r2
+ bt	LOCAL(inf_nan_arg1)
+ mov.l	r11,@-r15
+ sts	mach,r11	! y2*(a-1) ; u1.31
+ add	yn,r11		! z0       ; u1.31
+ dmulu.l r11,DBL1L
+ mov.l	LOCAL(x40000000),DBLRH	! bias + 1
+ and	r9,r2		! x ; u12.20
+ cmp/hi	DBL0L,DBL1L
+ sts	macl,r8
+ mov	#-24,r12
+ sts	mach,r9 	! r9:r8 := z0 * DBL1L; u-19.64
+ subc	DBL1H,DBLRH
+ mul.l	r11,r2  	! (r9+macl):r8 == z0*x; u-19.64
+ shll	r8
+ add	DBL0H,DBLRH	! result sign/exponent + 1
+ mov	r8,r10
+ sts	macl,DBLRL
+ add	DBLRL,r9
+ rotcl	r9		! r9:r8 := z*x; u-20.63
+ shld	r12,r10
+ mov.l	LOCAL(x7fe00000),DBLRL
+ sub	DBL0L,r9	! r9:r8 := -a ; u-20.63
+ mov.l	LOCAL(x00200000),r12
+FIXME: the following  shift might loose the sign.
+ shll8	r9
+ or	r10,r9	! -a1 ; s-28.32
+ mov.l	LOCAL(x00100000),r10
+ dmuls.l r9,yn	! r3 dead
+ mov	DBL1H,r3
+ mov.l LOCAL(xfff00000),DBL0L
+ xor	DBL0H,r3	! calculate expected sign & bit20
+ div0s	r3,DBLRH
+ xor	DBLRH,r3
+ bt	LOCAL(ret_denorm_inf)
+ tst	DBLRL,DBLRH
+ bt	LOCAL(ret_denorm)
+ sub	r12,DBLRH ! calculate sign / exponent minus implicit 1
+ tst	r10,r3	! set T if a >= x
+ sts	mach,r12! -z1 ; s-27.32
+ bt	0f
+ add	r11,r11	! z0 ; u1.31 / u0.31
+0: mov	#6,r3
+ negc	r3,r10 ! shift count := a >= x ? -7 : -6; T := 1
+ shll8	r8	! r9:r8 := -a1 ; s-28.64
+ shad	r10,r12	! -z1 ; truncate to s-20.32 / s-21.32
+ rotcl	r12	! -z1 ; s-21.32 / s-22.32 / round to odd 0.5 ulp ; T := sign
+ add	#20,r10
+ dmulu.l r12,DBL1L ! r12 signed, DBL1L unsigned
+ and	DBL0L,DBLRH	! isolate sign / exponent
+ shld	r10,r9
+ mov	r8,r3
+ shld	r10,r8
+ sts	macl,DBL0L
+ sts	mach,DBLRL
+ add	#-32,r10
+ shld	r10,r3
+ mul.l r12,r2
+ bf	0f	! adjustment for signed/unsigned multiply
+ sub	DBL1L,DBLRL	! DBL1L dead
+0: shar	r12	! -z1 ; truncate to s-20.32 / s-21.32
+ sts	macl,DBL1L
+ or	r3,r9	! r9:r8 := -a1 ;             s-41.64/s-42.64
+ !
+ cmp/hi	r8,DBL0L
+ add	DBLRL,DBL1L ! DBL1L:DBL0L := -z1*x ; s-41.64/s-42.64
+ subc	DBL1L,r9
+ not	r12,DBLRL ! z1, truncated to s-20.32 / s-21.32
+ shll	r9	! T :=  a2 > 0
+ mov	r11,r2
+ mov	#21,r7
+ shld	r7,r11
+ addc	r11,DBLRL
+ mov.l	@r15+,r11
+ mov.l	@r15+,r10
+ mov	#-11,r7
+ mov.l	@r15+,r9
+ shld	r7,r2
+ mov.l	@r15+,r8
+ addc	r2,DBLRH
+ rts
+ mov.l	@r15+,r12
+
+LOCAL(ret_denorm):
+	tst	r10,DBLRH
+	bra	LOCAL(denorm_have_count)
+	movt	DBLRH	! calculate shift count (off by 2)
+
+LOCAL(ret_denorm_inf):
+	mov	DBLRH,r12
+	add	r12,r12
+	cmp/pz	r12
+	mov	#-21,DBLRL
+	bt	LOCAL(ret_inf_late)
+	shld	DBLRL,DBLRH
+LOCAL(denorm_have_count):
+	add	#-2,DBLRH
+/* FIXME */
+	bra	LOCAL(return_0)
+	mov.l	@r15+,r11
+
+LOCAL(ret_inf_late):
+	mov.l	@r15+,r11
+	!
+	mov.l	@r15+,r10
+	!
+	mov.l	@r15+,r9
+	bra	LOCAL(return_inf)
+	mov.l	@r15+,r8
+
+	.balign	4
+LOCAL(clz):
+	mov.l	r8,@-r15
+	extu.w	r0,r8
+	mov.l	r9,@-r15
+	cmp/eq	r0,r8
+	bt/s	0f
+	mov	#8-11,r9
+	xtrct	r0,r8
+	add	#16,r9
+0:	tst	r12,r8	! 0xff00
+	mov.l	LOCAL(c_clz_tab),r0
+	bt	0f
+	shlr8	r8
+0:	bt	0f
+	add	#8,r9
+0:
+#ifdef	__PIC__
+	add	r0,r8
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r8),r8
+	mov	r9,r0
+	mov.l	@r15+,r9
+	!
+	!
+	!
+	sub	r8,r0
+	mov.l	@r15+,r8
+	rts
+	lds.l	@r15+,pr
+
+!	We encode even some words as pc-relative that would fit as immediate
+!	in the instruction in order to avoid some pipeline stalls on
+!	SH4-100 / SH4-200.
+LOCAL(d1):	.word 1
+LOCAL(d12):	.word 12
+LOCAL(d13):	.word 13
+
+	.balign 4
+LOCAL(x7ff00000): .long 0x7ff00000
+LOCAL(xfffe2006): .long 0xfffe2006
+LOCAL(x001fffff): .long 0x001fffff
+LOCAL(x40000000): .long 0x40000000
+LOCAL(x7fe00000): .long 0x7fe00000
+LOCAL(x00100000): .long 0x00100000
+LOCAL(x00200000): .long 0x00200000
+LOCAL(xfff00000): .long 0xfff00000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+LOCAL(ytab):
+        .byte   120, 105,  91,  78,  66,  54,  43,  33
+        .byte    24,  15,   8,   0,  -5, -12, -17, -22
+        .byte   -27, -31, -34, -37, -40, -42, -44, -45
+        .byte   -46, -46, -47, -46, -46, -45, -44, -42
+        .byte   -41, -39, -36, -34, -31, -28, -24, -20
+        .byte   -17, -12,  -8,  -4,   0,   5,  10,  16
+        .byte    21,  27,  33,  39,  45,  52,  58,  65
+        .byte    72,  79,  86,  93, 101, 109, 116, 124
+ENDFUNC(GLOBAL(divdf3))
--- gcc/libgcc/config/sh/IEEE-754/m3/fixunsdfsi.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/m3/fixunsdfsi.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,81 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!! fixunsdfsi for Renesas SH / STMicroelectronics ST40 CPUs
+!! Contributed by J"orn Rennecke joern.rennecke@st.com
+
+	! What is a bit unusal about this implementation is that the
+	! sign bit influences the result for NANs: for cleared sign bit, you
+	! get INT_MAX, for set sign bit, you get INT_MIN.
+	! However, since the result for NANs is undefined, this should be no
+	! problem.
+	! N.B. This is scheduled both for SH4-200 and SH4-300
+	.balign 4
+	.global GLOBAL(fixunsdfsi)
+	FUNC(GLOBAL(fixunsdfsi))
+	.balign	4
+GLOBAL(fixunsdfsi):
+	mov.w	LOCAL(x413),r1	! bias + 20
+	mov	DBL0H,r0
+	shll	DBL0H
+	mov.l	LOCAL(mask),r3
+	mov	#-21,r2
+	shld	r2,DBL0H	! SH4-200 will start this insn in a new cycle
+	bt/s	LOCAL(ret0)
+	sub	r1,DBL0H
+	cmp/pl	DBL0H		! SH4-200 will start this insn in a new cycle
+	and	r3,r0
+	bf/s	LOCAL(ignore_low)
+	addc	r3,r0	! uses T == 1; sets implict 1
+	mov	#11,r2
+	shld	DBL0H,r0	! SH4-200 will start this insn in a new cycle
+	cmp/gt	r2,DBL0H
+	add	#-32,DBL0H
+	bt	LOCAL(retmax)
+	shld	DBL0H,DBL0L
+	rts
+	or	DBL0L,r0
+
+	.balign	8
+LOCAL(ignore_low):
+	mov	#-21,r2
+	cmp/gt	DBL0H,r2	! SH4-200 will start this insn in a new cycle
+	add	#1,r0
+	bf	0f
+LOCAL(ret0): mov #0,r0		! results in 0 return
+0:	rts
+	shld	DBL0H,r0
+
+LOCAL(retmax):
+	rts
+	mov	#-1,r0
+
+LOCAL(x413): .word 0x413
+
+	.balign 4
+LOCAL(mask): .long 0x000fffff
+	ENDFUNC(GLOBAL(fixunsdfsi))
+
--- gcc/libgcc/config/sh/IEEE-754/m3/addsf3.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/m3/addsf3.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,290 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! addsf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+
+	.balign 4
+	.global GLOBAL(subsf3)
+	FUNC(GLOBAL(subsf3))
+	.global GLOBAL(addsf3)
+	FUNC(GLOBAL(addsf3))
+GLOBAL(subsf3):
+	cmp/pz	r5
+	add	r5,r5
+	rotcr	r5
+	.balign 4
+GLOBAL(addsf3):
+	mov.l	LOCAL(x7f800000),r3
+	mov	r4,r6
+	add	r6,r6
+	mov	r5,r7
+	add	r7,r7
+	mov	r4,r0
+	or	r3,r0
+	cmp/hi	r6,r7
+	mov	r5,r1
+	bf/s	LOCAL(r4_hs)
+	 or	r3,r1
+	cmp/eq	r5,r1
+	bt	LOCAL(ret_r5) /* sole Inf or NaN, return unchanged.  */
+	shll8	r0	! r4 fraction
+	shll8	r1	! r5 fraction
+	mov	r6,r3
+	mov	#-24,r2
+	mov	r7,r6
+	shld	r2,r6	! r5 exp
+	mov	r0,r7
+	shld	r2,r3	! r4 exp
+	tst	r3,r3
+	sub	r6,r3	! exp difference (negative or 0)
+	bt	LOCAL(denorm_r4)
+LOCAL(denorm_r4_done): ! r1: u1.31
+	shld	r3,r0	! Get 31 upper bits, including 8 guard bits
+	mov.l	LOCAL(xff000000),r2
+	add	#31,r3
+	mov.l	r5,@-r15 ! push result sign.
+	cmp/pl	r3	! r0 has no more than one bit set -> return arg 1
+	shld	r3,r7	! copy of lowest guard bit in r0 and lower guard bits
+	bf	LOCAL(ret_stack)
+	div0s	r4,r5
+	bf/s	LOCAL(add)
+	 cmp/pl	r7	/* Is LSB in r0 clear, but any lower guards bit set?  */
+	subc	r0,r1
+	mov.l	LOCAL(c__clz_tab),r7
+	tst	r2,r1
+	mov	#-24,r3
+	bf/s LOCAL(norm_r0)
+	 mov	r1,r0
+	extu.w	r1,r1
+	bra	LOCAL(norm_check2)
+	 cmp/eq	r0,r1
+LOCAL(ret_r5):
+	rts
+	mov	r5,r0
+LOCAL(ret_stack):
+	rts
+	mov.l	@r15+,r0
+
+/* We leave the numbers denormalized, but we change the bit position to be
+   consistent with normalized numbers.  This also removes the spurious
+   leading one that was inserted before.  */
+LOCAL(denorm_r4):
+	tst	r6,r6
+	add	r0,r0
+	bf	LOCAL(denorm_r4_done)
+	bra	LOCAL(denorm_r4_done)
+	add	r1,r1
+LOCAL(denorm_r5):
+	tst	r6,r6
+	add	r1,r1
+	bf	LOCAL(denorm_r5_done)
+	clrt
+	bra	LOCAL(denorm_r5_done)
+	add	r0,r0
+
+/* If the exponent differs by two or more, normalization is minimal, and
+   few guard bits are needed for an exact final result, so sticky guard
+   bit compresion before subtraction (or add) works fine.
+   If the exponent differs by one, only one extra guard bit is generated,
+   and effectively no guard bit compression takes place.  */
+
+	.balign	4
+LOCAL(r4_hs):
+	cmp/eq	r4,r0
+	mov	#-24,r3
+	bt	LOCAL(inf_nan_arg0)
+	shld	r3,r7
+	shll8	r0
+	tst	r7,r7
+	shll8	r1
+	mov.l	LOCAL(xff000000),r2
+	bt/s	LOCAL(denorm_r5)
+	shld	r3,r6
+LOCAL(denorm_r5_done):
+	mov	r1,r3
+	subc	r6,r7
+	bf	LOCAL(same_exp)
+	shld	r7,r1	/* Get 31 upper bits.  */
+	add	#31,r7
+	mov.l	r4,@-r15 ! push result sign.
+	cmp/pl	r7
+	shld	r7,r3
+	bf	LOCAL(ret_stack)
+	div0s	r4,r5
+	bf/s	LOCAL(add)
+	 cmp/pl	r3	/* Is LSB in r1 clear, but any lower guard bit set?  */
+	subc	r1,r0
+	mov.l	LOCAL(c__clz_tab),r7
+LOCAL(norm_check):
+	tst	r2,r0
+	mov	#-24,r3
+	bf LOCAL(norm_r0)
+	extu.w	r0,r1
+	cmp/eq	r0,r1
+LOCAL(norm_check2):
+	mov	#-8,r3
+	bt LOCAL(norm_r0)
+	mov	#-16,r3
+LOCAL(norm_r0):
+	mov	r0,r1
+	shld	r3,r0
+#ifdef __pic__
+	add	r0,r7
+	mova  LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r7),r7
+	add	#25,r3
+	add	#-9+1,r6
+	mov	r1,r0
+	sub	r7,r3
+	mov.l	LOCAL(xbfffffff),r7
+	sub	r3,r6	/* generate exp-1  */
+	mov.w	LOCAL(d24),r2
+	cmp/pz	r6	/* check exp > 0  */
+	shld	r3,r0	/* Leading 1 becomes +1 exp adjustment.  */
+	bf	LOCAL(zero_denorm)
+LOCAL(denorm_done):
+	add	#30,r3
+	shld	r3,r1
+	mov.w   LOCAL(m1),r3
+	tst	r7,r1	! clear T if rounding up
+	shld	r2,r6
+	subc	r3,r0	! round - overflow will boost exp adjustment to 2.
+	mov.l	@r15+,r2
+	add	r6,r0	! overflow will generate inf
+	cmp/ge	r2,r3	! get sign into T
+	rts
+	rotcr	r0
+LOCAL(ret_r4):
+	rts
+	mov	r4,r0
+
+/* At worst, we are shifting the number back in place where an incoming
+   denormal was.  Thus, the shifts won't get out of range.  They still
+   might generate a zero fraction, but that's OK, that makes it 0.  */
+LOCAL(zero_denorm):
+	add	r6,r3
+	mov	r1,r0
+	mov	#0,r6	/* leading one will become free (except for rounding) */
+	bra	LOCAL(denorm_done)
+	shld	r3,r0
+
+/* Handle abs(r4) >= abs(r5), same exponents specially so we don't need
+   check for a zero fraction in the main path.  */
+LOCAL(same_exp):
+	div0s	r4,r5
+	mov.l	r4,@-r15
+	bf	LOCAL(add)
+	cmp/eq	r1,r0
+	mov.l	LOCAL(c__clz_tab),r7
+	bf/s	LOCAL(norm_check)
+	 sub	r1,r0
+	rts	! zero difference -> return +zero
+	mov.l	@r15+,r1
+
+/* r2: 0xff000000 */
+LOCAL(add):
+	addc	r1,r0
+	mov.w	LOCAL(x2ff),r7
+	shll8	r6
+	bf/s	LOCAL(no_carry)
+	shll16	r6
+	tst	r7,r0		
+	shlr8	r0
+	mov.l	@r15+,r3	! discard saved sign
+	subc	r2,r0
+	sett
+	addc	r6,r0
+	cmp/hs	r2,r0
+	bt/s	LOCAL(inf)
+	div0s	r7,r4 /* Copy sign.  */
+	rts
+	rotcr	r0
+LOCAL(inf):
+	mov	r2,r0
+	rts
+	rotcr	r0
+	
+LOCAL(no_carry):
+	mov.w	LOCAL(m1),r3
+	tst	r6,r6
+	bt	LOCAL(denorm_add)
+	add	r0,r0
+	tst	r7,r0		! check if lower guard bit set or round to even
+	shlr8	r0
+	mov.l	@r15+,r1	! discard saved sign
+	subc	r3,r0	! round ; overflow -> exp++
+	cmp/ge	r4,r3	/* Copy sign.  */
+	add	r6,r0	! overflow -> inf
+	rts
+	rotcr	r0
+
+LOCAL(denorm_add):
+	cmp/ge	r4,r3	/* Copy sign.  */
+	shlr8	r0
+	mov.l	@r15+,r1	! discard saved sign
+	rts
+	rotcr	r0
+
+LOCAL(inf_nan_arg0):
+	cmp/eq	r5,r1
+	bf	LOCAL(ret_r4)
+	div0s	r4,r5		/* Both are inf or NaN, check signs.  */
+	bt	LOCAL(ret_nan)	/* inf - inf, or NaN.  */
+	mov	r4,r0		! same sign; return NaN if either is NaN.
+	rts
+	or	r5,r0
+LOCAL(ret_nan):
+	rts
+	mov	#-1,r0
+
+LOCAL(d24):
+	.word	24
+LOCAL(x2ff):
+	.word	0x2ff
+LOCAL(m1):
+	.word	-1
+	.balign	4
+LOCAL(x7f800000):
+	.long	0x7f800000
+LOCAL(xbfffffff):
+	.long	0xbfffffff
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(xfe000000):
+	.long	0xfe000000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+
+	ENDFUNC(GLOBAL(addsf3))
+	ENDFUNC(GLOBAL(subsf3))
+
--- gcc/libgcc/config/sh/IEEE-754/m3/adddf3.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/m3/adddf3.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,614 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! adddf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4-200 without FPU, but can also be used for SH3.
+! Numbers with same sign are added in typically 37 cycles, worst case is
+! 43 cycles, unless there is an overflow, in which case the addition can
+! take up to takes 47 cycles.
+! Normal numbers with different sign are added in 56 (57 for PIC) cycles
+! or less on SH4.
+! If one of the inputs is a denormal, the worst case is 59 (60 for PIC)
+! cycles. (Two denormal inputs are faster than normal inputs, and
+! denormal outputs don't slow down computation).
+! Subtraction takes two cycles to negate the second input and then drops
+! through to addition.
+
+/* If the input exponents of a difference of two normalized numbers
+   differ by more than one, the output does not need to be adjusted
+   by more than one bit position.  Hence, it makes sense to ensure that
+   the shifts by 0 & 1 are handled quickly to reduce average and worst
+   case times.  */
+FUNC(GLOBAL(adddf3))
+FUNC(GLOBAL(subdf3))
+	.global	GLOBAL(adddf3)
+	.global	GLOBAL(subdf3)
+LOCAL(denorm_arg1):
+	bt LOCAL(inf_nan_arg0)
+	tst	r0,r2
+	bt/s	LOCAL(denorm_both)
+	shlr	r1
+	mov.l	LOCAL(x00100000),r3
+	bra	LOCAL(denorm_arg1_done)
+	 sub	r2,r3
+
+! Handle denorm addition here because otherwise the ordinary addition would
+! have to check for denormal results.
+! Denormal subtraction could also be done faster, but the denorm subtraction
+! path here is still one cycles faster than the one for normalized input
+! numbers, and 16 instructions shorter than the fastest version.
+! Here we also generate +0.0 + +0.0 -> +0.0 ; -0.0 + -0.0 -> -0.0
+LOCAL(denorm_both):
+	div0s	r8,DBL1H
+	mov.l	LOCAL(x800fffff),r9
+	bt/s	LOCAL(denorm_sub)
+	and	r1,DBL1H
+	and	r9,r8
+	mov.l	@r15+,r9
+	mov	DBL0L,DBLRL
+	mov	r8,DBLRH
+	addc	DBL1L,DBLRL
+	mov.l	@r15+,r8
+	rts
+	 addc	DBL1H,DBLRH
+
+! N.B., since subtraction also generates +0.0 for subtraction of numbers
+! with identical fractions, this also covers the +0.0 + -0.0 -> +0.0 /
+! -0.0 + +0.0 -> +0.0 cases.
+LOCAL(denorm_sub):
+	mov	r8,DBL0H	! tentative result sign
+	and	r1,DBL0H
+	bra	LOCAL(sub_same_exp)
+	 addc	r1,r2	! exponent++, clear T
+
+LOCAL(inf_nan_arg0):
+	cmp/hs 	r0,r3
+	bf		LOCAL(inf_nan_ret)
+	tst	DBL1L,DBL1L
+	bf		LOCAL(ret_nan)
+	shlr	r1
+	and 	DBL1H,r1
+	tst	r1,r1
+	bf		LOCAL(ret_nan)
+	div0s	r8,DBL1H
+	bf		LOCAL(inf_nan_ret)
+LOCAL(ret_nan):
+	mov 	#-1,DBLRH
+	bra	LOCAL(pop_r8_r9)
+		mov DBLRH,DBLRL
+  
+LOCAL(inf_nan_ret):
+	mov	DBL0L,DBLRL
+	bra	LOCAL(pop_r8_r9)
+		mov r8,DBLRH
+
+LOCAL(ret_arg0):
+	mov.l LOCAL(x800fffff),DBLRH
+	mov	DBL0L,DBLRL
+	mov	r2,r3
+LOCAL(ret_arg):
+	mov.l	@r15+,r9
+	and	r8,DBLRH
+	mov.l	@r15+,r8
+	rts
+	or r3,DBLRH
+
+LOCAL(no_carry):
+	shlr	r0
+	mov.l	LOCAL(x000fffff),DBLRH
+	addc	r3,r9
+	mov.w	LOCAL(d0),DBL1H
+	mov	DBL0L,DBLRL
+	and	DBL0H,DBLRH	! mask out implicit 1
+	mov.l	LOCAL(x7ff00000),r3
+	addc	DBL1H,DBLRL
+	addc	r2,DBLRH
+	mov.l	@r15+,r9
+	add	DBL1H,DBLRH	! fraction overflow -> exp increase
+	bra	LOCAL(add_done)
+	 cmp/hi	r3,DBLRH
+
+LOCAL(inf):
+	mov	#0,DBLRL
+	bra	LOCAL(or_sign)
+	mov	r3,DBLRH
+
+	.balign	4
+GLOBAL(subdf3):
+	cmp/pz DBL1H
+	add 	DBL1H,DBL1H
+	rotcr	DBL1H
+	nop
+
+GLOBAL(adddf3):
+	mov.l	LOCAL(x7ff00000),r0
+	mov	DBL0H,r2
+	mov.l	LOCAL(x001fffff),r1
+	mov	DBL1H,r3
+	mov.l	r8,@-r15
+	and	r0,r2		! r2 <- exp0
+	mov.l	r9,@-r15
+	and	r0,r3		! r3 <- exp1
+	cmp/hi r2,r3
+	or		r0,DBL0H
+	or		r0,DBL1H
+	bt		LOCAL(arg1_gt)
+	tst	r0,r3
+	mov	#-20,r9
+	mov	DBL0H,r8	! tentative result sign
+	and	r1,DBL0H	! arg0 fraction
+	bt/s	LOCAL(denorm_arg1)
+		cmp/hs r0,r2
+	bt		LOCAL(inf_nan_arg0)
+	sub	r2,r3
+LOCAL(denorm_arg1_done):	! r2 is tentative result exponent
+	shad	r9,r3
+	mov.w	LOCAL(m32),r9
+	mov	DBL1H,r0	! the 'other' sign
+	and	r1,DBL1H	! arg1 fraction
+	cmp/ge r9,r3
+	mov	DBL1H,r1
+	bf/s	LOCAL(large_shift_arg1)
+	 shld	r3,DBL1H
+LOCAL(small_shift_arg1):
+	mov	DBL1L,r9
+	shld	r3,DBL1L
+	tst	r3,r3
+	add	#32,r3
+	bt/s	LOCAL(same_exp)
+	 div0s r8,r0	! compare signs
+	shld	r3,r1
+
+	or		r1,DBL1L
+	bf/s	LOCAL(add)
+	shld	r3,r9
+	clrt
+	negc	r9,r9
+	mov.l	LOCAL(x001f0000),r3
+LOCAL(sub_high):
+	mov	DBL0L,DBLRL
+	subc	DBL1L,DBLRL
+	mov	DBL0H,DBLRH
+	bra	LOCAL(subtract_done)
+	 subc	DBL1H,DBLRH
+
+LOCAL(large_shift_arg1):
+	mov	DBL1L,r9
+	shld	r3,r9
+	add	#64,r3
+	cmp/pl r3
+	shld	r3,r1
+	shld	r3,DBL1L
+	bf		LOCAL(ret_arg0)
+	tst	DBL1L,DBL1L
+	bt		LOCAL(large_shift_arg1_end)
+	mov	#1,DBL1L
+	or 	DBL1L,r9
+LOCAL(large_shift_arg1_end):
+	mov	DBL1H,DBL1L
+	mov	#0,DBL1H
+	add	r1,r9
+	div0s	r8,r0	! compare signs
+	bf		LOCAL(add)
+	clrt
+	mov.l	LOCAL(x001f0000),r3
+	bra	LOCAL(sub_high)
+	 negc	r9,r9
+
+LOCAL(add_clr_r9):
+	mov	#0,r9
+LOCAL(add):
+	mov.l	LOCAL(x00200000),r3
+	addc	DBL1L,DBL0L
+	addc	DBL1H,DBL0H
+	mov.l	LOCAL(x80000000),r1
+	tst	r3,DBL0H
+	mov.l	LOCAL(x7fffffff),r3
+	mov	DBL0L,r0
+	bt/s	LOCAL(no_carry)
+	and	r1,r8
+	tst	r9,r9
+	bf		LOCAL(add_one)
+	tst	#2,r0
+LOCAL(add_one):
+	subc	r9,r9
+	sett
+	mov	r0,DBLRL 
+	addc	r9,DBLRL
+	mov	DBL0H,DBLRH
+	addc	r9,DBLRH
+	shlr	DBLRH
+	mov.l	LOCAL(x7ff00000),r3
+	add	r2,DBLRH
+	mov.l	@r15+,r9
+	rotcr	DBLRL
+	cmp/hs r3,DBLRH
+LOCAL(add_done):
+	bt		LOCAL(inf)
+LOCAL(or_sign):
+	or		r8,DBLRH
+	rts
+	 mov.l @r15+,r8
+
+LOCAL(same_exp):
+	bf	LOCAL(add_clr_r9)
+	clrt
+LOCAL(sub_same_exp):
+	subc	DBL1L,DBL0L
+	mov.l	LOCAL(x001f0000),r3
+	subc	DBL1H,DBL0H
+	mov.w	LOCAL(d0),r9
+	bf	LOCAL(pos_difference_0)
+	clrt
+	negc	DBL0L,DBLRL
+	mov.l	LOCAL(x80000000),DBL0L
+	negc	DBL0H,DBLRH
+	mov.l	LOCAL(x00100000),DBL0H
+	tst	r3,DBLRH
+	not	r8,r8
+	bt/s	LOCAL(long_norm)
+	and	DBL0L,r8
+	bra	LOCAL(norm_loop_2)
+	 not	DBL0L,r3
+
+LOCAL(large_shift_arg0):
+	mov	DBL0L,r9
+	shld	r2,r9
+	add	#64,r2
+	cmp/pl	r2
+	shld	r2,r1
+	shld	r2,DBL0L
+	bf	LOCAL(ret_arg1_exp_r3)
+	tst	DBL0L,DBL0L
+	bt LOCAL(large_shift_arg0_end)
+	mov 	#1,DBL0L
+	or		DBL0L,r9 
+LOCAL(large_shift_arg0_end):
+	mov	DBL0H,DBL0L
+	mov	#0,DBL0H
+	add	r1,r9
+	div0s	r8,r0	! compare signs
+	mov	r3,r2	! tentative result exponent
+	bf	LOCAL(add)
+	clrt
+	negc	r9,r9
+	bra	LOCAL(subtract_arg0_arg1_done)
+	 mov	DBL1L,DBLRL
+
+LOCAL(arg1_gt):
+	tst	r0,r2			! r0 = 0x7ff00000 r2 = exp0
+	mov	#-20,r9
+	mov	DBL1H,r8		! tentative result sign
+	and	r1,DBL1H
+	bt/s	LOCAL(denorm_arg0)
+	cmp/hs	r0,r3
+	bt	LOCAL(inf_nan_arg1)
+	sub	r3,r2
+LOCAL(denorm_arg0_done):
+	shad	r9,r2			! r2 <- shifting value
+	mov.w	LOCAL(m32),r9
+	mov	DBL0H,r0		! the 'other' sign
+	and	r1,DBL0H
+	cmp/ge	r9,r2
+	mov	DBL0H,r1
+	bf/s	LOCAL(large_shift_arg0)
+		shld	r2,DBL0H
+LOCAL(small_shift_arg0):
+	mov	DBL0L,r9
+	shld	r2,DBL0L
+	mov.l	r3,@-r15
+	mov 	#32,r3
+	add	r3,r2
+	cmp/ge	r3,r2
+	bf LOCAL(shifting)
+	mov	#0,r1
+	mov 	r1,r9
+LOCAL(shifting):
+	shld	r2,r1
+	mov	r2,r3
+	shld	r3,r9
+	div0s	r8,r0		! compare signs
+	mov.l	@r15+,r2	! tentative result exponent
+	bf/s	LOCAL(add)
+	or	r1,DBL0L
+	clrt
+	negc	r9,r9
+	mov	DBL1L,DBLRL
+LOCAL(subtract_arg0_arg1_done):
+	subc	DBL0L,DBLRL
+	mov	DBL1H,DBLRH
+	mov.l	LOCAL(x001f0000),r3
+	subc	DBL0H,DBLRH
+/* Since the exponents were different, the difference is positive.  */
+/* Fall through */
+LOCAL(subtract_done):
+/* First check if a shift by a few bits is sufficient.  This not only
+   speeds up this case, but also alleviates the need for considering
+   lower bits from r9 or rounding in the other code.
+   Moreover, by handling the upper 1+4 bits of the fraction here, long_norm
+   can assume that DBLRH fits into 20 (20 < 16) bit.  */
+	tst	r3,DBLRH
+	mov.l	LOCAL(x80000000),r3
+	mov.l	LOCAL(x00100000),DBL0H
+	bt/s	LOCAL(long_norm)
+	and	r3,r8
+	mov.l	LOCAL(x7fffffff),r3
+LOCAL(norm_loop_2):	! Well, this used to be a loop...
+	tst	DBL0H,DBLRH
+	sub	DBL0H,r2
+	bf	LOCAL(norm_round)
+	shll	r9
+	rotcl	DBLRL
+	
+	rotcl	DBLRH
+	
+	 subc	DBL0H,r2
+LOCAL(norm_loop_1):
+	bt	LOCAL(denorm0_n)
+	tst	DBL0H,DBLRH
+	bf	LOCAL(norm_round)
+	shll	DBLRL
+	rotcl	DBLRH	! clears T
+	bra	LOCAL(norm_loop_1)
+	 subc	DBL0H,r2
+	 
+LOCAL(denorm_arg0):
+	bt	LOCAL(inf_nan_arg1)
+	mov.l	LOCAL(x00100000),r2
+	shlr	r1				! r1 <- 0xfffff
+	bra	LOCAL(denorm_arg0_done)
+	 sub	r3,r2			! r2 <- 1 - exp1
+
+LOCAL(inf_nan_arg1):
+	mov	DBL1L,DBLRL
+	bra	LOCAL(pop_r8_r9)
+	 mov	r8,DBLRH
+
+LOCAL(ret_arg1_exp_r3):
+	mov.l	LOCAL(x800fffff),DBLRH
+	bra	LOCAL(ret_arg)
+	 mov	DBL1L,DBLRL
+
+LOCAL(pos_difference_0):
+	tst	r3,DBL0H
+	mov	DBL0L,DBLRL
+	mov.l	LOCAL(x80000000),DBL0L
+	mov	DBL0H,DBLRH
+	mov.l	LOCAL(x00100000),DBL0H
+	bt/s	LOCAL(long_norm)
+	and	DBL0L,r8
+	bra	LOCAL(norm_loop_2)
+	 not	DBL0L,r3
+
+#ifdef __pic__
+	.balign 8
+#endif
+LOCAL(m32):
+	.word	-32
+LOCAL(d0):
+	.word	0
+#ifndef __pic__
+	.balign 8
+#endif
+! Because we had several bits of cancellations, we know that r9 contains
+! only one bit.
+! We'll normalize by shifting words so that DBLRH:DBLRL contains
+! the fraction with 0 < DBLRH <= 0x1fffff, then we shift DBLRH:DBLRL
+! up by 21 minus the number of non-zero bits in DBLRH.
+LOCAL(long_norm):
+	tst	DBLRH,DBLRH
+	mov.w	LOCAL(xff),DBL0L
+	mov	#21,r3
+	bf	LOCAL(long_norm_highset)
+	mov.l	LOCAL(x02100000),DBL1L	! shift 32, implicit 1
+	tst	DBLRL,DBLRL
+	extu.w	DBLRL,DBL0H
+	bt	LOCAL(zero_or_ulp)
+	mov	DBLRL,DBLRH
+	cmp/hi	DBL0H,DBLRL
+	bf	0f
+	mov.l	LOCAL(x01100000),DBL1L	! shift 16, implicit 1
+	clrt
+	shlr16  DBLRH
+	xtrct	DBLRL,r9
+	mov     DBLRH,DBL0H
+LOCAL(long_norm_ulp_done):
+0:	mov	r9,DBLRL	! DBLRH:DBLRL == fraction; DBL0H == DBLRH
+	subc	DBL1L,r2
+	bt	LOCAL(denorm1_b)
+#ifdef __pic__
+	mov.l	LOCAL(c__clz_tab),DBL1H
+LOCAL(long_norm_lookup):
+	mov	r0,r9
+	mova	LOCAL(c__clz_tab),r0
+	add	DBL1H,r0
+#else
+	mov	r0,r9
+LOCAL(long_norm_lookup):
+	mov.l	LOCAL(c__clz_tab),r0
+#endif /* __pic__ */
+	cmp/hi	DBL0L,DBL0H
+	bf	0f
+	shlr8	DBL0H
+0:	mov.b	@(r0,DBL0H),r0
+	bf	0f
+	add	#-8,r3
+0:	mov.w	LOCAL(d20),DBL0L
+	mov	#-20,DBL0H
+	clrt
+	sub	r0,r3
+	mov	r9,r0
+	mov	r3,DBL1H
+	shld	DBL0L,DBL1H
+	subc	DBL1H,r2
+	!
+	bf	LOCAL(no_denorm)
+	shad	DBL0H,r2
+	bra	LOCAL(denorm1_done)
+	add	r2,r3
+	
+LOCAL(norm_round):
+	cmp/pz	r2
+	mov	#0,DBL1H
+	bf	LOCAL(denorm0_1)
+	or	r8,r2
+	mov	DBLRL,DBL1L
+	shlr	DBL1L
+	addc	r3,r9
+	mov.l	@r15+,r9
+	addc	DBL1H,DBLRL	! round to even
+	mov.l	@r15+,r8
+	rts
+	 addc	r2,DBLRH
+
+LOCAL(norm_pack):
+	add	r8,DBLRH
+	mov.l	@r15+,r8
+	rts
+	add	r2,DBLRH
+
+LOCAL(denorm0_1):
+	mov.l	@r15+,r9
+	mov	r8,DBL0L
+	mov.l	@r15+,r8
+LOCAL(denorm0_shift):
+	shlr	DBLRH
+	rotcr	DBLRL
+
+	rts
+	add	DBL0L,DBLRH
+
+LOCAL(denorm0_n):
+	mov.l	@r15+,r9
+	mov	r8,DBL0L
+	addc	DBL0H,r2
+	mov.l	@r15+,r8
+	bra LOCAL(denorm0_shift)
+		  nop
+
+LOCAL(no_denorm):
+	add	r2,r8		! add (exponent - 1) to sign
+
+LOCAL(denorm1_done):
+	shld	r3,DBLRH
+	mov	DBLRL,DBL0L
+	shld	r3,DBLRL
+
+	add	r8,DBLRH	! add in sign and (exponent - 1)
+	mov.l	@r15+,r9
+	add	#-32,r3
+	mov.l	@r15+,r8
+	shld	r3,DBL0L
+
+	rts
+	add	DBL0L,DBLRH
+
+LOCAL(long_norm_highset):
+	mov.l	LOCAL(x00200000),DBL1L	! shift 1, implicit 1
+	shll	r9
+	rotcl	DBLRL
+	mov	DBLRH,DBL0H
+	rotcl	DBLRH	! clears T
+#ifdef __pic__
+	mov.l	LOCAL(c__clz_tab),DBL1H
+#else
+	mov	r0,r9
+#endif /* __pic__ */
+	subc	DBL1L,r2
+	add	#-1,r3
+	bf	LOCAL(long_norm_lookup)
+LOCAL(denorm1_a):
+	shlr	DBLRH
+	rotcr	DBLRL
+	mov.l	@r15+,r9
+	or	r8,DBLRH
+
+	rts
+	mov.l	@r15+,r8
+
+	.balign	4
+LOCAL(denorm1_b):
+	mov	#-20,DBL0L
+	shad	DBL0L,r2
+	mov	DBLRH,DBL0L
+	shld	r2,DBLRH
+	shld	r2,DBLRL
+	or	r8,DBLRH
+	mov.l	@r15+,r9
+	add	#32,r2
+	mov.l	@r15+,r8
+	shld	r2,DBL0L
+	rts
+	or	DBL0L,DBLRL
+
+LOCAL(zero_or_ulp):
+	tst	r9,r9
+	bf	LOCAL(long_norm_ulp_done)
+	! return +0.0
+LOCAL(pop_r8_r9):
+	mov.l	@r15+,r9
+	rts
+	mov.l	@r15+,r8
+
+LOCAL(d20):
+	.word	20
+LOCAL(xff):
+	.word 0xff
+	.balign	4
+LOCAL(x7ff00000):
+	.long	0x7ff00000
+LOCAL(x001fffff):
+	.long	0x001fffff
+LOCAL(x80000000):
+	.long	0x80000000
+LOCAL(x000fffff):
+	.long	0x000fffff
+LOCAL(x800fffff):
+	.long	0x800fffff
+LOCAL(x001f0000):
+	.long	0x001f0000
+LOCAL(x00200000):
+	.long	0x00200000
+LOCAL(x7fffffff):
+	.long	0x7fffffff
+LOCAL(x00100000):
+	.long	0x00100000
+LOCAL(x02100000):
+	.long	0x02100000
+LOCAL(x01100000):
+	.long	0x01100000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(adddf3))
+ENDFUNC(GLOBAL(subdf3))
+
--- gcc/libgcc/config/sh/IEEE-754/m3/mulsf3.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/m3/mulsf3.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,269 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! mulsf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+
+	.balign 4
+	.global GLOBAL(mulsf3)
+	FUNC(GLOBAL(mulsf3))
+GLOBAL(mulsf3):
+   mov.l   LOCAL(x7f800000),r1
+   not     r4,r2
+	mov		r4,r3
+	not		r5,r0
+	tst		r1,r2
+	or			r1,r3
+	bt/s		LOCAL(inf_nan_arg0)
+	 tst		r1,r0
+	bt			LOCAL(inf_nan_arg1)
+	tst		r1,r5
+	mov		r1,r2
+	shll8		r3
+	or			r5,r1
+	bt/s		LOCAL(zero_denorm_arg1)
+	 shll8		r1
+	tst		r2,r4
+	bt			LOCAL(zero_denorm_arg0)
+	dmulu.l	r3,r1
+	mov		r4,r0
+	and		r2,r0
+LOCAL(arg_norm):
+	and		r5,r2
+	mov.l 	LOCAL(x3f800000),r3
+	sts		mach,r1
+	sub		r3,r0
+	sts		macl,r3
+	add		r2,r0
+	cmp/pz	r1
+	mov.w 	LOCAL(x100),r2
+	bf/s		LOCAL(norm_frac) 
+	 tst		r3,r3
+	shll2		r1	 ! Shift one up, replace leading 1 with 0.  
+	shlr		r1
+	tst		r3,r3
+LOCAL(norm_frac):
+	mov.w 	LOCAL(mx80),r3
+	bf			LOCAL(round_frac)
+	tst		r2,r1
+LOCAL(round_frac):
+	mov.l 	LOCAL(xff000000),r2
+	subc		r3,r1	! Even overflow gives right result: exp++, frac=0. 
+	shlr8 	r1
+	add		r1,r0
+	shll		r0
+	bt			LOCAL(ill_exp)
+	tst		r2,r0
+	bt			LOCAL(denorm)
+	cmp/hs	r2,r0
+	bt			LOCAL(inf)
+LOCAL(insert_sign):
+	div0s	r4,r5
+	rts
+	rotcr		r0
+LOCAL(denorm0):
+	tst	r1,r1
+	mov.w 	LOCAL(x100),r2
+	bf			LOCAL(round_den0)
+	tst		r2,r0
+LOCAL(round_den0):
+	mov 		#-7,r2
+	mov.w 	LOCAL(mx80),r3
+	subc 		r3,r0
+	bra		LOCAL(insert_sign)
+	 shld 		r2,r0
+LOCAL(zero_denorm_arg1):
+	mov.l 	LOCAL(x60000000),r2	/* Check exp0 >= -64	*/
+	add		r1,r1
+	tst		r1,r1	/* arg1 == 0 ? */
+	mov		#0,r0
+	bt			LOCAL(insert_sign) /* argument 1 is zero ==> return 0  */
+	tst		r4,r2
+	bt			LOCAL(insert_sign) /* exp0 < -64  ==> return 0 */
+	mov.l 	LOCAL(c__clz_tab),r0
+	mov		r3,r2
+	mov		r1,r3
+	bra		LOCAL(arg_normalize)
+	mov		r2,r1
+LOCAL(zero_denorm_arg0):
+	mov.l 	LOCAL(x60000000),r2	/* Check exp1 >= -64	*/
+	add		r3,r3
+	tst		r3,r3	/* arg0 == 0 ? */
+	mov		#0,r0
+	bt			LOCAL(insert_sign) /* argument 0 is zero ==> return 0  */
+	tst		r5,r2
+	bt			LOCAL(insert_sign) /* exp1 < -64  ==> return 0 */
+	mov.l 	LOCAL(c__clz_tab),r0
+LOCAL(arg_normalize):
+	mov.l	r7,@-r15
+	extu.w	r3,r7
+	cmp/eq	r3,r7
+	mov.l 	LOCAL(xff000000),r7
+	mov		#-8,r2
+	bt			0f
+	tst		r7,r3
+	mov		#-16,r2
+	bt			0f
+	mov		#-24,r2
+0:
+	mov		r3,r7
+	shld		r2,r7
+#ifdef __pic__
+	add		r0,r7
+	mova  	LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r7),r0
+	add		#32,r2
+	mov		r2,r7
+	mov		#23,r2
+	sub		r0,r7
+	mov.l	LOCAL(x7f800000),r0
+	shld		r7,r3
+	shld		r2,r7
+	mov		r0,r2
+	and		r4,r0
+	sub		r7,r0
+	mov.l	@r15+,r7
+	bra		LOCAL(arg_norm)
+	 dmulu.l	r3,r1
+#if 0 /* This is slightly slower, but could be used if table lookup causes
+         cache thrashing.  */
+	bt			LOCAL(insert_sign) /* exp1 < -64  ==> return 0 */
+	mov.l 	LOCAL(xff000000),r2
+	mov		r4,r0
+LOCAL(arg_normalize):
+	tst		r2,r3
+	bf			LOCAL(arg_bit_norm)
+LOCAL(arg_byte_loop):
+	tst		r2,r3
+	add		r2,r0
+	shll8		r3
+	bt			LOCAL(arg_byte_loop)
+	add		r4,r0
+LOCAL(arg_bit_norm):
+	mov.l 	LOCAL(x7f800000),r2
+	rotl		r3
+LOCAL(arg_bit_loop):
+	add		r2,r0
+	bf/s		LOCAL(arg_bit_loop)
+	 rotl		r3
+	rotr		r3
+	rotr		r3
+	sub		r2,r0
+	bra		LOCAL(arg_norm)
+	 dmulu.l	r3,r1
+#endif /* 0 */
+LOCAL(inf):
+	bra		LOCAL(insert_sign)
+	 mov		r2,r0
+LOCAL(inf_nan_arg0):
+	bt			LOCAL(inf_nan_both)
+	add		r0,r0
+!	cmp/eq	#-1,r0	Here : modif -1 replace by -2 
+	cmp/eq	#-2,r0	/* arg1 zero? -> NAN */
+	bt			LOCAL(insert_sign)
+	mov		r4,r0
+LOCAL(inf_insert_sign):
+	bra		LOCAL(insert_sign)
+	 add		r0,r0
+LOCAL(inf_nan_both):
+	mov		r4,r0
+	bra		LOCAL(inf_insert_sign)
+	 or		r5,r0
+LOCAL(inf_nan_arg1):
+	mov		r2,r0
+	add		r0,r0
+! cmp/eq	#-1,r0	Here : modif -1 replace by -2 
+	cmp/eq	#-2,r0	/* arg0 zero? */
+	bt			LOCAL(insert_sign)
+	bra		LOCAL(inf_insert_sign)
+	 mov		r5,r0
+LOCAL(ill_exp):
+	cmp/pz	r0
+	bt			LOCAL(inf)
+LOCAL(denorm):
+	mov		#-24,r3
+	add		r1,r1
+	mov		r0,r2
+	sub		r1,r2	! remove fraction to get back pre-rounding exponent.
+	tst 		r2,r2
+	sts		mach,r0
+	sts		macl,r1
+	bt			LOCAL(denorm0)
+	shad		r3,r2
+	mov		r0,r3
+	shld		r2,r0
+	add		#32,r2
+	cmp/pz	r2
+	shld		r2,r3
+	bf			LOCAL(zero)
+	or			r1,r3
+	mov		#-1,r1
+	tst		r3,r3
+	mov.w	LOCAL(x100),r3
+	bf/s		LOCAL(denorm_round_up)
+	mov		#-0x80,r1
+	tst		r3,r0
+LOCAL(denorm_round_up):
+	mov		#-7,r3
+	subc		r1,r0
+	bra		LOCAL(insert_sign)
+	 shld		r3,r0
+LOCAL(zero):
+	bra		LOCAL(insert_sign)
+	 mov 	#0,r0
+LOCAL(x100):
+	.word	0x100
+LOCAL(x200):
+	.word	0x200
+LOCAL(x17f):
+	.word	0x17f
+LOCAL(x80):
+	.word	0x80
+LOCAL(mx80):
+	.word	-0x80
+	.balign	4
+LOCAL(mx100):
+	.word	-0x100
+	.balign	4
+LOCAL(x7f800000):
+	.long 0x7f800000
+LOCAL(x3f800000):
+	.long 0x3f800000
+LOCAL(x1000000):
+	.long	0x1000000
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(x60000000):
+	.long	0x60000000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+	ENDFUNC(GLOBAL(mulsf3))
--- gcc/libgcc/config/sh/IEEE-754/m3/floatsisf.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/m3/floatsisf.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,106 @@
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! floatsisf for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+
+FUNC(GLOBAL(floatsisf))
+	.global GLOBAL(floatsisf)
+	.balign	4
+GLOBAL(floatsisf):
+	cmp/pz	r4
+	mov	r4,r5
+	bt	0f
+	neg	r4,r5
+0:	mov.l	LOCAL(c_clz_tab),r0
+	extu.w	r5,r1
+	mov.w	LOCAL(xff00),r3
+	cmp/eq	r5,r1
+	mov	#24,r2
+	bt	0f
+	mov	r5,r1
+	shlr16	r1
+	add	#-16,r2
+0:	tst	r3,r1	! 0xff00
+	bt	0f
+	shlr8	r1
+0:	bt	0f
+	add	#-8,r2
+0:
+#ifdef	__PIC__
+	add	r0,r1
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r1),r1
+	cmp/pz	r4
+	mov.l	LOCAL(x4a800000),r3	! bias + 23 - implicit 1
+	bt	0f
+	mov.l	LOCAL(xca800000),r3	! sign + bias + 23 - implicit 1
+0:	mov	r5,r0
+	sub	r1,r2
+	mov.l	LOCAL(x80000000),r1
+	shld	r2,r0
+	cmp/pz	r2
+	add	r3,r0
+	bt	LOCAL(noround)
+	add	#31,r2
+	shld	r2,r5
+	add	#-31,r2
+	rotl	r5
+	cmp/hi	r1,r5
+	mov	#0,r3
+	addc	r3,r0
+	mov	#23,r1
+	shld	r1,r2
+	rts
+	sub	r2,r0
+	.balign	8
+LOCAL(noround):
+	mov	#23,r1
+	tst	r4,r4
+	shld	r1,r2
+	bt	LOCAL(ret0)
+	rts
+	sub	r2,r0
+LOCAL(ret0):
+	rts
+	mov	#0,r0
+
+LOCAL(xff00):	.word 0xff00
+	.balign	4
+LOCAL(x4a800000): .long 0x4a800000
+LOCAL(xca800000): .long 0xca800000
+LOCAL(x80000000): .long 0x80000000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(floatsisf))
--- gcc/libgcc/config/sh/IEEE-754/m3/muldf3.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/m3/muldf3.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,502 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! muldf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+! Normal numbers are multiplied in 53 or 54 cycles on SH4-200.
+
+FUNC(GLOBAL(muldf3))
+	.global GLOBAL(muldf3)
+LOCAL(normalize_arg53):
+	tst	r2,DBL0H
+	mov	#1,r2
+	bt	LOCAL(normalize_arg48)
+	mov	DBL0H,r1
+	shlr16	r1
+	bra	LOCAL(normalize_DBL0H)
+	mov	#21-16,r3
+
+LOCAL(normalize_arg16):
+	mov.w	LOCAL(m31),r2 ! 1-32
+	mov	#0,DBL0L
+LOCAL(normalize_arg48):
+	mov	DBL0H,r1
+	mov	#21,r3
+LOCAL(normalize_DBL0H):
+	extu.b	r1,r8
+	mov.l	LOCAL(c__clz_tab),r0
+	cmp/eq	r8,r1
+	!
+	bt	0f
+	shlr8	r1
+0:
+#ifdef	__pic__
+	add	r0,r1
+
+	mova	LOCAL(c__clz_tab),r0
+
+#endif /* __pic__ */
+	mov.b	@(r0,r1),r8
+	mov	DBL0L,r1
+	mov.l	@r15+,r0
+	bt	0f
+	add	#-8,r3
+0:	clrt
+	sub	r8,r3
+	mov.w	LOCAL(d20),r8
+	shld	r3,DBL0H 	! Normalization
+	shld	r3,DBL0L
+	sub	r3,r2 	! r2 <- shifting number
+	add	#-32,r3
+	shld	r3,r1
+	or	r1,DBL0H
+	shld	r8,r2 	! positioning r2 for exp
+	mov.l	@r15+,r8
+
+	! Here :  test tinyness 
+	mov 	DBL1H, r1
+	neg 	r2,r3
+	shll 	r1
+	shll 	r3
+	cmp/hi r1,r3
+	bt LOCAL(zero)
+	
+	add	r2,DBL1H
+	mov.l	LOCAL(x001fffff),r2
+	mov.l LOCAL(x00100000),r3
+	dmulu.l	DBL0L,DBL1L
+	bra	LOCAL(arg_denorm_done)
+	or	r3,r0		! set implicit 1 bit
+
+LOCAL(inf_nan_denorm_or_zero_a):
+	mov.l r8,@-r15
+	sub 	r3,DBL0H 	! isolate high fraction (r3 = 0xfff00000)
+	mov.l @(4,r15),r8 ! original DBL0H (with sign & exp)
+	mov.l r0,@-r15
+	mov.l	r10,@-r15
+	mov 	r1,r10
+	sub 	r3,r1 	! r1 <- 0x7ff00000
+	mov.l LOCAL(x60000000),r3
+	shll16 	r2 	! r2 <- 0xffff0000
+	!			  no stall here for sh4-200
+	!
+	tst 	r1,r8 	! test DBL0 Inf or NaN ?
+	bf LOCAL(inf_nan_a)
+	tst r10,r0 	! test for DBL1 inf, nan or small
+	mov.l	@r15+,r10
+	bt LOCAL(ret_inf_nan_zero)
+LOCAL(normalize_arg):
+	tst 	DBL0H,DBL0H
+	bf LOCAL(normalize_arg53)
+	tst 	DBL0L,DBL0L 	! test for DBL0 is zero
+	bt LOCAL(a_zero)
+	tst 	r2,DBL0L 	! test DBL0L = 0x0000xxxx
+	mov 	DBL0L,DBL0H ! left shift 32
+	bt LOCAL(normalize_arg16)
+	shlr16 	DBL0H
+	mov.w LOCAL(m15),r2	! 1-16
+	bra 	LOCAL(normalize_arg48)
+	shll16 	DBL0L
+
+LOCAL(a_zero):
+	mov.l	@(4,r15),r8
+	add	#8,r15
+LOCAL(zero):
+	mov	#0,DBLRH
+	bra	LOCAL(pop_ret)
+	mov	#0,DBLRL
+
+! both inf / nan -> result is nan if at least one is none, else inf.
+! BBL0 inf/nan, DBL1 zero   -> result is nan
+! DBL0 inf/nan, DBL1 finite -> result is DBL0 with sign adjustemnt
+LOCAL(inf_nan_a):
+	mov.l	@r15+,r10
+	mov	r8,DBL0H
+	mov.l	@(4,r15),r8
+	tst	r1,r0	! arg1 inf/nan ?
+	mov	DBL0H,DBLRH
+	add	#8,r15
+	mov	DBL0L,DBLRL
+	bt	LOCAL(both_inf_nan)
+	tst	DBL1L,DBL1L
+	mov	DBL1H,r2
+	bf	LOCAL(pop_ret)
+	add	r2,r2
+	tst	r2,r2
+	!
+	bf	LOCAL(pop_ret)
+LOCAL(nan):
+	mov	#-1,DBLRL
+	bra	LOCAL(pop_ret)
+	mov	#-1,DBLRH
+
+LOCAL(both_inf_nan):
+	or	DBL1L,DBLRL
+	bra	LOCAL(pop_ret)
+	or	DBL1H,DBLRH
+
+LOCAL(ret_inf_nan_zero):
+	tst	r1,r0
+	mov.l	@(4,r15),r8
+	or	DBL0L,DBL0H
+	bf/s	LOCAL(zero)
+	add	#8,r15
+	tst	DBL0H,DBL0H
+	bt	LOCAL(nan)
+LOCAL(inf_nan_b):
+	mov	DBL1L,DBLRL
+	mov	DBL1H,DBLRH
+LOCAL(pop_ret):
+	mov.l	@r15+,DBL0H
+	add	DBLRH,DBLRH
+
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+
+	.balign	4
+/* Argument a has already been tested for being zero or denorm.
+   On the other side, we have to swap a and b so that we can share the
+   normalization code.
+   a: sign/exponent : @r15 fraction: DBL0H:DBL0L
+   b: sign/exponent: DBL1H fraction:    r0:DBL1L  */
+LOCAL(inf_nan_denorm_or_zero_b):
+	sub	r3,r1		! 0x7ff00000
+	mov.l	@r15,r2		! get original DBL0H
+	tst	r1,DBL1H
+	sub	r3,r0		! isolate high fraction
+	bf	LOCAL(inf_nan_b)
+	mov.l	DBL1H,@r15
+	mov	r0,DBL0H
+	mov.l	r8,@-r15
+	mov	r2,DBL1H
+	mov.l	LOCAL(0xffff0000),r2
+	mov.l	DBL1H,@-r15
+	mov	DBL1L,r1
+	mov	DBL0L,DBL1L
+	bra	LOCAL(normalize_arg)
+	mov	r1,DBL0L
+
+LOCAL(d20):
+	.word	20
+LOCAL(m15):
+	.word	-15
+LOCAL(m31):
+	.word	-31
+LOCAL(xff):
+	.word	0xff
+
+	.balign	4
+LOCAL(0xffff0000): .long 0xffff0000
+
+	! calculate a (DBL0H:DBL0L) * b (DBL1H:DBL1L)
+	.balign	4
+GLOBAL(muldf3):
+	mov.l	LOCAL(xfff00000),r3
+	mov	DBL1H,r0
+	dmulu.l	DBL0L,DBL1L
+	mov.l	LOCAL(x7fe00000),r1
+	sub	r3,r0
+	mov.l	DBL0H,@-r15
+	sub	r3,DBL0H
+	tst	r1,DBL0H
+	or	r3,DBL0H
+	mov.l	LOCAL(x001fffff),r2
+	bt	LOCAL(inf_nan_denorm_or_zero_a)
+	tst	r1,r0
+	or	r3,r0		! r0:DBL1L    := b fraction ; u12.52
+	bt	LOCAL(inf_nan_denorm_or_zero_b) ! T clear on fall-through
+LOCAL(arg_denorm_done):
+	and	r2,r0		! r0:DBL1L    := b fraction ; u12.52
+	sts	macl,r3
+	sts	mach,r1
+	dmulu.l	DBL0L,r0 ! r0 = DBL1H - exp
+	and	r2,DBL0H	! DBL0H:DBL0L := a fraction ; u12.52
+	mov.l	r8,@-r15
+	mov	#0,DBL0L
+	mov.l	r9,@-r15
+	sts	macl,r2
+	sts	mach,r8
+	dmulu.l	DBL0H,DBL1L
+	addc	r1,r2
+
+	addc	DBL0L,r8	! add T; clears T
+
+	sts	macl,r1
+	sts	mach,DBL1L
+	dmulu.l	DBL0H,r0
+	addc	r1,r2
+	mov.l	LOCAL(x7ff00000),DBL0H
+	addc	DBL1L,r8	! clears T
+	mov.l	@(8,r15),DBL1L	! a sign/exp w/fraction
+	sts	macl,DBLRL
+	sts	mach,DBLRH
+	and	DBL0H,DBL1L	! a exponent
+	mov.w	LOCAL(x200),r9
+	addc	r8,DBLRL
+	mov.l	LOCAL(x3ff00000),r8	! bias
+	addc	DBL0L,DBLRH	! add T
+	cmp/hi	DBL0L,r3	! 32 guard bits -> sticky: T := r3 != 0
+	movt	r3
+	tst	r9,DBLRH	! T := fraction < 2
+	or	r3,r2		! DBLRH:DBLRL:r2 := result fraction; u24.72
+	bt/s	LOCAL(shll12)
+	sub	r8,DBL1L
+	mov.l	LOCAL(x002fffff),r8
+	and	DBL1H,DBL0H	! b exponent
+	mov.l	LOCAL(x00100000),r9
+	add	DBL0H,DBL1L ! result exponent - 1
+	tst	r8,r2
+	mov.w	LOCAL(m20),r8
+	subc	DBL0L,r9
+	addc	r2,r9 ! r2 value is still needed for denormal rounding
+	mov.w	LOCAL(d11),DBL0L
+	rotcr	r9
+	clrt
+	shld	r8,r9
+	mov.w	LOCAL(m21),r8
+	mov	DBLRL,r3
+	shld	DBL0L,DBLRL
+	addc	r9,DBLRL
+	mov.l	@r15+,r9
+	shld	r8,r3
+	mov.l	@r15+,r8
+	shld	DBL0L,DBLRH
+	mov.l	@r15+,DBL0H
+	addc	r3,DBLRH
+	mov.l	LOCAL(x7ff00000),DBL0L
+	add	DBL1L,DBLRH	! implicit 1 adjusts exponent
+	mov.l	LOCAL(xffe00000),r3
+	cmp/hs	DBL0L,DBLRH
+	add	DBLRH,DBLRH
+	bt	LOCAL(ill_exp_11)
+	tst	r3,DBLRH
+	bt	LOCAL(denorm_exp0_11)
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+
+
+LOCAL(shll12):
+	mov.l	LOCAL(x0017ffff),r8
+	extu.b	DBLRH,DBLRH	! remove implicit 1.
+	mov.l	LOCAL(x00080000),r9
+	and	DBL1H,DBL0H	! b exponent
+	add	DBL0H,DBL1L	! result exponent
+	tst	r8,r2		! rounding adjust for lower guard ...
+	mov.w	LOCAL(m19),r8
+	subc	DBL0L,r9	! ... bits and round to even; clear T
+	addc	r2,r9 ! r2 value is still needed for denormal rounding
+	mov.w	LOCAL(d12),DBL0L
+	rotcr	r9
+	clrt
+	shld	r8,r9
+	mov.w	LOCAL(m20),r8
+	mov	DBLRL,r3
+	shld	DBL0L,DBLRL
+	addc	r9,DBLRL
+	mov.l	@r15+,r9
+	shld	r8,r3
+	mov.l	@r15+,r8
+	shld	DBL0L,DBLRH
+	mov.l	LOCAL(x7ff00000),DBL0L
+	addc	r3,DBLRH
+	mov.l	@r15+,DBL0H
+	add	DBL1L,DBLRH
+	mov.l	LOCAL(xffe00000),r3
+	cmp/hs	DBL0L,DBLRH
+	add	DBLRH,DBLRH
+	bt	LOCAL(ill_exp_12)
+	tst	r3,DBLRH
+	bt	LOCAL(denorm_exp0_12)
+LOCAL(insert_sign):
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+
+LOCAL(overflow):
+	mov	r3,DBLRH
+	mov	#0,DBLRL
+	bra	LOCAL(insert_sign)
+	mov.l	@r15+,r8
+
+LOCAL(denorm_exp0_11):
+	mov.l	r8,@-r15
+	mov	#-21,r8
+	mov.l	r9,@-r15
+	bra	LOCAL(denorm)
+	mov	#-2,DBL1L	! one for denormal, and one for sticky bit
+
+LOCAL(ill_exp_11):
+	mov	DBL1H,DBL1L
+	and	r3,DBL0L	! 0x7fe00000
+	add	DBL1L,DBL1L
+	mov.l	r8,@-r15
+	cmp/hi	DBL1L,DBL0L	! check if exp a was large
+	mov	#-20,DBL0L
+	bf	LOCAL(overflow)
+	mov	#-21,r8
+	mov	DBLRH,DBL1L
+	rotcr	DBL1L		! shift in negative sign
+	mov.l	r9,@-r15
+	shad	DBL0L,DBL1L	! exponent ; s32
+	bra	LOCAL(denorm)
+	add	#-2,DBL1L	! add one for denormal, and one for sticky bit
+
+LOCAL(denorm_exp0_12):
+	mov.l	r8,@-r15
+	mov	#-20,r8
+	mov.l	r9,@-r15
+	bra	LOCAL(denorm)
+	mov	#-2,DBL1L	! one for denormal, and one for sticky bit
+
+	.balign 4		! also aligns LOCAL(denorm)
+LOCAL(ill_exp_12):
+	and	r3,DBL0L	! 0x7fe00000
+	mov	DBL1H,DBL1L
+	add	DBL1L,DBL1L
+	mov.l	r8,@-r15
+	cmp/hi	DBL1L,DBL0L	! check if exp a was large
+	bf	LOCAL(overflow)
+	mov	DBLRH,DBL1L
+	rotcr	DBL1L		! shift in negative sign
+	mov	#-20,r8
+	shad	r8,DBL1L	! exponent ; s32
+	mov.l	r9,@-r15
+	add	#-2,DBL1L	! add one for denormal, and one for sticky bit
+LOCAL(denorm):
+	not	r3,r9		! 0x001fffff
+	mov.l	r10,@-r15
+	mov	r2,r10
+	shld	r8,r10	! 11 or 12 lower bit valid
+	and	r9,DBLRH ! Mask away vestiges of exponent.
+	add	#32,r8
+	sub	r3,DBLRH ! Make leading 1 explicit.
+	shld	r8,r2	! r10:r2 := unrounded result lowpart
+	shlr	DBLRH	! compensate for doubling at end of normal code
+	sub	DBLRL,r10	! reconstruct effect of previous rounding
+	exts.b	r10,r9
+	shad	r3,r10	! sign extension
+	mov	#0,r3
+	clrt
+	addc	r9,DBLRL	! Undo previous rounding.
+	bt LOCAL(unround_done)
+	addc	r9,DBLRH
+LOCAL(unround_done):
+	mov.w	LOCAL(m32),r9
+	cmp/hi	r3,r2
+	rotcl	DBLRL	! fit in the rest of r2 as a sticky bit.
+	mov.l	@r15+,r10
+	rotcl	DBLRH
+	cmp/ge	r9,DBL1L
+	bt	LOCAL(small_norm_shift)
+	cmp/hi	r3,DBLRL
+	add	#31,DBL1L
+	movt	DBLRL
+	shll 	DBLRH
+	cmp/ge	r9,DBL1L
+	or	DBLRH,DBLRL
+	bt/s	LOCAL(small_norm_shift)
+	mov	r3,DBLRH
+	mov	r3,DBLRL	! exponent too negative to shift - return zero
+	mov.l	@r15+,r9
+	mov.l	@r15+,r8
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+	.balign	4
+LOCAL(small_norm_shift):
+	mov	DBLRL,r2	! stash away guard bits
+	shld	DBL1L,DBLRL
+	mov	DBLRH,DBL0L
+	shld	DBL1L,DBLRH
+	mov.l	LOCAL(x7fffffff),r9
+	add	#32,DBL1L
+	shld	DBL1L,r2
+	shld	DBL1L,DBL0L
+	or	DBL0L,DBLRL
+	or	DBLRL,DBL0L
+	shlr	DBL0L
+	addc	r2,r9
+	mov.l	@r15+,r9
+	mov.l	@r15+,r8
+	addc	r3,DBLRL
+	addc	r3,DBLRH
+	div0s	DBL0H,DBL1H
+	add	DBLRH,DBLRH
+	rts
+	rotcr	DBLRH
+
+
+LOCAL(x200):
+	.word 0x200
+LOCAL(m19):
+	.word	-19
+LOCAL(m20):
+	.word	-20
+LOCAL(m21):
+	.word	-21
+LOCAL(m32):
+	.word	-32
+LOCAL(d11):
+	.word	11
+LOCAL(d12):
+	.word	12
+	.balign	4
+LOCAL(x60000000):
+	.long	0x60000000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+LOCAL(xfff00000):
+	.long	0xfff00000
+LOCAL(x7fffffff):
+	.long	0x7fffffff
+LOCAL(x00100000):
+	.long	0x00100000
+LOCAL(x7fe00000):
+	.long	0x7fe00000
+LOCAL(x001fffff):
+	.long	0x001fffff
+LOCAL(x7ff00000):
+	.long	0x7ff00000
+LOCAL(x3ff00000):
+	.long	0x3ff00000
+LOCAL(x002fffff):
+	.long	0x002fffff
+LOCAL(xffe00000):
+	.long	0xffe00000
+LOCAL(x0017ffff):
+	.long	0x0017ffff
+LOCAL(x00080000):
+	.long	0x00080000
+ENDFUNC(GLOBAL(muldf3))
--- gcc/libgcc/config/sh/IEEE-754/m3/floatsidf.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/m3/floatsidf.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,103 @@
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! floatsidf for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+
+FUNC(GLOBAL(floatsidf))
+	.global GLOBAL(floatsidf)
+	.balign	4
+GLOBAL(floatsidf):
+	tst	r4,r4
+	mov	r4,r1
+	bt	LOCAL(ret0)
+	cmp/pz	r4
+	bt	0f
+	neg	r4,r1
+0:	mov.l	LOCAL(c_clz_tab),r0
+	extu.w	r1,r5
+	mov.w	LOCAL(xff00),r3
+	cmp/eq	r1,r5
+	mov	#21,r2
+	bt	0f
+	mov	r1,r5
+	shlr16	r5
+	add	#-16,r2
+0:	tst	r3,r5	! 0xff00
+	bt	0f
+	shlr8	r5
+0:	bt	0f
+	add	#-8,r2
+0:
+#ifdef	__PIC__
+	add	r0,r5
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r5),r5
+	cmp/pz	r4
+	mov.l	LOCAL(x41200000),r3	! bias + 20 - implicit 1
+	bt	0f
+	mov.l	LOCAL(xc1200000),r3	! sign + bias + 20 - implicit 1
+0:	mov	r1,r0	! DBLRL & DBLRH
+	sub	r5,r2
+	mov	r2,r5
+	shld	r2,DBLRH
+	cmp/pz	r2
+	add	r3,DBLRH
+	add	#32,r2
+	shld	r2,DBLRL
+	bf	0f
+	mov.w	LOCAL(d0),DBLRL
+0:	mov	#20,r2
+	shld	r2,r5
+	rts
+	sub	r5,DBLRH
+LOCAL(ret0):
+	mov	#0,DBLRL
+	rts
+	mov	#0,DBLRH
+
+LOCAL(xff00):	.word 0xff00
+	.balign	4
+LOCAL(x41200000):
+#ifdef __LITTLE_ENDIAN__
+LOCAL(d0):	  .word 0
+		  .word 0x4120
+#else
+		  .word 0x4120
+LOCAL(d0):	  .word 0
+#endif
+LOCAL(xc1200000): .long 0xc1200000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(floatsidf))
--- gcc/libgcc/config/sh/IEEE-754/m3/fixdfsi.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/m3/fixdfsi.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,113 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!! fixdfsi for Renesas SH / STMicroelectronics ST40 CPUs
+!! Contributed by J"orn Rennecke joern.rennecke@st.com
+
+	! What is a bit unusal about this implementation is that the
+	! sign bit influences the result for NANs: for cleared sign bit, you
+	! get UINT_MAX, for set sign bit, you get 0.
+	! However, since the result for NANs is undefined, this should be no
+	! problem.
+	! N.B. This is scheduled both for SH4-200 and SH4-300
+	.balign 4
+	.global GLOBAL(fixdfsi)
+	FUNC(GLOBAL(fixdfsi))
+	.balign	4
+GLOBAL(fixdfsi):
+	mov.w	LOCAL(x413),r1
+	mov	DBL0H,r0
+	shll	DBL0H
+	mov.l	LOCAL(mask),r3
+	mov	#-21,r2
+	shld	r2,DBL0H	! SH4-200 will start this insn in a new cycle
+	bt/s	LOCAL(neg)
+	sub	r1,DBL0H
+	cmp/pl	DBL0H		! SH4-200 will start this insn in a new cycle
+	and	r3,r0
+	bf/s	LOCAL(ignore_low)
+	addc	r3,r0	! uses T == 1; sets implict 1
+	mov	#10,r2
+	shld	DBL0H,r0	! SH4-200 will start this insn in a new cycle
+	cmp/gt	r2,DBL0H
+	add	#-32,DBL0H
+	bt	LOCAL(retmax)
+	shld	DBL0H,DBL0L
+	rts
+	or	DBL0L,r0
+
+	.balign	8
+LOCAL(ignore_low):
+	mov	#-21,r2
+	cmp/gt	DBL0H,r2	! SH4-200 will start this insn in a new cycle
+	bf	0f		! SH4-200 will start this insn in a new cycle
+	mov	#-31,DBL0H	! results in 0 return
+0:	add	#1,r0
+	rts
+	shld	DBL0H,r0
+
+	.balign 4
+LOCAL(neg):
+	cmp/pl	DBL0H
+	and	r3,r0
+	bf/s	LOCAL(ignore_low_neg)
+	addc	r3,r0	! uses T == 1; sets implict 1
+	mov	#10,r2
+	shld	DBL0H,r0	! SH4-200 will start this insn in a new cycle
+	cmp/gt	r2,DBL0H
+	add	#-32,DBL0H
+	bt	LOCAL(retmin)
+	shld	DBL0H,DBL0L
+	or	DBL0L,r0	! SH4-200 will start this insn in a new cycle
+	rts
+	neg	r0,r0
+
+	.balign 4
+LOCAL(ignore_low_neg):
+	mov	#-21,r2
+	cmp/gt	DBL0H,r2	! SH4-200 will start this insn in a new cycle
+	add	#1,r0
+	shld	DBL0H,r0
+	bf	0f
+	mov	#0,r0		! results in 0 return
+0:	rts
+	neg	r0,r0
+
+LOCAL(retmax):
+	mov	#-1,r0
+	rts
+	shlr	r0
+
+LOCAL(retmin):
+	mov	#1,r0
+	rts
+	rotr	r0
+
+LOCAL(x413): .word 0x413
+
+	.balign 4
+LOCAL(mask): .long 0x000fffff
+	ENDFUNC(GLOBAL(fixdfsi))
--- gcc/libgcc/config/sh/IEEE-754/divdf3.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/divdf3.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,598 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!division of two double precision floating point numbers
+!Author:Aanchal Khanna
+!
+!Entry:
+!r4,r5:dividend
+!
+!r6,r7:divisor
+!
+!Exit:
+!r0,r1:quotient
+
+!Notes: dividend is passed in regs r4 and r5 and divisor is passed in regs 
+!r6 and r7, quotient is returned in regs r0 and r1. dividend is referred as op1
+!and divisor as op2.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align	5
+	.global	GLOBAL (divdf3)
+	FUNC (GLOBAL (divdf3))
+
+GLOBAL (divdf3):
+
+#ifdef  __LITTLE_ENDIAN__
+	mov	r4,r1
+	mov	r5,r4
+	mov	r1,r5
+
+        mov     r6,r1
+        mov     r7,r6
+        mov     r1,r7
+#endif
+	mov	r4,r2
+	mov.l	.L_inf,r1
+
+	and	r1,r2
+	mov.l   r8,@-r15
+
+	cmp/eq	r1,r2
+	mov     r6,r8
+
+	bt	.L_a_inv
+	and	r1,r8
+
+	cmp/eq	r1,r8
+	mov.l	.L_high_mant,r3
+
+	bf	.L_chk_zero
+	and	r6,r3
+
+	mov.l   .L_mask_sign,r8	
+	cmp/pl	r7
+
+	mov	r8,r0
+	bt	.L_ret_b	!op2=NaN,return op2
+
+	and	r4,r8
+	cmp/pl	r3
+
+	and	r6,r0
+	bt	.L_ret_b	!op2=NaN,return op2
+
+	xor     r8,r0           !op1=normal no,op2=Inf, return Zero
+	mov     #0,r1
+	
+#ifdef __LITTLE_ENDIAN__
+	mov	r0,r2
+	mov	r1,r0
+	mov	r2,r1
+#endif
+	rts
+	mov.l   @r15+,r8
+
+.L_ret_b:
+	mov	r7,r1
+	mov     r6,r0
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts
+	mov.l   @r15+,r8
+
+.L_a_inv:
+	!chk if op1 is Inf or NaN
+	mov.l   .L_high_mant,r2
+	cmp/pl  r5
+
+	and	r4,r2
+	bt	.L_ret_a
+
+	and	r1,r8		!r1 contains infinity
+	cmp/pl	r2
+
+	bt	.L_ret_a
+	cmp/eq	r1,r8
+
+	mov	r1,DBLRH
+	add	DBLRH,DBLRH
+	bf	0f
+	mov	#-1,DBLRH	! Inf/Inf, return NaN.
+0:	div0s	r4,r6
+	mov.l   @r15+,r8	
+	rts
+	rotcr	DBLRH
+
+.L_ret_a:
+	!return op1
+	mov	r5,r1
+	mov	r4,r0
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+        mov.l   @r15+,r8
+
+.L_chk_zero:
+	!chk if op1=0
+	mov.l   .L_mask_sign,r0
+        mov     r4,r3
+
+        and     r0,r3
+        shll    r4
+
+        and     r6,r0
+        shlr    r4
+
+        xor     r3,r0
+        shll    r6
+
+	shlr	r6
+	tst	r4,r4
+
+
+	bf      .L_op1_not_zero	
+	tst	r5,r5
+	
+        bf      .L_op1_not_zero
+	tst	r7,r7
+
+	mov.l   @r15+,r8
+	bf	.L_ret_zero
+
+	tst	r6,r6
+	bf	.L_ret_zero
+
+	rts
+	mov     #-1,DBLRH       !op1=op2=0, return NaN
+	
+.L_ret_zero:
+	!return zero
+	mov	r0,r1
+	rts
+#ifdef __LITTLE__ENDIAN
+	mov	#0,r0
+#else
+	mov	#0,r1		!op1=0,op2=normal no,return zero
+#endif
+
+.L_norm_b:
+	!normalize op2
+        shll    r7
+        mov.l   .L_imp_bit,r3
+
+        rotcl   r6
+        tst     r3,r6
+
+        add     #-1,r8
+        bt      .L_norm_b
+
+        bra     .L_divide
+        add     #1,r8
+
+.L_op1_not_zero:
+	!op1!=0, chk if op2=0
+	tst	r7,r7	
+	mov	r1,r3
+	
+	mov	#0,r1
+	bf	.L_normal_nos
+
+	tst	r6,r6
+	bf      .L_normal_nos
+
+	mov.l   @r15+,r8
+	or	r3,r0
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts
+	nop
+
+.L_normal_nos:
+	!op1 and op2 are normal nos
+	tst	r2,r2
+	mov	#-20,r1
+
+! The subsequent branch is for the upper compare
+! Shifting will not alter the result, for the
+! macro is declared with care.
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld    r1,r2
+#else
+	SHLR20 (r2)
+#endif
+	bt	.L_norm_a	!normalize dividend
+	
+.L_chk_b:
+	mov.l	r9,@-r15
+	tst	r8,r8
+
+        mov.l   .L_high_mant,r9
+
+! The subsequent branch is for the upper compare
+! Shifting will not alter the result, for the
+! macro is declared with care.
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r1,r8
+#else
+        SHLR20 (r8)
+#endif
+				! T set -> normalize divisor
+	SL(bt,	.L_norm_b,
+	 and	r9,r4)
+
+.L_divide:
+	mov.l   .L_2047,r1
+	sub	r8,r2
+
+	mov.l	.L_1023,r8
+	and	r9,r6
+
+	!resultant exponent
+	add	r8,r2
+	!chk the exponent for overflow
+	cmp/ge	r1,r2
+	
+	mov.l	.L_imp_bit,r1
+	bt	.L_overflow
+	
+	mov	#0,r8
+	or	r1,r4
+	
+	or      r1,r6	
+	mov	#-24,r3
+
+	!chk if the divisor is 1(mantissa only)
+	cmp/eq	r8,r7
+	bf	.L_div2
+
+	cmp/eq	 r6,r1
+	bt	.L_den_one
+
+.L_div2:
+	!divide the mantissas
+	shll8	r4
+	mov	r5,r9
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r3,r9
+#else
+        SHLR24 (r9)
+#endif
+	shll8	r6
+
+	or	r9,r4
+	shll8   r5
+
+	mov	r7,r9
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r3,r9
+#else
+        SHLR24 (r9)
+#endif
+	mov	r8,r3
+	shll8	r7
+
+	or	r9,r6	
+	cmp/gt	r4,r6
+
+	mov	r3,r9
+	bt	.L_shift
+
+	cmp/eq	r4,r6
+	bf	.L_loop
+
+	cmp/gt	r5,r7
+	bf	.L_loop
+
+.L_shift:
+	add	#-1,r2
+	shll	r5
+	rotcl	r4
+
+.L_loop:
+	!actual division loop
+	cmp/gt	r6,r4
+	bt	.L_subtract
+
+	cmp/eq	r6,r4
+	bf	.L_skip
+
+	cmp/ge	r7,r5
+	bf	.L_skip
+
+.L_subtract:
+	clrt
+	subc	r7,r5
+	
+	or	r1,r8
+	subc	r6,r4
+
+.L_skip:
+	shlr	r1
+	shll	r5
+
+	rotcl	r4
+	cmp/eq	r1,r3
+
+	bf	.L_loop
+	mov.l	.L_imp_bit,r1
+
+	!chk if the divison was for the higher word of the quotient
+	tst	r1,r9
+	bf	.L_chk_exp
+
+	mov	r8,r9
+	mov.l   .L_mask_sign,r1
+
+	!divide for the lower word of the quotient
+	bra	.L_loop
+	mov	r3,r8
+
+.L_chk_exp:
+	!chk if the result needs to be denormalized
+	cmp/gt	r2,r3
+	bf	.L_round
+	mov     #-53,r7
+
+.L_underflow:
+	!denormalize the result
+	add	#1,r2
+	cmp/gt	r2,r7
+
+	or      r4,r5           !remainder
+	add	#-2,r2
+
+	mov	#32,r4
+	bt      .L_return_zero
+
+	add	r2,r4
+	cmp/ge	r3,r4
+
+	mov	r2,r7
+	mov	r3,r1
+
+	mov     #-54,r2
+	bt	.L_denorm
+	mov	#-32,r7
+
+.L_denorm:
+	shlr	r8
+	rotcr	r1
+
+	shll	r8
+	add     #1,r7
+
+	shlr	r9
+	rotcr	r8
+
+	cmp/eq	r3,r7
+	bf	.L_denorm
+
+	mov	r4,r7
+	cmp/eq	r2,r4
+
+	bt	.L_break
+	mov     r3,r6
+
+	cmp/gt	r7,r3
+	bf	.L_break
+
+	mov	r2,r4
+	mov	r1,r6
+
+	mov	r3,r1
+	bt	.L_denorm
+
+.L_break:
+	mov     #0,r2
+
+	cmp/gt	r1,r2
+
+	addc	r2,r8
+	mov.l   .L_comp_1,r4
+
+	addc	r3,r9		
+	or	r9,r0
+
+	cmp/eq	r5,r3
+	bf	.L_return	
+
+	cmp/eq	r3,r6
+	mov.l	.L_mask_sign,r7
+
+	bf	.L_return
+	cmp/eq	r7,r1
+
+	bf	.L_return
+	and	r4,r8
+
+.L_return:
+	mov.l	@r15+,r9
+	mov     r8,r1
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l   @r15+,r8
+
+.L_norm_a:
+        !normalize op1
+        shll    r5
+        mov.l   .L_imp_bit,r3
+
+        rotcl   r4
+        tst     r3,r4
+
+        add     #-1,r2
+        bt      .L_norm_a
+
+        bra     .L_chk_b
+        add     #1,r2
+
+.L_overflow:
+	!overflow, return inf
+	mov.l   .L_inf,r2
+#ifdef __LITTLE_ENDIAN__
+	or	r2,r1	
+	mov	#0,r0
+#else
+	or	r2,r0
+	mov	#0,r1
+#endif
+        mov.l   @r15+,r9
+        rts
+        mov.l   @r15+,r8
+
+.L_den_one:
+	!denominator=1, result=numerator
+        mov     r4,r9
+        mov   	#-53,r7
+
+	cmp/ge	r2,r8
+	mov	r8,r4
+
+	mov	r5,r8
+	mov	r4,r3
+
+	!chk the exponent for underflow
+	SL(bt,	.L_underflow,
+	 mov     r4,r5)
+
+	mov.l	.L_high_mant,r7
+        bra     .L_pack
+	mov     #20,r6
+
+.L_return_zero:
+	!return zero
+	mov	r3,r1
+	mov.l	@r15+,r9
+
+	rts
+	mov.l   @r15+,r8
+
+.L_round:
+	!apply rounding
+	cmp/eq	r4,r6
+	bt	.L_lower
+
+	clrt
+	subc    r6,r4
+
+	bra     .L_rounding
+	mov	r4,r6
+	
+.L_lower:
+	clrt
+	subc	r7,r5
+	mov	r5,r6
+	
+.L_rounding:
+	!apply rounding
+	mov.l   .L_invert,r1
+	mov	r3,r4
+
+	movt	r3
+	clrt
+	
+	not	r3,r3
+	and	r1,r3	
+
+	addc	r3,r8
+	mov.l   .L_high_mant,r7
+
+	addc	r4,r9
+	cmp/eq	r4,r6
+
+	mov.l   .L_comp_1,r3
+	SL (bf,	.L_pack,
+	 mov     #20,r6)
+	and	r3,r8
+
+.L_pack:
+	!pack the result, r2=exponent,r0=sign,r8=lower mantissa, r9=higher mantissa
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld    r6,r2
+#else
+        SHLL20 (r2)
+#endif
+	and	r7,r9
+
+	or	r2,r0
+	mov	r8,r1
+
+	or      r9,r0
+	mov.l	@r15+,r9
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+	.align	2
+
+.L_mask_sign:
+	.long	0x80000000
+.L_high_mant:
+	.long	0x000fffff
+.L_inf:
+	.long	0x7ff00000
+.L_1023:
+	.long	1023
+.L_2047:
+	.long	2047
+.L_imp_bit:
+	.long	0x00100000	
+.L_comp_1:
+	.long	0xfffffffe
+.L_invert:
+	.long	0x00000001
+
+ENDFUNC (GLOBAL (divdf3))
--- gcc/libgcc/config/sh/IEEE-754/floatunssisf.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/floatunssisf.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,137 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Conversion of unsigned integer to floating point
+
+! Author: Rakesh Kumar
+
+! Argument: r4
+! Result: r0
+
+! r4 is referred as op1
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+        .text
+        .align 5
+        .global GLOBAL (floatunsisf)
+	FUNC (GLOBAL (floatunsisf))
+
+GLOBAL (floatunsisf):
+	tst	r4,r4
+	mov	#23,r6
+
+	mov.l	.L_set_24_bits,r7
+	SL(bt,	.L_return,
+	 not	r7,r3)
+
+	! Decide the direction for shifting
+	mov.l	.L_set_24_bit,r5
+	cmp/hi	r7,r4
+
+	not	r5,r2
+	SL(bt,	.L_shift_right,
+	 mov	#0,r7)
+
+	tst	r5,r4
+	
+	mov	#0,r0
+	bf	.L_pack_sf
+
+! Shift the bits to the left. Adjust the exponent
+.L_shift_left:
+	shll	r4
+	tst	r5,r4
+
+	add	#-1,r6
+	bt	.L_shift_left
+
+! Pack the value in floating point format.
+! r6 has unbiased exponent, r4 has mantissa
+.L_pack_sf:
+	mov	#23,r3
+	add	#127,r6
+
+	! Align the exponent
+	and	r2,r4
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+        SHLL23 (r6)
+#else
+	shld	r3,r6
+#endif
+
+	or	r6,r0
+	rts
+	or	r4,r0
+
+! Shift right the number with rounding
+.L_shift_right:
+	shlr	r4
+	rotcr	r7
+
+	tst	r4,r3
+	add	#1,r6
+
+	bf	.L_shift_right
+	
+	tst	r7,r7
+	bt	.L_sh_rt_1
+
+	shll	r7
+	movt	r1
+
+	add	r1,r4
+
+	tst	r7,r7
+	bf	.L_sh_rt_1
+
+	! Halfway between two numbers.
+	! Round towards LSB = 0
+	shlr	r4
+	shll	r4
+
+.L_sh_rt_1:
+	mov	r4,r0
+
+	! Rounding may have misplaced MSB. Adjust.
+	and	r3,r0
+	cmp/eq	#0,r0
+
+	bf	.L_shift_right
+	bt	.L_pack_sf
+
+.L_return:
+	rts
+	mov	r4,r0
+
+	.align 2
+.L_set_24_bit:
+	.long 0x00800000
+
+.L_set_24_bits:
+	.long 0x00FFFFFF
+
+ENDFUNC (GLOBAL (floatunsisf))
--- gcc/libgcc/config/sh/IEEE-754/fixunssfsi.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/fixunssfsi.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,155 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Conversion from floating point to unsigned integer
+
+! Author: Rakesh Kumar
+
+! Argument: r4 (in floating point format)
+! Result: r0
+
+! For negative floating point numbers, it returns zero
+
+! The argument is referred as op1
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align 5
+	.global	GLOBAL (fixunssfsi)
+	FUNC (GLOBAL (fixunssfsi))
+
+GLOBAL (fixunssfsi):
+	mov.l	.L_sign,r0
+	mov	r4,r2
+
+	! Check for NaN
+	mov.l	.L_inf,r1
+	and	r4,r0
+
+	mov.l	.L_mask_sign,r7
+	mov	#127,r5
+
+	! Remove sign bit
+	cmp/eq	#0,r0
+	and	r7,r2
+
+	! If number is negative, return 0
+	! LIBGCC deviates from standard in this regard.
+	mov	r4,r3
+	SL(bf,	.L_epil,
+	 mov	#0,r0)
+
+	mov.l	.L_frac,r6
+	cmp/gt	r1,r2
+
+	shll	r2
+	SL1(bt,	.L_epil,
+	 shlr16	r2)
+
+	shlr8	r2	! r2 has exponent
+	mov.l	.L_24bit,r1
+
+	and	r6,r3	! r3 has fraction
+	cmp/gt	r2,r5
+
+	! If exponent is less than 127, return 0
+	or	r1,r3
+	bt	.L_epil
+
+	! Process only if exponent is less than 158
+	mov.l	.L_158,r1
+	shll8	r3
+
+	cmp/gt	r1,r2
+	sub	r2,r1
+
+	neg	r1,r1
+	bt	.L_ret_max
+
+! Shift the mantissa with exponent difference from 158
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r1,r3
+#else
+	cmp/gt	r0,r1
+	bt	.L_mov_left
+
+.L_mov_right:
+	cmp/eq	r1,r0
+	bt	.L_ret
+
+	add	#1,r1
+	bra	.L_mov_right
+	shlr	r3
+
+.L_mov_left:
+	add	#-1,r1
+	
+	shll	r3
+	cmp/eq	r1,r0
+
+	bf	.L_mov_left
+
+.L_ret:	
+#endif
+	rts
+	mov	r3,r0
+
+! r0 already has appropriate value
+.L_epil:
+	rts
+	nop
+
+! Return the maximum unsigned integer value
+.L_ret_max:
+	mov.l	.L_max,r3
+
+	rts
+	mov	r3,r0
+
+	.align 2
+.L_inf:
+	.long 0x7F800000
+
+.L_158:
+	.long 158
+
+.L_max:
+	.long 0xFFFFFFFF
+
+.L_frac:
+	.long 0x007FFFFF
+
+.L_sign:
+	.long 0x80000000
+
+.L_24bit:
+	.long 0x00800000
+
+.L_mask_sign:
+	.long 0x7FFFFFFF
+
+ENDFUNC (GLOBAL (fixunssfsi))
--- gcc/libgcc/config/sh/IEEE-754/floatunssidf.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/floatunssidf.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,76 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!conversion of unsigned integer to double precision floating point number
+!Author:Rakesh Kumar
+!Rewritten for SH1 support: Joern Rennecke
+!
+!Entry:
+!r4:operand
+!
+!Exit:
+!r0,r1:result
+!
+!Note:argument is passed in reg r4 and the result is returned in
+!regs r0 and r1.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+        .text
+        .align 5
+        .global GLOBAL (floatunsidf)
+	FUNC (GLOBAL (floatunsidf))
+
+GLOBAL (floatunsidf):
+	mov.w	LOCAL(x41f0),DBLRH	! bias + 32
+	tst	r4,r4			! check for zero
+	bt	.L_ret_zero
+.L_loop:
+	shll	r4	
+	SL(bf,	.L_loop,
+	 add	#-16,DBLRH)
+
+	mov	r4,DBLRL
+
+        SHLL20 (DBLRL)
+
+        shll16	DBLRH ! put exponent in proper place
+
+        SHLR12 (r4)
+
+	rts
+	or	r4,DBLRH
+	
+.L_ret_zero:
+	mov	#0,r1
+	rts
+	mov	#0,r0
+
+LOCAL(x41f0):	.word	0x41f0
+	.align 2
+
+ENDFUNC (GLOBAL (floatunsidf))
--- gcc/libgcc/config/sh/IEEE-754/fixunsdfsi.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/fixunsdfsi.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,181 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!conversion of double precision floating point number to unsigned integer
+!Author:Aanchal Khanna
+!
+!Entry:
+!r4,r5:operand
+!
+!Exit:
+!r0:result
+!
+!Note:argument is passed in regs r4 and r5, the result is returned in
+!reg r0.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align 5
+	.global GLOBAL (fixunsdfsi)
+	FUNC (GLOBAL (fixunsdfsi))
+
+GLOBAL (fixunsdfsi):
+
+#ifdef  __LITTLE_ENDIAN__
+        mov     r4,r1
+        mov     r5,r4
+        mov     r1,r5
+#endif
+	mov.l	.L_p_inf,r2
+	mov     #-20,r1
+	
+	mov	r2,r7
+	mov.l   .L_1023,r3
+
+	and	r4,r2
+	shll    r4
+
+        movt    r6		! r6 contains the sign bit
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r1,r2           ! r2 contains the exponent
+#else
+        SHLR20 (r2)
+#endif
+	shlr    r4
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r1,r7
+#else
+        SHLR20 (r7)
+#endif
+	tst	r6,r6	
+	SL(bf,	.L_epil,
+	 mov	#0,r0)
+
+	cmp/hi	r2,r3		! if exp < 1023,return 0
+	mov.l	.L_high_mant,r1
+
+	SL(bt,	.L_epil,
+	 and	r4,r1)		! r1 contains high mantissa
+
+	cmp/eq	r2,r7		! chk if exp is invalid
+	mov.l	.L_1054,r7
+
+	bt	.L_inv_exp
+	mov	#11,r0
+	
+	cmp/hi	r7,r2		! If exp > 1054,return maxint
+	sub     r2,r7		!r7 contains the number of shifts
+
+	mov.l	.L_21bit,r2
+	bt	.L_ret_max
+
+	or	r2,r1
+	mov	r7,r3
+
+	shll8   r1
+	neg     r7,r7
+
+	shll2	r1
+
+        shll	r1
+	cmp/hi	r3,r0
+
+	SL(bt,	.L_lower_mant,
+	 mov	#21,r0)
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r7,r1
+#else
+.L_sh_loop:
+        tst	r7,r7
+        bt      .L_break
+        add     #1,r7
+        bra     .L_sh_loop
+        shlr    r1
+
+.L_break:
+#endif
+	rts
+	mov     r1,r0
+
+.L_lower_mant:
+	neg	r0,r0
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r0,r5
+#else
+        SHLR21 (r5)
+#endif
+	or	r5,r1		!pack lower and higher mantissas
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r7,r1
+#else
+.L_loop:
+        tst	r7,r7
+        bt      .L_break1
+        add     #1,r7
+        bra     .L_loop
+        shlr    r1
+
+.L_break1:
+#endif
+	mov	r1,r0
+.L_epil:
+	rts
+	nop
+
+.L_inv_exp:
+	cmp/hi	r0,r5
+	bt	.L_epil
+
+	cmp/hi	r0,r1		!compare high mantissa,r1
+	bt	.L_epil
+
+.L_ret_max:
+	mov.l   .L_maxint,r0
+
+	rts
+	nop
+
+	.align	2
+
+.L_maxint:
+	.long	0xffffffff
+.L_p_inf:
+	.long	0x7ff00000
+.L_high_mant:
+	.long	0x000fffff
+.L_1023:
+	.long	0x000003ff
+.L_1054:
+	.long	1054
+.L_21bit:
+	.long	0x00100000
+
+ENDFUNC (GLOBAL (fixunsdfsi))
--- gcc/libgcc/config/sh/IEEE-754/addsf3.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/addsf3.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,535 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Add floating point numbers in r4, r5.
+
+! Author: Rakesh Kumar
+
+! Arguments are in r4, r5 and result in r0
+
+! Entry points: ___subsf3, ___addsf3
+
+! r4 and r5 are referred as op1 and op2
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align 5
+        .global GLOBAL (subsf3)
+	.global	GLOBAL (addsf3)
+	FUNC (GLOBAL (subsf3))
+	FUNC (GLOBAL (addsf3))
+
+GLOBAL (subsf3):
+        mov.l   .L_sign_bit,r1
+        xor     r1,r5
+
+GLOBAL (addsf3):
+	mov.l	r8,@-r15
+	mov	r4,r3
+
+	mov.l	.L_pinf,r2
+	mov	#0,r8
+
+	and	r2,r3 ! op1's exponent.
+	mov	r5,r6
+
+	! Check NaN or Infinity
+	and	r2,r6 ! op2's exponent.
+	cmp/eq	r2,r3
+
+	! go if op1 is NaN or INF. 
+	mov.l	.L_sign_bit,r0
+	SL(bt,	.L_inv_op1,
+	 mov	#-23,r1)
+	
+	! Go if op2 is NaN/INF.
+	cmp/eq	r2,r6
+	mov	r0,r7
+	bt	.L_ret_op2
+
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLR23 (r3)
+#else
+	shld	r1,r3
+#endif
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLR23 (r6)
+#else
+	shld	r1,r6
+#endif
+
+	! Check for negative zero
+	cmp/eq	r0,r5
+
+	mov	r5,r1
+	SL(bt,	.L_ret_op1,
+	 and	r7,r1)
+
+	cmp/eq	r0,r4
+	bt	.L_ret_op2
+
+	! if op1 is zero return op2
+	tst	r4,r4
+	bt	.L_ret_op2
+
+	! Equal numbers with opposite sign
+	mov	r4,r2
+	xor	r5,r2
+
+	cmp/eq	r0,r2
+	bt	.L_ret_zero
+
+	! if op2 is zero return op1
+	mov.l	.L_mask_fra,r2
+	tst	r5,r5
+
+	! Extract the mantissa
+	mov	r4,r0
+	SL(bt,	.L_ret_op1,
+	 and	r2,r5)
+
+	and	r2,r4
+
+	mov.l	.L_imp_bit,r2
+	and	r7,r0	! sign bit of op1
+
+	! Check for denormals
+	tst	r3,r3
+	bt	.L_norm_op1
+
+	! Attach the implicit bit
+	or	r2,r4
+	tst	r6,r6
+
+	bt	.L_norm_op2
+
+	or	r2,r5
+	tst	r0,r0
+
+	! operands are +ve or -ve??
+	bt	.L_ptv_op1
+
+	neg	r4,r4
+
+.L_ptv_op1:
+	tst	r1,r1
+	bt	.L_ptv_op2
+
+	neg	r5,r5
+
+! Test exponents for equality
+.L_ptv_op2:
+	cmp/eq	r3,r6
+	bt	.L_exp_eq
+
+! Make exponents of two arguments equal
+.L_exp_ne:
+	! r0, r1 contain sign bits.
+	! r4, r5 contain mantissas.
+	! r3, r6 contain exponents.
+	! r2, r7 scratch.
+
+	! Calculate result exponent.
+	mov	r6,r2
+	sub	r3,r2	! e2 - e1
+
+	cmp/pl	r2
+	mov	#23,r7
+
+	! e2 - e1 is -ve
+	bf	.L_exp_ne_1
+
+	mov	r6,r3 ! Result exp.
+	cmp/gt	r7,r2 ! e2-e1 > 23
+
+	mov	#1,r7
+	bt	.L_pack_op2_0
+
+	! Align the mantissa
+.L_loop_ne:
+	shar	r4
+
+	rotcr	r8
+	cmp/eq	r7,r2
+
+	add	#-1,r2
+	bf	.L_loop_ne
+
+	bt	.L_exp_eq
+
+! Exponent difference is too high.
+! Return op2 after placing pieces in proper place
+.L_pack_op2_0:
+	! If op1 is -ve
+	tst	r1,r1
+	bt	.L_pack_op2
+
+	neg	r5,r5
+
+! r6 has exponent
+! r5 has mantissa, r1 has sign
+.L_pack_op2:
+	mov.l	.L_nimp_bit,r2
+	mov	#23,r3
+
+	mov	r1,r0
+	
+	and	r2,r5
+	mov.l	@r15+,r8
+
+	or	r5,r0
+
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLL23 (r6)
+#else
+	shld	r3,r6
+#endif
+        rts
+	or	r6,r0
+
+! return op1. It is NAN or INF or op2 is zero.
+.L_ret_op1:
+	mov	r4,r0
+
+	rts
+	mov.l	@r15+,r8
+
+! return zero
+.L_ret_zero:
+	mov	#0,r0
+
+	rts
+	mov.l	@r15+,r8
+
+! return op2. It is NaN or INF or op1 is zero.
+.L_ret_op2:
+	mov	r5,r0
+
+	rts
+	mov.l	@r15+,r8
+
+! op2 is denormal. Normalize it.
+.L_norm_op2:
+	shll	r5
+	add	#-1,r6
+
+	tst	r2,r5
+	bt	.L_norm_op2
+
+	! Check sign
+	tst	r1,r1
+	bt	.L_norm_op2_2
+
+	neg	r5,r5
+
+.L_norm_op2_2:
+	add	#1,r6
+	cmp/eq	r3,r6
+
+	bf	.L_exp_ne
+	bt	.L_exp_eq
+
+! Normalize op1
+.L_norm_op1:
+	shll	r4
+	add	#-1,r3
+
+	tst	r2,r4
+	bt	.L_norm_op1
+
+	! Check sign
+	tst	r0,r0
+	bt	.L_norm_op1_1
+
+	neg	r4,r4
+
+.L_norm_op1_1:
+	! Adjust biasing
+	add	#1,r3
+
+	! Check op2 for denormalized value
+	tst	r6,r6
+	bt	.L_norm_op2
+
+	mov.l	.L_imp_bit,r2
+
+	tst	r1,r1	! Check sign
+	or	r2,r5	! Attach 24th bit
+
+	bt	.L_norm_op1_2
+
+	neg	r5,r5
+
+.L_norm_op1_2:
+	cmp/eq	r3,r6
+
+	bt	.L_exp_eq
+	bf	.L_exp_ne
+
+! op1 is NaN or Inf
+.L_inv_op1:
+	! Return op1 if it is NAN. 
+	! r2 is infinity
+	cmp/gt	r2,r4
+	bt	.L_ret_op1
+
+	! op1 is +/- INF
+	! If op2 is same return now.
+	cmp/eq	r4,r5
+	bt	.L_ret_op1
+
+	! return op2 if it is NAN
+	cmp/gt	r2,r5
+	bt	.L_ret_op2
+
+	! Check if op2 is inf
+	cmp/eq	r2,r6
+	bf	.L_ret_op1
+	
+	! Both op1 and op2 are infinities 
+	!of opp signs, or there is -NAN. Return a NAN.
+	mov.l	@r15+,r8
+	rts
+	mov	#-1,r0
+
+! Make unequal exponents equal.
+.L_exp_ne_1:
+	mov	#-25,r7
+	cmp/gt	r2,r7 ! -23 > e2 - e1
+
+	add	#1,r2
+	bf	.L_exp_ne_2
+
+	tst	r0,r0
+	bt	.L_pack_op1
+
+.L_pack_op1_0:
+	bra	.L_pack_op1
+	neg	r4,r4
+
+! Accumulate the shifted bits in r8
+.L_exp_ne_2:
+	! Shift with rounding
+	shar	r5
+	rotcr	r8
+
+	tst	r2,r2
+
+	add	#1,r2
+	bf	.L_exp_ne_2
+
+! Exponents of op1 and op2 are equal (or made so)
+! The mantissas are in r4-r5 and remaining bits in r8
+.L_exp_eq:
+	add	r5,r4 ! Add fractions.
+	mov.l	.L_sign_bit,r2
+
+	! Check for negative result
+	mov	#0,r0
+	tst	r2,r4
+
+	mov.l	.L_255,r5
+	bt	.L_post_add
+
+	negc	r8,r8
+	negc	r4,r4
+	or	r2,r0
+
+.L_post_add:
+	! Check for extra MSB
+	mov.l	.L_chk_25,r2
+
+	tst	r2,r4
+	bt	.L_imp_check
+
+	shar 	r4
+	rotcr	r8
+
+	add	#1,r3
+	cmp/ge	r5,r3
+
+	! Return Inf if exp > 254
+	bt	.L_ret_inf
+
+! Check for implicit (24th) bit in result
+.L_imp_check:
+        mov.l	.L_imp_bit,r2
+	tst	r2,r4
+
+	bf	.L_pack_op1
+
+! Result needs left shift
+.L_lft_shft:
+	shll	r8
+	rotcl	r4
+
+	add	#-1,r3
+	tst	r2,r4
+
+	bt	.L_lft_shft
+	
+! Pack the result after rounding
+.L_pack_op1:
+	! See if denormalized result is possible 
+	mov.l	.L_chk_25,r5
+	cmp/pl	r3
+
+	bf	.L_denorm_res
+
+	! Are there any bits shifted previously?
+	tst	r8,r8
+	bt	.L_pack_1
+
+	! Round
+	shll	r8
+	movt	r6
+
+	add	r6,r4
+
+	! If we are halfway between two numbers,
+	! round towards LSB = 0
+	tst	r8,r8
+
+	bf	.L_pack_1
+
+	shlr	r4
+	shll	r4
+
+.L_pack_1:
+	! Adjust extra MSB generated after rounding
+	tst	r4,r5
+	mov.l	.L_255,r2
+
+	bt	.L_pack_2
+	shar	r4
+
+	add	#1,r3 
+	cmp/ge	r2,r3	! Check for exp overflow
+
+	bt	.L_ret_inf
+	
+! Pack it finally
+.L_pack_2:
+	! Do not store implicit bit
+	mov.l	.L_nimp_bit,r2
+	mov	#23,r1
+
+	and	r2,r4
+
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLL23 (r3)
+#else
+	shld	r1,r3
+#endif
+	mov.l	@r15+,r8
+
+	or	r4,r0
+        rts
+	or	r3,r0
+
+! Return infinity
+.L_ret_inf:
+	mov.l	.L_pinf,r2
+
+	mov.l	@r15+,r8
+	rts
+	or	r2,r0
+
+! Result must be denormalized
+.L_denorm_res:
+	mov	#0,r2
+	
+! Denormalizing loop with rounding
+.L_den_1:
+	shar	r4
+	movt	r6
+
+	tst	r3,r3
+	bt	.L_den_2
+
+	! Increment the exponent
+	add	#1,r3
+
+	tst	r6,r6
+	bt	.L_den_0
+
+	! Count number of ON bits shifted
+	add	#1,r2
+
+.L_den_0:
+	bra	.L_den_1
+	nop
+
+! Apply rounding
+.L_den_2:
+	cmp/eq	r6,r1
+	bf	.L_den_3
+
+	add	r6,r4
+	mov	#1,r1
+
+	! If halfway between two numbers,
+	! round towards LSB = 0
+	cmp/eq	r2,r1
+	bf	.L_den_3
+
+	shar	r4
+	shll	r4
+
+.L_den_3:
+
+	mov.l	@r15+,r8
+	rts
+	or	r4,r0
+	
+	.align 2
+.L_imp_bit:
+        .long   0x00800000
+
+.L_nimp_bit:
+	.long	0xFF7FFFFF
+
+.L_mask_fra:
+        .long   0x007FFFFF
+
+.L_pinf:
+        .long   0x7F800000
+
+.L_sign_bit:
+	.long	0x80000000
+
+.L_bit_25:
+	.long	0x01000000
+
+.L_chk_25:
+        .long   0x7F000000
+
+.L_255:
+	.long	0x000000FF
+
+ENDFUNC (GLOBAL (addsf3))
+ENDFUNC (GLOBAL (subsf3))
--- gcc/libgcc/config/sh/IEEE-754/adddf3.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/adddf3.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,799 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Routine for adding two double numbers
+
+! Author: Rakesh Kumar
+! SH1 Support by Joern Rennecke
+! Sticky Bit handling : Joern Rennecke
+
+! Arguments: r4-r5, r6-r7
+! Result: r0-r1
+
+! The value in r4-r5 is referred to as op1
+! and that in r6-r7 is referred to as op2
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+        .align 5
+	.global	GLOBAL (subdf3)
+	FUNC (GLOBAL (subdf3))
+        .global GLOBAL (adddf3)
+	FUNC (GLOBAL (adddf3))
+
+GLOBAL (subdf3):
+#ifdef __LITTLE_ENDIAN__
+	mov	r4,r1
+	mov	r6,r2
+
+	mov	r5,r4
+	mov	r7,r6
+
+	mov	r1,r5
+	mov	r2,r7
+#endif
+	mov.l	.L_sign,r2
+	bra	.L_adddf3_1
+	xor	r2,r6
+
+GLOBAL (adddf3):
+#ifdef	__LITTLE_ENDIAN__
+	mov	r4,r1
+	mov	r6,r2
+
+	mov	r5,r4
+	mov	r7,r6
+
+	mov	r1,r5
+	mov	r2,r7
+#endif
+	
+.L_adddf3_1:
+	mov.l	r8,@-r15
+	mov	r4,r1
+
+	mov.l 	.L_inf,r2
+	mov	r6,r3
+
+	mov.l	r9,@-r15
+	and	r2,r1		!Exponent of op1 in r1
+
+	mov.l	r10,@-r15
+	and	r2,r3		!Exponent of op2 in r3
+
+	! Check for Nan or Infinity
+	mov.l	.L_sign,r9
+	cmp/eq	r2,r1
+
+	mov	r9,r10
+	bt	.L_thread_inv_exp_op1
+
+	mov	r9,r0
+	cmp/eq	r2,r3
+! op1 has a valid exponent. We need not check it again.
+! Return op2 straight away.
+	and	r4,r9		!r9 has sign bit for op1
+	bt	.L_ret_op2
+
+	! Check for -ve zero
+	cmp/eq	r4,r0
+	and	r6,r10		!r10 has sign bit for op2
+
+	bt	.L_op1_nzero
+
+	cmp/eq	r6,r0
+	bt	.L_op2_nzero
+
+! Check for zero
+.L_non_zero:
+	tst	r4,r4
+	bt	.L_op1_zero
+
+	! op1 is not zero, check op2 for zero
+	tst	r6,r6
+	bt	.L_op2_zero
+
+! r1 and r3 has masked out exponents, r9 and r10 has signs
+.L_add:
+	mov.l	.L_high_mant,r8
+	mov	#-20,r2
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r2,r1		! r1 now has exponent for op1 in its lower bits
+#else
+	SHLR20 (r1)
+#endif
+	and	r8,r6	! Higher bits of mantissa of op2
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r2,r3		! r3 has exponent for op2 in its lower bits
+#else
+	SHLR20 (r3)
+#endif
+	and	r8,r4	! Higher bits of mantissa of op1
+
+	mov.l	.L_21bit,r8
+
+	tst	r1,r1
+	bt	.L_norm_op1
+
+	! Set the 21st bit.
+	or	r8,r4
+	tst	r3,r3
+
+	bt	.L_norm_op2
+	or	r8,r6
+
+! Check for negative mantissas. Make them positive by negation
+! r9 and r10 have signs of op1 and op2 respectively
+.L_neg_mant:
+	tst	r9,r9
+	bf	.L_neg_op1
+
+	tst	r10,r10
+	bf	.L_neg_op2
+
+.L_add_1:
+	cmp/ge	r1,r3
+
+	mov	r1,r0
+	bt	.L_op2_exp_greater
+
+	sub	r3,r0
+	! If exponent difference is greater than 54, the resultant exponent
+	! won't be changed. Return op1 straight away.
+	mov	#54,r2
+	cmp/gt	r2,r0
+
+	bt	.L_pack_op1
+
+	mov	r1,r3
+	clrt
+
+	cmp/eq	#0,r0
+	bt	.L_add_mant
+
+	! Shift left the first operand and apply rest of shifts to second operand.
+	mov	#0,r2
+	shll	r5
+
+	rotcl	r4
+
+	add	#-1,r3
+	dt	r0
+
+	bt	.L_add_mant
+	dt	r0
+
+	bt	LOCAL(got_guard)
+	dt	r0
+
+	bt	LOCAL(got_sticky)
+
+! Shift the mantissa part of op2 so that both exponents are equal
+.L_shfrac_op2:
+	shar	r6
+	or	r7,r2	! sticky bit
+
+	rotcr	r7
+	dt	r0
+
+	bf	.L_shfrac_op2
+
+	shlr	r2
+
+	subc	r2,r2	! spread sticky bit across r2
+LOCAL(got_sticky):
+	shar	r6
+
+	rotcr	r7
+
+	rotcr	r2
+LOCAL(got_guard):
+	shar	r6
+
+	rotcr	r7
+
+	rotcr	r2
+
+
+! Add the psotive mantissas and check for overflow by checking the
+! MSB of the resultant. In case of overflow, negate the result.
+.L_add_mant:
+	clrt
+	addc	r7,r5
+
+	mov	#0,r10	! Assume resultant to be positive
+	addc	r6,r4
+
+	cmp/pz	r4
+
+	bt	.L_mant_ptv
+	negc	r2,r2
+
+	negc	r5,r5
+
+	mov.l	.L_sign,r10 ! The assumption was wrong, result is negative
+	negc	r4,r4
+
+! 23rd bit in the high part of mantissa could be set.
+! In this case, right shift the mantissa.
+.L_mant_ptv:
+	mov.l	.L_23bit,r0
+
+	tst	r4,r0
+	bt	.L_mant_ptv_0
+
+	shlr	r4
+	rotcr	r5
+
+	add	#1,r3
+	bra	.L_mant_ptv_1
+	rotcr	r2
+
+.L_mant_ptv_0:
+	mov.l	.L_22bit,r0
+	tst	r4,r0
+
+	bt	.L_norm_mant
+
+.L_mant_ptv_1:
+	! 22 bit of resultant mantissa is set. Shift right the mantissa
+	! and add 1 to exponent
+	add	#1,r3
+	shlr	r4
+	rotcr	r5
+	! The mantissa is already normalized. We don't need to
+	! spend any effort. Branch to epilogue. 
+	bra	.L_epil
+	rotcr	r2
+
+! Normalize operands
+.L_norm_op1:
+	shll	r5
+
+	rotcl	r4
+	add	#-1,r1
+
+	tst	r4,r8
+	bt	.L_norm_op1
+
+	tst	r3,r3
+	SL(bf,	.L_neg_mant,
+	 add	#1,r1)
+
+.L_norm_op2:
+	shll	r7
+
+	rotcl	r6
+	add	#-1,r3
+
+	tst	r6,r8
+	bt	.L_norm_op2
+
+	bra	.L_neg_mant
+	add	#1,r3
+
+! Negate the mantissa of op1
+.L_neg_op1:
+	clrt
+	negc	r5,r5
+
+	negc	r4,r4
+	tst	r10,r10
+
+	bt	.L_add_1
+
+! Negate the mantissa of op2
+.L_neg_op2:
+	clrt
+	negc	r7,r7
+
+	bra	.L_add_1
+	negc	r6,r6
+
+! Thread the jump to .L_inv_exp_op1
+.L_thread_inv_exp_op1:
+	bra	.L_inv_exp_op1
+	nop
+
+.L_ret_op2:
+	mov.l	@r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r6,r1
+#else
+	mov	r6,r0
+#endif
+
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r7,r0
+#else
+	mov	r7,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+
+.L_op1_nzero:
+	tst	r5,r5
+	bt	.L_ret_op2
+
+	! op1 is not zero. Check op2 for negative zero
+	cmp/eq	r6,r0
+	bf	.L_non_zero	! both op1 and op2 are not -0
+
+.L_op2_nzero:
+	tst	r7,r7
+	bf	.L_non_zero
+
+	mov.l	@r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r4,r1
+#else
+	mov	r4,r0	! op2 is -0, return op1
+#endif
+
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r5,r0
+#else
+	mov	r5,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+
+! High bit of op1 is known to be zero.
+! Check low bit. r2 contains 0x00000000
+.L_op1_zero:
+	tst	r5,r5
+	bt	.L_ret_op2
+
+	! op1 is not zero. Check high bit of op2
+	tst	r6,r6
+	bf	.L_add	! both op1 and op2 are not zero
+
+! op1 is not zero. High bit of op2 is known to be zero.
+! Check low bit of op2. r2 contains 0x00000000
+.L_op2_zero:
+	tst	r7,r7
+	bf	.L_add
+
+	mov.l	@r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r4,r1
+#else
+	mov	r4,r0	! op2 is zero, return op1
+#endif
+
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r5,r0
+#else
+	mov	r5,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+
+! exp (op1) is smaller or equal to exp (op2)
+! The logic of same operations is present in .L_add. Kindly refer it for
+! comments
+.L_op2_exp_greater:
+	mov	r3,r0
+	sub	r1,r0
+
+	mov	#54,r2
+	cmp/gt	r2,r0
+
+	bt	.L_pack_op2
+
+	cmp/eq	#0,r0
+	bt	.L_add_mant
+
+	mov	#0,r2
+	shll	r7
+	rotcl	r6
+	add	#-1,r0
+	add	#-1,r3
+
+	cmp/eq	#0,r0
+	bt	.L_add_mant
+.L_shfrac_op1:	
+        add     #-1,r0
+        shar    r4
+
+	rotcr	r5
+	rotcr	r2
+
+        cmp/eq  #0,r0
+        bf      .L_shfrac_op1
+
+	bra	.L_add_mant
+	nop
+
+! Return the value in op1
+.L_ret_op1:
+        mov.l   @r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r4,r1
+#else
+        mov     r4,r0
+#endif
+
+        mov.l   @r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r5,r0
+#else
+        mov     r5,r1
+#endif
+
+        rts
+        mov.l   @r15+,r8
+
+! r1 has exp, r9 has sign, r4 and r5 mantissa
+.L_pack_op1:
+	mov.l	.L_high_mant,r7
+	mov	r4,r0
+
+	tst	r9,r9
+	bt	.L_pack_op1_1
+
+	clrt
+	negc	r5,r5
+	negc	r0,r0
+
+.L_pack_op1_1:
+	and	r7,r0
+	mov	r1,r3
+
+	mov	#20,r2
+	mov	r5,r1
+
+	mov.l	@r15+,r10
+	or	r9,r0
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r2,r3
+#else
+	SHLL20 (r3)
+#endif
+	mov.l	@r15+,r9
+
+	or	r3,r0
+#ifdef	__LITTLE_ENDIAN__
+	mov	r0,r2
+	mov	r1,r0
+	mov	r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+!r2 has exp, r10 has sign, r6 and r7 mantissa
+.L_pack_op2:
+	mov.l	.L_high_mant,r9
+	mov	r6,r0
+
+	tst	r10,r10
+	bt	.L_pack_op2_1
+
+	clrt
+	negc	r7,r7
+	negc	r0,r0
+
+.L_pack_op2_1:
+	and	r9,r0
+	mov	r7,r1
+
+	mov	#20,r2
+	or	r10,r0
+
+	mov.l	@r15+,r10
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r2,r3
+#else
+	SHLL20 (r3)
+#endif
+
+	mov.l	@r15+,r9
+
+	or	r3,r0
+#ifdef	__LITTLE_ENDIAN__
+	mov	r0,r2
+	mov	r1,r0
+	mov	r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+! Normalize the mantissa by setting its 21 bit in high part
+.L_norm_mant:
+	mov.l	.L_21bit,r0
+
+	tst	r4,r0
+	bf	.L_epil
+
+	tst	r4,r4
+	bf	.L_shift_till_1
+
+	tst	r5,r5
+	bf	.L_shift_till_1
+
+	! Mantissa is zero, return 0
+	mov.l	@r15+,r10
+	mov	#0,r0
+
+	mov.l	@r15+,r9
+	mov.l	@r15+,r8
+
+	rts
+	mov	#0,r1
+
+! A loop for making the 21st bit 1 in high part of resultant mantissa
+! It is already ensured that 1 bit is present in the mantissa
+.L_shift_till_1:
+	clrt
+	shll	r5
+
+	rotcl	r4
+	add	#-1,r3
+
+	tst	r4,r0
+	bt	.L_shift_till_1
+
+! Return the result. Mantissa is in r4-r5. Exponent is in r3
+! Sign bit in r10
+.L_epil:
+	cmp/pl	r3
+
+	bf	.L_denorm
+	mov.l	LOCAL(x7fffffff),r0
+
+	mov	r5,r1
+	shlr	r1
+
+	mov	#0,r1
+	addc	r0,r2
+
+! Check extra MSB here
+	mov.l	.L_22bit,r9
+	addc	r1,r5	! round to even
+
+	addc	r1,r4
+	tst	r9,r4
+
+	bf	.L_epil_1
+
+.L_epil_0:
+	mov.l	.L_21bit,r1
+
+	not	r1,r1
+	and	r1,r4
+
+	mov	r4,r0
+	or	r10,r0
+
+	mov.l	@r15+,r10
+	mov	#20,r2
+
+	mov.l	@r15+,r9
+	mov	r5,r1
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r2,r3
+#else
+	SHLL20 (r3)
+#endif
+	or	r3,r0
+
+#ifdef	__LITTLE_ENDIAN__
+	mov	r0,r2
+	mov	r1,r0
+	mov	r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+.L_epil_1:
+	shlr	r4
+	add	#1,r3
+	bra	.L_epil_0
+	rotcr	r5
+
+.L_denorm:
+	add	#-1,r3
+.L_denorm_1:
+	tst	r3,r3
+	bt	.L_denorm_2
+
+	shlr	r4
+	rotcr	r5
+
+	movt	r1
+	bra	.L_denorm_1
+	add	#1,r3
+
+.L_denorm_2:
+	clrt
+	mov	#0,r2
+	addc	r1,r5
+
+	addc	r2,r4
+	mov	r4,r0
+
+	or	r10,r0
+	mov.l	@r15+,r10
+
+	mov	r5,r1
+
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r0,r2
+	mov	r1,r0
+	mov	r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+! op1 is known to be positive infinity, and op2 is Inf. The sign
+! of op2 is not known. Return the appropriate value
+.L_op1_pinf_op2_inf:
+	mov.l	.L_sign,r0
+	tst	r6,r0
+
+	bt	.L_ret_op2_1
+
+	! op2 is negative infinity. Inf - Inf is being performed
+	mov.l	.L_inf,r0
+	mov.l	@r15+,r10
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r0,r1
+#endif
+	mov.l	@r15+,r8
+
+	rts
+#ifdef	__LITTLE_ENDIAN__
+	mov	#1,r0
+#else
+	mov	#1,r1	! Any value here will return Nan
+#endif
+	
+.L_ret_op1_1:
+        mov.l   @r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r4,r1
+#else
+        mov     r4,r0
+#endif
+
+        mov.l   @r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r5,r0
+#else
+        mov     r5,r1
+#endif
+
+        rts
+        mov.l   @r15+,r8
+
+.L_ret_op2_1:
+	mov.l	@r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r6,r1
+#else
+	mov	r6,r0
+#endif
+
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r7,r0
+#else
+	mov	r7,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+
+! op1 is negative infinity. Check op2 for infinity or Nan
+.L_op1_ninf:
+	cmp/eq	r2,r3
+	bf	.L_ret_op1_1	! op2 is neither Nan nor Inf
+
+	mov.l	@r15+,r9
+	div0s	r4,r6		! different signs -> NaN
+	mov	r4,DBLRH
+	or	r6,DBLRH
+	mov.l	@r15+,r8
+	SL(bf, 0f,
+	 mov	r5,DBLRL)
+	mov	#-1,DBLRH	! return NaN.
+0:	rts
+	or	r7,DBLRL
+
+!r1 contains exponent for op1, r3 contains exponent for op2
+!r2 has .L_inf (+ve Inf)
+!op1 has invalid exponent. Either it contains Nan or Inf
+.L_inv_exp_op1:
+	! Check if a is Nan
+	cmp/pl	r5
+	bt	.L_ret_op1_1
+
+	mov.l	.L_high_mant,r0
+	and	r4,r0
+
+	cmp/pl	r0
+	bt	.L_ret_op1_1
+
+	! op1 is not Nan. It is infinity. Check the sign of it.
+	! If op2 is Nan, return op2
+	cmp/pz	r4
+
+	bf	.L_op1_ninf
+
+	! op2 is +ve infinity here
+	cmp/eq	r2,r3
+	bf	.L_ret_op1_1	! op2 is neither Nan nor Inf
+
+	! r2 is free now
+	mov.l	.L_high_mant,r0
+	tst	r6,r0		! op2 also has invalid exponent
+
+	bf	.L_ret_op2_1	! op2 is Infinity, and op1 is +Infinity
+
+	tst	r7,r7
+	bt	.L_op1_pinf_op2_inf	! op2 is Infinity, and op1 is +Infinity
+	!op2 is not infinity, It is Nan
+	bf	.L_ret_op2_1
+
+	.align 2	
+.L_high_mant:
+	.long 0x000FFFFF
+
+.L_21bits:
+	.long 0x001FFFFF
+
+.L_22bit:
+	.long 0x00200000
+
+.L_23bit:
+	.long 0x00400000
+
+.L_21bit:
+	.long 0x00100000
+
+.L_sign:
+	.long 0x80000000
+
+.L_inf:
+	.long 0x7ff00000
+
+LOCAL(x7fffffff): .long 0x7fffffff
+
+ENDFUNC (GLOBAL (subdf3))
+ENDFUNC (GLOBAL (adddf3))
--- gcc/libgcc/config/sh/IEEE-754/mulsf3.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/mulsf3.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,352 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Routine for multiplying two floating point numbers
+
+! Author: Rakesh Kumar
+
+! Arguments: r4 and r5
+! Result: r0
+
+! The arguments are referred as op1 and op2
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+        .text
+        .align 5
+        .global GLOBAL (mulsf3)
+        FUNC (GLOBAL (mulsf3))
+
+GLOBAL (mulsf3):
+	! Extract the sign bits
+	mov.l	.L_sign,r3
+	mov	r3,r0
+
+	and	r4,r3		! sign bit for op1
+	mov.l	.L_sign_mask,r6
+
+	! Mask out the sign bit from op1 and op2
+	and	r5,r0		! sign bit for op2
+	mov.l	.L_inf,r2
+
+	and	r6,r4
+	xor	r3,r0		! Final sign in r0
+
+	and	r6,r5
+	tst	r4,r4
+
+	! Check for zero
+	mov	r5,r7
+	! Check op1 for zero
+	SL(bt,	.L_op1_zero,
+	 mov	r4,r6)
+
+	tst	r5,r5
+	bt	.L_op2_zero	! op2 is zero
+
+	! Extract the exponents
+	and	r2,r6		! Exponent of op1
+	cmp/eq	r2,r6
+
+	and	r2,r7
+	bt	.L_inv_op1	! op1 is NaN or Inf
+
+	mov.l	.L_mant,r3
+	cmp/eq	r2,r7
+
+	and	r3,r4	! Mantissa of op1
+	bt	.L_ret_op2	! op2 is Nan or Inf
+
+	and	r3,r5	! Mantissa of op2
+
+	mov	#-23,r3
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLR23 (r6)
+	SHLR23 (r7)
+#else
+	shld	r3,r6
+	shld	r3,r7
+#endif
+	! Check for denormals
+	mov.l	.L_24bit,r3
+	tst	r6,r6
+
+	bt	.L_norm_op1	! op1 is denormal
+	add	#-127,r6	! Unbias op1's exp
+
+	tst	r7,r7
+	bt	.L_norm_op2	! op2 is denormal
+
+	add	#-127,r7	! Unbias op2's exp
+
+.L_multiply:
+	add	r6,r7	! Final exponent in r7
+	mov.l	.L_24bit,r1
+
+	! set 24th bit of mantissas
+	mov	#127,r3
+	or	r1,r4
+
+	DMULU_SAVE
+
+	! Multiply
+	or	r1,r5
+	DMULUL	(r4,r5,r4)
+
+	DMULUH	(r5)
+
+	DMULU_RESTORE
+
+	mov.l	.L_16bit,r6
+
+	! Check for extra MSB generated
+	tst	r5,r6
+
+	mov.l	.L_255,r1
+	bf	.L_shift_by_1	! Adjust the extra MSB
+	
+! Normalize the result with rounding
+.L_epil:
+	! Bias the exponent
+	add	#127,r7
+	cmp/ge	r1,r7
+	
+	! Check exponent overflow and underflow
+	bt	.L_ret_inf
+
+	cmp/pl	r7
+	bf	.L_denorm
+
+.L_epil_0:
+	mov	#-23,r3
+	shll	r5
+	mov	#0,r6
+
+! Fit resultant mantissa in 24 bits
+! Apply default rounding
+.L_loop_epil_0:
+        tst	r3,r3
+	bt	.L_loop_epil_out
+
+	add	#1,r3
+	shlr	r4
+
+	bra	.L_loop_epil_0
+	rotcr	r6
+
+! Round mantissa
+.L_loop_epil_out:
+	shll8	r5
+	or	r5,r4
+
+	mov.l	.L_mant,r2
+	mov	#23,r3
+
+	! Check last bit shifted out of result
+	tst	r6,r6
+	bt	.L_epil_2
+
+	! Round
+	shll	r6
+	movt	r5
+
+	add	r5,r4
+
+	! If this is the only ON bit shifted
+	! Round towards LSB = 0
+	tst	r6,r6
+	bf	.L_epil_2
+
+	shlr	r4
+	shll	r4
+
+.L_epil_2:
+	! Rounding may have produced extra MSB.
+	mov.l	.L_25bit,r5
+	tst	r4,r5
+
+	bt	.L_epil_1
+
+	add	#1,r7
+	shlr	r4
+
+.L_epil_1:
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLL23 (r7)
+#else
+	shld	r3,r7
+#endif
+
+	and	r2,r4
+
+	or	r7,r4
+	rts
+	or	r4,r0
+
+.L_denorm:
+	mov	#0,r3
+
+.L_den_1:
+	shlr	r5
+	rotcr	r4
+
+	cmp/eq	r3,r7
+	bt	.L_epil_0
+
+	bra	.L_den_1
+	add	#1,r7
+	
+
+! Normalize the first argument
+.L_norm_op1:
+	shll	r4
+	tst	r3,r4
+
+	add	#-1,r6
+	bt	.L_norm_op1
+
+	! The biasing is by 126
+	add	#-126,r6
+	tst	r7,r7
+
+	bt      .L_norm_op2
+
+	bra	.L_multiply
+	add	#-127,r7
+
+! Normalize the second argument
+.L_norm_op2:
+	shll	r5
+	tst	r3,r5
+
+	add	#-1,r7
+	bt	.L_norm_op2
+
+	bra	.L_multiply
+	add	#-126,r7
+
+! op2 is zero. Check op1 for exceptional cases
+.L_op2_zero:
+	mov.l	.L_inf,r2
+	and	r2,r6
+
+	! Check if op1 is deterministic
+	cmp/eq	r2,r6
+	SL(bf,	.L_ret_op2,
+	 mov	#1,r1)
+
+	! Return NaN
+	rts
+	mov	#-1,r0
+
+! Adjust the extra MSB
+.L_shift_by_1:
+	shlr	r5
+	rotcr	r4
+
+	add	#1,r7		! Show the shift in exponent
+
+	cmp/gt	r3,r7
+	bf	.L_epil
+
+	! The resultant exponent is invalid
+	mov.l	.L_inf,r1
+	rts
+	or	r1,r0
+
+.L_ret_op1:
+	rts
+	or	r4,r0
+
+! op1 is zero. Check op2 for exceptional cases
+.L_op1_zero:
+	mov.l	.L_inf,r2
+	and	r2,r7
+	
+	! Check if op2 is deterministic
+	cmp/eq	r2,r7
+	SL(bf,	.L_ret_op1,
+	 mov	#1,r1)
+
+	! Return NaN
+	rts
+	mov	#-1,r0
+
+.L_inv_op1:
+	mov.l	.L_mant,r3
+	mov	r4,r6
+
+	and	r3,r6
+	tst	r6,r6
+
+	bf	.L_ret_op1	! op1 is Nan
+	! op1 is not Nan. It is Inf
+
+	cmp/eq	r2,r7
+	bf	.L_ret_op1	! op2 has a valid exponent
+
+! op2 has a invalid exponent. It could be Inf, -Inf, Nan.
+! It doesn't make any difference.
+.L_ret_op2:
+	rts
+	or	r5,r0
+
+.L_ret_inf:
+	rts
+	or	r2,r0
+
+.L_ret_zero:
+	mov	#0,r2
+	rts
+	or	r2,r0
+
+	
+	.align 2
+.L_mant:
+	.long 0x007FFFFF
+
+.L_inf:
+	.long 0x7F800000
+
+.L_24bit:
+	.long 0x00800000
+
+.L_25bit:
+	.long 0x01000000
+
+.L_16bit:
+	.long 0x00008000
+
+.L_sign:
+	.long 0x80000000
+
+.L_sign_mask:
+	.long 0x7FFFFFFF
+
+.L_255:
+	.long 0x000000FF
+
+ENDFUNC (GLOBAL (mulsf3))
--- gcc/libgcc/config/sh/IEEE-754/floatsisf.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/floatsisf.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,200 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Conversion of integer to floating point
+
+! Author: Rakesh Kumar
+
+! Argument: r4
+! Result: r0
+
+! r4 is referred as op1
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+        .text
+        .align 5
+        .global GLOBAL (floatsisf)
+        FUNC (GLOBAL (floatsisf))
+
+GLOBAL (floatsisf):
+	mov.l	.L_sign,r2
+	mov	#23,r6
+
+	! Check for zero
+	tst	r4,r4
+	mov.l	.L_24_bits,r7
+
+	! Extract sign
+	and	r4,r2
+	bt	.L_ret
+
+	! Negative ???
+	mov.l	.L_imp_bit,r5
+	cmp/pl	r4
+
+	not	r7,r3
+	bf	.L_neg
+
+	! Decide the direction for shifting
+	cmp/gt	r7,r4
+	mov	r4,r0
+
+	and	r5,r0
+	bt	.L_shr_0
+
+	! Number may already be in normalized form
+	cmp/eq	#0,r0
+	bf	.L_pack
+
+! Shift the bits to the left. Adjust the exponent
+.L_shl:
+	shll	r4
+	mov	r4,r0
+
+	and	r5,r0
+	cmp/eq	#0,r0
+
+	SL(bt,	.L_shl,
+	 add	#-1,r6)
+
+! Pack the value in floating point format.
+! r6 has unbiased exponent, r4 has mantissa, r2 has sign
+.L_pack:
+	mov	#23,r3
+	not	r5,r5
+
+	mov	r2,r0
+	add	#127,r6
+
+	and	r5,r4
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLL23 (r6)
+#else
+	shld	r3,r6
+#endif
+
+	or	r6,r0
+	rts
+	or	r4,r0
+
+! Negate the number
+.L_neg:
+	! Take care for -2147483648.
+	mov	r4,r0
+	shll	r0
+	
+	cmp/eq	#0,r0
+	SL(bt,	.L_ret_min,
+	 neg	r4,r4)
+
+        cmp/gt  r7,r4
+        bt	.L_shr_0
+
+	mov	r4,r0
+	and	r5,r0
+
+	cmp/eq	#0,r0
+	bf	.L_pack
+	bt	.L_shl
+	
+.L_shr_0:
+	mov	#0,r1
+
+! Shift right the number with rounding
+.L_shr:
+	shlr	r4
+	movt	r7
+
+	tst	r7,r7
+
+	! Count number of ON bits shifted
+	bt	.L_shr_1
+	add	#1,r1
+
+.L_shr_1:
+	mov	r4,r0
+	add	#1,r6
+
+	and	r3,r0
+	cmp/eq	#0,r0
+
+	! Add MSB of shifted bits
+	bf	.L_shr
+	add	r7,r4
+
+	tst	r7,r7
+	bt	.L_pack
+
+.L_pack1:
+	mov	#1,r0
+	cmp/eq	r1,r0
+
+	bt	.L_rnd
+	mov	r4,r0
+
+	! Rounding may have misplaced MSB. Adjust.
+	and	r3,r0
+	cmp/eq	#0,r0
+
+	bf	.L_shr
+	bt	.L_pack
+
+! If only MSB of shifted bits is ON, we are halfway
+! between two numbers. Round towards even LSB of
+! resultant mantissa.
+.L_rnd:
+	shlr	r4
+	bra	.L_pack
+	shll	r4
+
+.L_ret:
+	rts
+	mov	r4,r0
+
+! Return value for -2147483648
+.L_ret_min:
+	mov.l	.L_min_val,r0
+	rts
+	nop
+
+	.align 2
+.L_sign:
+	.long 0x80000000
+
+.L_imp_bit:
+	.long 0x00800000
+
+.L_24_bits:
+	.long 0x00FFFFFF
+
+.L_nsign:
+	.long 0x7FFFFFFF
+
+.L_min_val:
+	.long 0xCF000000
+
+ENDFUNC (GLOBAL (floatsisf))
--- gcc/libgcc/config/sh/IEEE-754/muldf3.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/muldf3.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,601 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!multiplication of two double precision floating point numbers
+!Author:Aanchal Khanna
+!SH1 Support / Simplifications: Joern Rennecke
+!
+!Entry:
+!r4,r5:operand 1
+!
+!r6,r7:operand 2
+!
+!Exit:
+!r0,r1:result
+!
+!Notes: argument 1 is passed in regs r4 and r5 and argument 2 is passed in regs
+!r6 and r7, result is returned in regs r0 and r1. operand 1 is referred as op1
+!and operand 2 as op2.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+	.text
+	.align	5	
+	.global	GLOBAL (muldf3)
+	FUNC (GLOBAL (muldf3))
+
+GLOBAL (muldf3):
+
+#ifdef  __LITTLE_ENDIAN__
+        mov     r4,r1
+        mov     r5,r4
+        mov     r1,r5
+
+        mov     r6,r1
+        mov     r7,r6
+        mov     r1,r7
+#endif
+	mov.l	.L_mask_sign,r0
+	mov	r4,r2
+
+	and	r0,r2		
+	mov	#0,r1
+
+	shll	r4
+	and	r6,r0		
+	
+	xor     r2,r0		!r0 contains the result's sign bit
+	shlr	r4
+
+	mov.l   .L_inf,r2
+	shll	r6
+
+	mov	r4,r3
+	shlr	r6
+	
+.L_chk_a_inv:
+	!chk if op1 is Inf/NaN
+	and	r2,r3
+	mov.l	r8,@-r15
+
+	cmp/eq	r3,r2
+	mov.l	.L_mask_high_mant,r8
+
+	mov	r2,r3
+	bf	.L_chk_b_inv
+
+	mov	r8,r3
+	and	r4,r8
+
+	cmp/hi  r1,r8		
+	bt	.L_return_a	!op1 NaN, return op1
+
+	cmp/hi  r1,r5	
+	mov	r2,r8
+
+	bt      .L_return_a	!op1 NaN, return op1
+	and	r6,r8
+
+	cmp/eq	r8,r2		
+	and	r6,r3
+
+	bt      .L_b_inv
+	cmp/eq	r1,r6		
+
+	bf	.L_return_a	!op1 Inf,op2= normal no return op1
+	cmp/eq	r1,r7
+
+	bf	.L_return_a	!op1 Inf,op2= normal no return op1
+	mov.l   @r15+,r8	
+
+	rts
+	mov	#-1,DBLRH	!op1=Inf, op2=0,return nan
+
+.L_b_inv:
+	!op2 is NaN/Inf
+	cmp/hi	r1,r7
+	mov	r1,r2
+
+	mov	r5,r1
+	bt	.L_return_b	!op2=NaN,return op2
+
+	cmp/hi	r2,r6
+	or	r4,r0
+
+	bt	.L_return_b	!op2=NaN,return op2
+	mov.l   @r15+,r8
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts			!op1=Inf,op2=Inf,return Inf with sign
+	nop
+
+.L_chk_b_inv:
+	!Chk if op2 is NaN/Inf
+	and	r6,r2
+	cmp/eq	r3,r2
+
+	bf	.L_chk_a_for_zero
+	and	r6,r8
+
+	cmp/hi	r1,r8
+	bt	.L_return_b	 !op2=NaN,return op2
+
+	cmp/hi	r1,r7
+	bt	.L_return_b	 !op2=NaN,return op2
+
+	cmp/eq	r5,r1
+	bf      .L_return_b	 !op1=normal number,op2=Inf,return Inf
+
+	mov	r7,r1
+	cmp/eq	r4,r1
+
+	bf	.L_return_b	/* op1=normal number, op2=Inf,return Inf */
+	mov.l   @r15+,r8
+
+	rts
+	mov	#-1,DBLRH	!op1=0,op2=Inf,return NaN
+
+.L_return_a:
+	mov	r5,r1
+	or	r4,r0
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts
+	mov.l   @r15+,r8
+
+.L_return_b:
+	mov	r7,r1
+	or	r6,r0	
+	
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+	
+.L_chk_a_for_zero:
+	!Chk if op1 is zero
+	cmp/eq	r1,r4
+	bf	.L_chk_b_for_zero
+	
+	cmp/eq	r1,r5
+	bf	.L_chk_b_for_zero
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+.L_chk_b_for_zero:
+	!op1=0,chk if op2 is zero
+        cmp/eq  r1,r6
+        mov	r1,r3
+	
+	mov.l   .L_inf,r1
+	bf      .L_normal_nos
+
+        cmp/eq  r3,r7
+        bf      .L_normal_nos
+
+	mov	r3,r1
+	mov.l   @r15+,r8
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	nop
+
+.L_normal_nos:
+	!op1 and op2 are normal nos
+	mov.l	r9,@-r15
+	mov	r4,r3
+
+	mov     #-20,r9	
+	and	r1,r3	
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r9,r2
+#else
+        SHLR20 (r2)
+#endif
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r9,r3
+#else
+        SHLR20 (r3)
+#endif
+	cmp/pl	r3
+
+	bf	.L_norm_a	!normalize op1
+.L_chk_b:	
+	cmp/pl	r2
+	bf	.L_norm_b	!normalize op2
+
+.L_mul1:
+	add	r3,r2
+	mov.l  .L_1023,r1
+	
+	!resultant exponent in r2
+	add     r1,r2
+	mov.l   .L_2047,r1	
+
+	!Chk the exponent for overflow
+	cmp/ge	r1,r2
+	and     r8,r4
+
+	bt	.L_return_inf
+	mov.l	.L_imp_bit,r1
+	
+	or	r1,r4		
+	and	r8,r6
+
+	or	r1,r6
+	clrt
+
+	!multiplying the mantissas
+	DMULU_SAVE
+	DMULUL	(r7,r5,r1) 	!bits 0-31 of product 	
+
+	DMULUH	(r3)
+	
+	DMULUL	(r4,r7,r8)
+
+	addc	r3,r8
+
+	DMULUH	(r3)
+
+	movt	r9
+	clrt
+
+	DMULUL	(r5,r6,r7)
+
+	addc	r7,r8		!bits 63-32 of product
+
+	movt	r7
+	add	r7,r9
+
+	DMULUH	(r7)
+
+	add	r7,r3
+
+	add	r9,r3
+	clrt
+
+	DMULUL	(r4,r6,r7)
+
+	addc	r7,r3		!bits 64-95 of product
+
+	DMULUH	(r7)
+	DMULU_RESTORE
+	
+	mov	#0,r5
+	addc	r5,r7		!bits 96-105 of product
+
+	cmp/eq	r5,r1
+	mov     #1,r4
+
+	bt	.L_skip
+	or	r4,r8
+.L_skip:
+	mov.l   .L_106_bit,r4
+	mov	r8,r9
+
+.L_chk_extra_msb:
+	!chk if exra MSB is generated
+	and     r7,r4
+	cmp/eq	r5,r4
+
+	mov     #12,r4
+	SL(bf,	.L_shift_rt_by_1,
+	 mov     #31,r5)
+	
+.L_pack_mantissa:
+	!scale the mantissa t0 53 bits
+	mov	#-19,r6
+	mov.l	.L_mask_high_mant,r5
+
+        SHLRN (19, r6, r8)
+
+	and	r3,r5
+
+	shlr	r8
+	movt	r1
+
+        SHLLN (12, r4, r5)
+
+	add	#-1,r6
+
+	or	r5,r8		!lower bits of resulting mantissa
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r6,r3
+#else
+        SHLR20 (r3)
+#endif
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r4,r7
+#else
+        SHLL12 (r7)
+#endif
+	clrt
+
+	or	r7,r3		!higher bits of resulting mantissa
+	mov     #0,r7
+
+	!chk the exponent for underflow
+	cmp/ge	r2,r7
+	bt	.L_underflow
+
+	addc    r1,r8           !rounding
+	mov	r8,r1
+
+	addc	r7,r3		!rounding
+	mov.l	.L_mask_22_bit,r5
+
+	and	r3,r5
+	!chk if extra msb is generated after rounding
+	cmp/eq	r7,r5
+
+	mov.l	.L_mask_high_mant,r8
+	bt	.L_pack_result
+
+	add	#1,r2
+	mov.l	.L_2047,r6
+
+	cmp/ge	r6,r2
+
+	bt	.L_return_inf
+	shlr	r3
+
+	rotcr	r1
+
+.L_pack_result:
+	!pack the result, r2=exponent, r3=higher mantissa, r1=lower mantissa
+	!r0=sign bit
+	mov	#20,r6
+	and	r8,r3
+	
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r6,r2
+#else
+        SHLL20 (r2)
+#endif
+	or	r3,r0
+	
+	or      r2,r0
+	mov.l   @r15+,r9
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l   @r15+,r8
+
+.L_norm_a:
+	!normalize op1
+	shll	r5
+	mov.l	.L_imp_bit,r1
+
+	rotcl	r4
+	add	#-1,r3
+
+	tst	r1,r4
+	bt	.L_norm_a
+
+	bra	.L_chk_b
+	add	#1,r3
+
+.L_norm_b:
+	!normalize op2
+        shll    r7
+        mov.l   .L_imp_bit,r1
+
+        rotcl   r6
+        add     #-1,r2
+
+        tst     r1,r6
+        bt      .L_norm_b
+
+        bra     .L_mul1
+        add     #1,r2
+
+.L_shift_rt_by_1:
+	!adjust the extra msb
+
+	add     #1,r2           !add 1 to exponent
+	mov.l	.L_2047,r6
+
+	cmp/ge	r6,r2
+	mov	#20,r6
+
+	bt	.L_return_inf
+	shlr	r7		!r7 contains bit 96-105 of product
+
+	rotcr	r3		!r3 contains bit 64-95 of product
+
+	rotcr	r8		!r8 contains bit 32-63 of product
+	bra	.L_pack_mantissa
+
+	rotcr	r1		!r1 contains bit 31-0 of product
+
+.L_return_inf:
+	!return Inf
+	mov.l	.L_inf,r2
+	mov     #0,r1
+
+	or	r2,r0
+	mov.l   @r15+,r9
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l   @r15+,r8
+	
+.L_underflow:
+	!check if the result needs to be denormalized
+	mov	#-53,r1
+	add	#1,r2
+
+	cmp/gt	r2,r1
+	mov	#32,r4
+
+	add	#-2,r2
+	bt	.L_return_zero
+
+	add	r2,r4
+	mov	r7,r1
+	
+	cmp/ge	r7,r4
+	mov	r2,r6
+
+	mov	#-54,r2
+	bt	.L_denorm
+
+	mov	#-32,r6
+	
+.L_denorm:
+	!denormalize the result
+	shlr	r8
+	rotcr	r1	
+
+	shll	r8
+	add	#1,r6
+
+	shlr	r3
+	rotcr	r8
+
+	cmp/eq	r7,r6
+	bf	.L_denorm
+
+	mov	r4,r6
+	cmp/eq	r2,r4
+
+	bt	.L_break
+	mov	r7,r5
+
+	cmp/gt	r6,r7
+	bf	.L_break
+
+	mov	r2,r4
+	mov	r1,r5
+
+	mov	r7,r1
+	bt	.L_denorm
+
+.L_break:
+	mov	#0,r2
+
+	cmp/gt	r1,r2
+
+	addc	r2,r8
+	mov.l	.L_comp_1,r4
+	
+	addc	r7,r3
+	or	r3,r0
+
+	cmp/eq	r9,r7
+	bf	.L_return
+
+	cmp/eq	r7,r5
+	mov.l	.L_mask_sign,r6
+
+	bf	.L_return
+	cmp/eq	r1,r6
+	
+	bf	.L_return
+	and	r4,r8
+
+.L_return:
+	mov.l	@r15+,r9
+	mov	r8,r1
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l   @r15+,r8
+
+.L_return_zero:
+	mov.l	@r15+,r9
+	mov	r7,r1
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+
+	.align	2
+
+.L_mask_high_mant:
+	.long	0x000fffff
+.L_inf:
+	.long	0x7ff00000	
+.L_mask_sign:
+	.long	0x80000000
+.L_1023:
+	.long	-1023
+.L_2047:
+	.long	2047
+.L_imp_bit:
+	.long	0x00100000
+.L_mask_22_bit:
+	.long	0x00200000
+.L_106_bit:
+	.long	0x00000200
+.L_comp_1:
+	.long	0xfffffffe
+
+ENDFUNC (GLOBAL (muldf3))
--- gcc/libgcc/config/sh/IEEE-754/fixsfsi.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/fixsfsi.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,165 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Conversion routine for float to integer
+
+! Author: Rakesh Kumar
+
+! Arguments: r4 (in floating point format)
+! Return: r0
+
+! r4 is referred as op1
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align 5
+	.global	GLOBAL (fixsfsi)
+	FUNC (GLOBAL (fixsfsi))
+
+GLOBAL (fixsfsi):
+	mov.l	.L_mask_sign,r7
+	mov	r4,r2
+
+	! Check for NaN
+	mov.l	.L_inf,r1
+	and	r7,r2
+
+	cmp/gt	r1,r2
+	mov	#127,r5
+
+	mov	r4,r3
+	SL(bt,	.L_epil,
+	 mov	#0,r0)
+
+	shll	r2
+	mov.l	.L_frac,r6
+
+	shlr16	r2
+	and	r6,r3	! r3 has fraction
+
+	shlr8	r2	! r2 has exponent
+	mov.l	.L_24bit,r1
+
+	! If exponent is less than 127, return 0
+	cmp/gt	r2,r5
+	or	r1,r3	! Set the implicit bit
+
+	mov.l	.L_157,r1
+	SL1(bt,	.L_epil,
+	 shll8	r3)
+
+	! If exponent is greater than 157,
+	! return the maximum/minumum integer
+	! value deducing from sign
+	cmp/gt	r1,r2
+	sub	r2,r1
+
+	mov.l	.L_sign,r2
+	SL(bt,	.L_ret_max,
+	 add	#1,r1)
+
+	and	r4,r2	! Sign in r2
+	neg	r1,r1
+
+	! Shift mantissa by exponent difference from 157
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r1,r3
+#else
+        cmp/gt  r0,r1
+        bt      .L_mov_left
+
+.L_mov_right:
+        cmp/eq  r1,r0
+        bt      .L_ret
+
+        add     #1,r1
+        bra     .L_mov_right
+
+        shlr    r3
+
+.L_mov_left:
+        add     #-1,r1
+
+        shll    r3
+        cmp/eq  r1,r0
+
+        bf      .L_mov_left
+.L_ret:
+#endif
+	! If op1 is negative, negate the result
+	cmp/eq	r0,r2
+	SL(bf,	.L_negate,
+	 mov	r3,r0)
+
+! r0 has the appropriate value
+.L_epil:
+	rts
+	nop
+
+! Return the max/min integer value
+.L_ret_max:
+	and	r4,r2	! Sign in r2
+	mov.l	.L_max,r3
+
+	mov.l	.L_sign,r1
+	cmp/eq	r0,r2
+
+	mov	r3,r0
+	bt	.L_epil
+
+	! Negative number, return min int
+	rts
+	mov	r1,r0
+
+! Negate the result
+.L_negate:
+	rts
+	neg	r0,r0
+
+	.align 2
+.L_inf:
+	.long 0x7F800000
+
+.L_157:
+	.long 157
+
+.L_max:
+	.long 0x7FFFFFFF
+
+.L_frac:
+	.long 0x007FFFFF
+
+.L_sign:
+	.long 0x80000000
+
+.L_24bit:
+	.long 0x00800000
+
+.L_mask_sign:
+	.long 0x7FFFFFFF
+
+ENDFUNC (GLOBAL (fixsfsi))
--- gcc/libgcc/config/sh/IEEE-754/floatsidf.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/floatsidf.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,151 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!conversion of signed integer to double precision floating point number
+!Author:Rakesh Kumar
+!
+!Entry:
+!r4:operand 
+!
+!Exit:
+!r0,r1:result
+!
+!Note:argument is passed in reg r4 and the result is returned in 
+!regs r0 and r1.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+        .text
+        .align 5
+        .global GLOBAL (floatsidf)
+	FUNC (GLOBAL (floatsidf))
+
+GLOBAL (floatsidf):
+        mov.l   .L_sign,r0
+        mov     #0,r1
+
+	mov	r0,r2
+	tst	r4,r4 ! check r4 for zero
+
+	! Extract the sign
+	mov	r2,r3
+	SL(bt,	.L_ret_zero,
+	 and	r4,r0)
+
+	cmp/eq	r1,r0
+	not	r3,r3
+
+	mov	r1,r7
+	SL(bt,	.L_loop,
+	 and	r4,r3)
+
+	! Treat -2147483648 as special case
+	cmp/eq	r1,r3
+	neg	r4,r4
+
+	bt	.L_ret_min	
+
+.L_loop:
+	shll	r4	
+	mov	r4,r5
+
+	and	r2,r5
+	cmp/eq	r1,r5
+	
+	add	#1,r7
+	bt	.L_loop
+
+	mov.l	.L_initial_exp,r6
+	not	r2,r2
+	
+	and	r2,r4
+	mov	#21,r3
+
+	sub	r7,r6
+	mov	r4,r1
+
+	mov	#20,r7
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r3,r1
+#else
+        SHLL21 (r1)
+#endif
+	mov	#-11,r2
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r7,r6	! Exponent in proper place
+#else
+        SHLL20 (r6)
+#endif
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r2,r4
+#else
+        SHLR11 (r4)
+#endif
+	or	r6,r0
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+#ifdef __LITTLE_ENDIAN__
+	or	r4,r1
+#else
+	or	r4,r0
+#endif
+	
+.L_ret_zero:
+	rts
+	mov	#0,r0
+
+.L_ret_min:
+	mov.l	.L_min,r0
+	
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	nop
+
+	.align 2
+
+.L_initial_exp:
+	.long 0x0000041E
+
+.L_sign:
+	.long 0x80000000
+
+.L_min:
+	.long 0xC1E00000
+
+ENDFUNC (GLOBAL (floatsidf))
--- gcc/libgcc/config/sh/IEEE-754/fixdfsi.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/IEEE-754/fixdfsi.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,200 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!conversion of double precision floating point number to signed integer
+!Author:Aanchal Khanna
+!
+!Entry:
+!r4,r5:operand
+!
+!Exit:
+!r0:result
+!
+!Note:argument is passed in regs r4 and r5, the result is returned in
+!reg r0.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align 	5
+	.global GLOBAL (fixdfsi)
+	FUNC (GLOBAL (fixdfsi))
+
+GLOBAL (fixdfsi):
+
+#ifdef  __LITTLE_ENDIAN__
+        mov     r4,r1
+        mov     r5,r4
+        mov     r1,r5
+
+#endif
+	mov.l	.L_p_inf,r2
+	mov     #-20,r1
+	
+	mov	r2,r7
+	mov.l   .L_1023,r3
+
+	and	r4,r2
+	shll    r4
+        
+	movt    r6		! r6 contains the sign bit
+	
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r1,r2		! r2 contains the exponent
+#else
+        SHLR20 (r2)
+#endif
+	 shlr    r4
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r1,r7
+#else
+        SHLR20 (r7)
+#endif
+	cmp/hi	r2,r3		! if exp < 1023,return 0
+	mov.l	.L_mask_high_mant,r1
+
+	SL(bt,	.L_epil,
+	 mov	#0,r0)
+	and	r4,r1		! r1 contains high mantissa
+
+	cmp/eq	r2,r7		! chk if exp is invalid
+	mov.l	.L_1053,r7
+
+	bt	.L_inv_exp
+	mov	#11,r0
+	
+	cmp/hi	r7,r2		! If exp > 1053,return maxint
+	sub     r2,r7
+
+	mov.l	.L_21bit,r2
+	SL(bt,	.L_ret_max,
+	 add	#1,r7)		! r7 contains the number of shifts
+
+	or	r2,r1
+	mov	r7,r3
+	shll8   r1
+
+	neg     r7,r7
+	shll2	r1
+
+        shll	r1
+	cmp/hi	r3,r0
+
+	!chk if the result can be made only from higher mantissa
+	SL(bt,	.L_lower_mantissa,
+	 mov	#21,r0)
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r7,r1
+#else
+.L_loop:
+        tst	r7,r7
+        bt      .L_break1
+        add     #1,r7
+        bra     .L_loop
+        shlr    r1
+
+.L_break1:
+#endif
+	tst	r6,r6
+	SL(bt,	.L_epil,
+	 mov	r1,r0)
+
+	rts
+	neg	r0,r0
+
+.L_lower_mantissa:
+	!result is made from lower mantissa also
+	neg	r0,r0
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r0,r5
+#else
+        SHLR21 (r5)
+#endif
+
+	or	r5,r1		!pack lower and higher mantissas
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r7,r1
+#else
+.L_sh_loop:
+	tst	r7,r7
+	bt	.L_break
+	add	#1,r7
+	bra	.L_sh_loop
+	shlr	r1
+
+.L_break:
+#endif
+	mov	r1,r0
+	bra	.L_chk_sign
+	nop
+
+.L_epil:
+	rts
+	nop
+
+.L_inv_exp:
+	cmp/hi	r0,r5
+	bt	.L_epil
+
+	cmp/hi	r0,r1		!compare high mantissa,r1
+	bt	.L_epil
+
+.L_ret_max:
+	mov.l   .L_maxint,r0
+	tst	r6,r6
+	bt	.L_epil
+
+	rts
+	add	#1,r0
+
+.L_chk_sign:
+	tst	r6,r6		!sign bit is set, number is -ve
+	bt	.L_epil
+	
+	rts
+	neg	r0,r0
+
+	.align	2
+
+.L_maxint:
+	.long	0x7fffffff
+.L_p_inf:
+	.long	0x7ff00000
+.L_mask_high_mant:
+	.long	0x000fffff
+.L_1023:
+	.long	0x000003ff
+.L_1053:
+	.long	1053
+.L_21bit:
+	.long	0x00100000
+
+ENDFUNC (GLOBAL (fixdfsi))
--- gcc/libgcc/config/sh/lib1funcs.S	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libgcc/config/sh/lib1funcs.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,6 +1,7 @@
 /* Copyright (C) 1994, 1995, 1997, 1998, 1999, 2000, 2001, 2002, 2003,
    2004, 2005, 2006, 2009
    Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is free software; you can redistribute it and/or modify it
 under the terms of the GNU General Public License as published by the
@@ -713,7 +714,7 @@ LOCAL(lshrsi3_0):
 GLOBAL(movmem):
 	sts.l	pr,@-r15
 	shll2	r6
-	bsr	GLOBAL(movmemSI52+2)
+	bsr	GLOBAL(movmemSI52)+2
 	mov.l	@(48,r5),r0
 	.balign	4
 LOCAL(movmem_loop): /* Reached with rts */
@@ -742,101 +743,83 @@ LOCAL(movmem_done): ! share slot insn, works out a
 	rts
 	mov.l	r0,@(52,r4)
 	.balign	4
-! ??? We need aliases movstr* for movmem* for the older libraries.  These
-! aliases will be removed at the some point in the future.
 	.global	GLOBAL(movmemSI64)
 	HIDDEN_FUNC(GLOBAL(movmemSI64))
-	HIDDEN_ALIAS(movstrSI64,movmemSI64)
 GLOBAL(movmemSI64):
 	mov.l	@(60,r5),r0
 	mov.l	r0,@(60,r4)
 	.global	GLOBAL(movmemSI60)
 	HIDDEN_FUNC(GLOBAL(movmemSI60))
-	HIDDEN_ALIAS(movstrSI60,movmemSI60)
 GLOBAL(movmemSI60):
 	mov.l	@(56,r5),r0
 	mov.l	r0,@(56,r4)
 	.global	GLOBAL(movmemSI56)
 	HIDDEN_FUNC(GLOBAL(movmemSI56))
-	HIDDEN_ALIAS(movstrSI56,movmemSI56)
 GLOBAL(movmemSI56):
 	mov.l	@(52,r5),r0
 	mov.l	r0,@(52,r4)
 	.global	GLOBAL(movmemSI52)
 	HIDDEN_FUNC(GLOBAL(movmemSI52))
-	HIDDEN_ALIAS(movstrSI52,movmemSI52)
 GLOBAL(movmemSI52):
 	mov.l	@(48,r5),r0
 	mov.l	r0,@(48,r4)
 	.global	GLOBAL(movmemSI48)
 	HIDDEN_FUNC(GLOBAL(movmemSI48))
-	HIDDEN_ALIAS(movstrSI48,movmemSI48)
 GLOBAL(movmemSI48):
 	mov.l	@(44,r5),r0
 	mov.l	r0,@(44,r4)
 	.global	GLOBAL(movmemSI44)
 	HIDDEN_FUNC(GLOBAL(movmemSI44))
-	HIDDEN_ALIAS(movstrSI44,movmemSI44)
 GLOBAL(movmemSI44):
 	mov.l	@(40,r5),r0
 	mov.l	r0,@(40,r4)
 	.global	GLOBAL(movmemSI40)
 	HIDDEN_FUNC(GLOBAL(movmemSI40))
-	HIDDEN_ALIAS(movstrSI40,movmemSI40)
 GLOBAL(movmemSI40):
 	mov.l	@(36,r5),r0
 	mov.l	r0,@(36,r4)
 	.global	GLOBAL(movmemSI36)
 	HIDDEN_FUNC(GLOBAL(movmemSI36))
-	HIDDEN_ALIAS(movstrSI36,movmemSI36)
 GLOBAL(movmemSI36):
 	mov.l	@(32,r5),r0
 	mov.l	r0,@(32,r4)
 	.global	GLOBAL(movmemSI32)
 	HIDDEN_FUNC(GLOBAL(movmemSI32))
-	HIDDEN_ALIAS(movstrSI32,movmemSI32)
 GLOBAL(movmemSI32):
 	mov.l	@(28,r5),r0
 	mov.l	r0,@(28,r4)
 	.global	GLOBAL(movmemSI28)
 	HIDDEN_FUNC(GLOBAL(movmemSI28))
-	HIDDEN_ALIAS(movstrSI28,movmemSI28)
 GLOBAL(movmemSI28):
 	mov.l	@(24,r5),r0
 	mov.l	r0,@(24,r4)
 	.global	GLOBAL(movmemSI24)
 	HIDDEN_FUNC(GLOBAL(movmemSI24))
-	HIDDEN_ALIAS(movstrSI24,movmemSI24)
 GLOBAL(movmemSI24):
 	mov.l	@(20,r5),r0
 	mov.l	r0,@(20,r4)
 	.global	GLOBAL(movmemSI20)
 	HIDDEN_FUNC(GLOBAL(movmemSI20))
-	HIDDEN_ALIAS(movstrSI20,movmemSI20)
 GLOBAL(movmemSI20):
 	mov.l	@(16,r5),r0
 	mov.l	r0,@(16,r4)
 	.global	GLOBAL(movmemSI16)
 	HIDDEN_FUNC(GLOBAL(movmemSI16))
-	HIDDEN_ALIAS(movstrSI16,movmemSI16)
 GLOBAL(movmemSI16):
 	mov.l	@(12,r5),r0
 	mov.l	r0,@(12,r4)
 	.global	GLOBAL(movmemSI12)
 	HIDDEN_FUNC(GLOBAL(movmemSI12))
-	HIDDEN_ALIAS(movstrSI12,movmemSI12)
 GLOBAL(movmemSI12):
 	mov.l	@(8,r5),r0
 	mov.l	r0,@(8,r4)
 	.global	GLOBAL(movmemSI8)
 	HIDDEN_FUNC(GLOBAL(movmemSI8))
-	HIDDEN_ALIAS(movstrSI8,movmemSI8)
 GLOBAL(movmemSI8):
 	mov.l	@(4,r5),r0
 	mov.l	r0,@(4,r4)
 	.global	GLOBAL(movmemSI4)
 	HIDDEN_FUNC(GLOBAL(movmemSI4))
-	HIDDEN_ALIAS(movstrSI4,movmemSI4)
 GLOBAL(movmemSI4):
 	mov.l	@(0,r5),r0
 	rts
@@ -876,7 +859,7 @@ GLOBAL(movmemSI4):
 	HIDDEN_ALIAS(movstrSI12_i4,movmemSI12_i4)
 
 	.p2align	5
-L_movmem_2mod4_end:
+LOCAL(movmem_2mod4_end):
 	mov.l	r0,@(16,r4)
 	rts
 	mov.l	r1,@(20,r4)
@@ -885,7 +868,7 @@ GLOBAL(movmemSI4):
 
 GLOBAL(movmem_i4_even):
 	mov.l	@r5+,r0
-	bra	L_movmem_start_even
+	bra	LOCAL(movmem_start_even)
 	mov.l	@r5+,r1
 
 GLOBAL(movmem_i4_odd):
@@ -896,20 +879,20 @@ GLOBAL(movmem_i4_odd):
 	mov.l	r1,@(4,r4)
 	mov.l	r2,@(8,r4)
 
-L_movmem_loop:
+LOCAL(movmem_loop):
 	mov.l	r3,@(12,r4)
 	dt	r6
 	mov.l	@r5+,r0
-	bt/s	L_movmem_2mod4_end
+	bt/s	LOCAL(movmem_2mod4_end)
 	mov.l	@r5+,r1
 	add	#16,r4
-L_movmem_start_even:
+LOCAL(movmem_start_even):
 	mov.l	@r5+,r2
 	mov.l	@r5+,r3
 	mov.l	r0,@r4
 	dt	r6
 	mov.l	r1,@(4,r4)
-	bf/s	L_movmem_loop
+	bf/s	LOCAL(movmem_loop)
 	mov.l	r2,@(8,r4)
 	rts
 	mov.l	r3,@(12,r4)
@@ -947,17 +930,23 @@ GLOBAL(movmemSI12_i4):
 ! aa = bb*dd + (aa*dd*65536) + (cc*bb*65536)
 !
 
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif
 GLOBAL(mulsi3):
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	nop
+#endif	
 	mulu.w  r4,r5		! multiply the lsws  macl=bb*dd
 	mov     r5,r3		! r3 = ccdd
 	swap.w  r4,r2		! r2 = bbaa
 	xtrct   r2,r3		! r3 = aacc
 	tst  	r3,r3		! msws zero ?
-	bf      hiset
+	bf      LOCAL(hiset)
 	rts			! yes - then we have the answer
 	sts     macl,r0
 
-hiset:	sts	macl,r0		! r0 = bb*dd
+LOCAL(hiset):	sts	macl,r0		! r0 = bb*dd
 	mulu.w	r2,r5		! brewing macl = aa*dd
 	sts	macl,r1
 	mulu.w	r3,r4		! brewing macl = cc*bb
@@ -978,6 +967,9 @@ GLOBAL(mulsi3):
 
 	.global	GLOBAL(sdivsi3_i4)
 	HIDDEN_FUNC(GLOBAL(sdivsi3_i4))
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	5
+#endif	
 GLOBAL(sdivsi3_i4):
 	lds r4,fpul
 	float fpul,dr0
@@ -1249,13 +1241,18 @@ LOCAL(sdivsi3_dontsub):
 	blink tr2,r63
 	ENDFUNC(GLOBAL(sdivsi3))
 #else /* ! __SHMEDIA__ */
+	
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif
+	
 	FUNC(GLOBAL(sdivsi3))
 GLOBAL(sdivsi3):
 	mov	r4,r1
 	mov	r5,r0
 
 	tst	r0,r0
-	bt	div0
+	bt	LOCAL(div0)
 	mov	#0,r2
 	div0s	r2,r1
 	subc	r3,r3
@@ -1330,8 +1327,10 @@ GLOBAL(sdivsi3):
 	rts
 	mov	r1,r0
 
-
-div0:	rts
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	nop
+#endif		
+LOCAL(div0):	rts
 	mov	#0,r0
 
 	ENDFUNC(GLOBAL(sdivsi3))
@@ -1346,16 +1345,19 @@ GLOBAL(sdivsi3):
 !! args in r4 and r5, result in fpul, clobber r0, r1, r4, r5, dr0, dr2, dr4,
 !! and t bit
 
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif
 	.global	GLOBAL(udivsi3_i4)
 	HIDDEN_FUNC(GLOBAL(udivsi3_i4))
 GLOBAL(udivsi3_i4):
 	mov #1,r1
 	cmp/hi r1,r5
-	bf trivial
+	bf LOCAL(trivial)
 	rotr r1
 	xor r1,r4
 	lds r4,fpul
-	mova L1,r0
+	mova 1f,r0
 #ifdef FMOVD_WORKS
 	fmov.d @r0+,dr4
 #else
@@ -1372,7 +1374,7 @@ GLOBAL(udivsi3_i4):
 	rts
 	ftrc dr0,fpul
 
-trivial:
+LOCAL(trivial):
 	rts
 	lds r4,fpul
 
@@ -1380,7 +1382,7 @@ GLOBAL(udivsi3_i4):
 #ifdef FMOVD_WORKS
 	.align 3	! make double below 8 byte aligned.
 #endif
-L1:
+1:
 	.double 2147483648
 
 	ENDFUNC(GLOBAL(udivsi3_i4))
@@ -1407,15 +1409,14 @@ GLOBAL(udivsi3_i4):
 #endif /* ! __SH5__ || __SH5__ == 32 */
 #elif defined(__SH4_SINGLE__) || defined(__SH4_SINGLE_ONLY__)
 !! args in r4 and r5, result in fpul, clobber r0, r1, r4, r5, dr0, dr2, dr4
-
 	.global	GLOBAL(udivsi3_i4)
 	HIDDEN_FUNC(GLOBAL(udivsi3_i4))
 GLOBAL(udivsi3_i4):
 	mov #1,r1
 	cmp/hi r1,r5
-	bf trivial
+	bf LOCAL(trivial)
 	sts.l fpscr,@-r15
-	mova L1,r0
+	mova 1f,r0
 	lds.l @r0+,fpscr
 	rotr r1
 	xor r1,r4
@@ -1440,12 +1441,12 @@ GLOBAL(udivsi3_i4):
 #ifdef FMOVD_WORKS
 	.align 3	! make double below 8 byte aligned.
 #endif
-trivial:
+LOCAL(trivial):
 	rts
 	lds r4,fpul
 
 	.align 2
-L1:
+1:
 #ifndef FMOVD_WORKS
 	.long 0x80000
 #else
@@ -1461,7 +1462,6 @@ GLOBAL(udivsi3_i4):
 /* __SH4_SINGLE_ONLY__ keeps this part for link compatibility with
    sh2e/sh3e code.  */
 #if (! defined(__SH4__) && ! defined (__SH4_SINGLE__)) || defined (__linux__)
-
 !! args in r4 and r5, result in r0, clobbers r4, pr, and t bit
 	.global	GLOBAL(udivsi3)
 	HIDDEN_FUNC(GLOBAL(udivsi3))
@@ -1603,8 +1603,16 @@ LOCAL(divx4):
  div1 r5,r4; rotcl r0
  div1 r5,r4; rotcl r0
  div1 r5,r4; rotcl r0
- rts; div1 r5,r4
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	nop
+#endif		
+ rts
+ div1 r5,r4
 
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif
+
 GLOBAL(udivsi3):
  sts.l pr,@-r15
  extu.w r5,r0
@@ -1617,6 +1625,9 @@ GLOBAL(udivsi3):
  div0u
  swap.w r4,r0
  shlr16 r4
+#ifdef	DB_ST40300_BUG_WORKAROUND
+ nop
+#endif			
  bsr LOCAL(div8)
  shll16 r5
  bsr LOCAL(div7)
@@ -1641,6 +1652,9 @@ LOCAL(large_divisor):
  mov #0,r0
  xtrct r4,r0
  xtrct r0,r4
+#ifdef	DB_ST40300_BUG_WORKAROUND
+ nop
+#endif		
  bsr LOCAL(divx4)
  rotcl r0
  bsr LOCAL(divx4)
@@ -1969,6 +1983,10 @@ GLOBAL(moddi3):
 #ifdef __SH5__
 	.mode	SHcompact
 #endif
+	
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif	
 	.global GLOBAL(set_fpscr)
 	HIDDEN_FUNC(GLOBAL(set_fpscr))
 GLOBAL(set_fpscr):
@@ -2010,6 +2028,9 @@ GLOBAL(set_fpscr):
 #endif
 #if defined(__SH4__) || defined (__SH2A_DOUBLE__)
 	swap.w r0,r2
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	nop
+#endif		
 	rts
 	mov.l r2,@r1
 #else /* defined(__SH2E__) || defined(__SH3E__) || defined(__SH4_SINGLE*__) */
@@ -2046,6 +2067,54 @@ LOCAL(set_fpscr_L1):
 #endif /* SH2E / SH3E / SH4 */
 #endif /* __SH2A_NOFPU__ */
 #endif /* L_set_fpscr */
+
+/* i-cache flushing
+ * For system code, we use ic_invalidate_line_i, but user code
+ * needs a different mechanism.  There are a number of ways of doing
+ * this:
+ *	1.  A kernel syscall (generally not available, slow but supports multiple arch variants)
+ *	2.  A kernel vsyscall (optimum solution, supports multiple arch variants)
+ *	3.  A jump table (supports 1 arch variant)
+ * For 3, different SH4 variants use different sizes and associativities
+ * of the Icache.  We use a small bit of dispatch code that can be put
+ * hidden in every shared object, which calls the actual processor-specific
+ * invalidation code in a separate module.
+ */
+
+/*
+ * SYSCALL method for i-cache flushing
+ */
+
+#ifdef L_ic_invalidate_syscall
+#include <asm/unistd.h>
+#include <asm/cachectl.h>
+
+	.global GLOBAL(ic_invalidate_syscall)
+	HIDDEN_FUNC(GLOBAL(ic_invalidate_syscall))
+	HIDDEN_ALIAS(ic_invalidate,ic_invalidate_syscall)
+GLOBAL(ic_invalidate_syscall):
+	mov.l	1f, r6
+	mov.l	2f, r3
+	/* Note: L1 cacheline size is not exposed to userspace
+	 *       so we use a length of 4 so it will work for
+	 *       different cacheline sizes
+	 */
+	mov	#4, r5
+	trapa	#0x13
+	rts
+	 nop
+	.balign 4
+1:	.long (CACHEFLUSH_D_WB | CACHEFLUSH_I)
+2:	.long __NR_cacheflush
+
+	ENDFUNC(GLOBAL(ic_invalidate_syscall))
+
+#endif /* L_ic_invalidate_syscall */
+
+/*
+ * Jump table method for i-cache flushing
+ */
+
 #ifdef L_ic_invalidate
 #if __SH5__ == 32
 	.mode	SHmedia
@@ -2079,7 +2148,7 @@ GLOBAL(ic_invalidate):
 	synci
 	blink	tr0, r63
 	ENDFUNC(GLOBAL(ic_invalidate))
-#elif defined(__SH4A__)
+#elif defined(__SH4A__) || defined(__FORCE_SH4A__)
 	.global GLOBAL(ic_invalidate)
 	HIDDEN_FUNC(GLOBAL(ic_invalidate))
 GLOBAL(ic_invalidate):
@@ -2090,18 +2159,11 @@ GLOBAL(ic_invalidate):
 	  nop
 	ENDFUNC(GLOBAL(ic_invalidate))
 #elif defined(__SH4_SINGLE__) || defined(__SH4__) || defined(__SH4_SINGLE_ONLY__) || (defined(__SH4_NOFPU__) && !defined(__SH5__))
-	/* For system code, we use ic_invalidate_line_i, but user code
-	   needs a different mechanism.  A kernel call is generally not
-	   available, and it would also be slow.  Different SH4 variants use
-	   different sizes and associativities of the Icache.  We use a small
-	   bit of dispatch code that can be put hidden in every shared object,
-	   which calls the actual processor-specific invalidation code in a
-	   separate module.
-	   Or if you have operating system support, the OS could mmap the
-	   procesor-specific code from a single page, since it is highly
-	   repetitive.  */
 	.global GLOBAL(ic_invalidate)
 	HIDDEN_FUNC(GLOBAL(ic_invalidate))
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif	
 GLOBAL(ic_invalidate):
 #ifdef __pic__
 #ifdef __vxworks
@@ -2146,7 +2208,6 @@ GLOBAL(ic_invalidate):
 
 #ifdef L_ic_invalidate_array
 #if defined(__SH4A__) || (defined (__FORCE_SH4A__) && (defined(__SH4_SINGLE__) || defined(__SH4__) || defined(__SH4_SINGLE_ONLY__) || (defined(__SH4_NOFPU__) && !defined(__SH5__))))
-	.global GLOBAL(ic_invalidate_array)
 	/* This is needed when an SH4 dso with trampolines is used on SH4A.  */
 	.global GLOBAL(ic_invalidate_array)
 	FUNC(GLOBAL(ic_invalidate_array))
@@ -3049,7 +3110,7 @@ GLOBAL(GCC_pop_shmedia_regs_nofpu):
  .global GLOBAL(sdivsi3)
 GLOBAL(sdivsi3):
 #ifdef TEXT_DATA_BUG
- ptb datalabel Local_div_table,tr0
+ ptb datalabel LOCAL(Ldiv_table),tr0
 #else
  ptb GLOBAL(div_table_internal),tr0
 #endif
@@ -3103,8 +3164,8 @@ Defects for bias -330:
 #endif /* __pic__ */
 #if defined(TEXT_DATA_BUG) && defined(__pic__) && defined(__SHMEDIA__)
 	.balign 2
-	.type	Local_div_table,@object
-	.size	Local_div_table,128
+	.type	LOCAL(Ldiv_table),@object
+	.size	LOCAL(Ldiv_table),128
 /* negative division constants */
 	.word	-16638
 	.word	-17135
@@ -3140,7 +3201,7 @@ Defects for bias -330:
 	.byte	214
 	.byte	241
 	.skip 16
-Local_div_table:
+LOCAL(Ldiv_table):
 	.skip 16
 /* positive division factors */
 	.byte	241
@@ -3272,11 +3333,19 @@ GLOBAL(div_table):
 #define L_MSWLSB 1
 #endif
 
+	
 	.balign 4
 	.global	GLOBAL(udivsi3_i4i)
 	FUNC(GLOBAL(udivsi3_i4i))
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	5
+#endif			
 GLOBAL(udivsi3_i4i):
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	mov.w LOCAL(c128_lw), r1
+#else
 	mov.w LOCAL(c128_w), r1
+#endif
 	div0u
 	mov r4,r0
 	shlr8 r0
@@ -3324,8 +3393,14 @@ LOCAL(div_le128_2):
 	rts
 	shld r1,r0
 
+#ifdef	DB_ST40300_BUG_WORKAROUND	
+LOCAL(c128_lw):
+	.word 128
+#else
 LOCAL(div_by_1_neg):
 	neg r4,r0
+#endif
+	
 LOCAL(div_by_1):
 	mov.l @r15+,r5
 	rts
@@ -3395,6 +3470,17 @@ LOCAL(div_r8_2):
 	FUNC(GLOBAL(sdivsi3_i4i))
 	/* This is link-compatible with a GLOBAL(sdivsi3) call,
 	   but we effectively clobber only r1.  */
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	5
+	
+LOCAL(div_by_1_neg):
+	neg r4,r0
+LOCAL(div_by_12):
+	mov.l @r15+,r5
+	rts
+	mov.l @r15+,r4
+#endif			
+	
 GLOBAL(sdivsi3_i4i):
 	mov.l r4,@-r15
 	cmp/pz r5
@@ -3891,6 +3977,9 @@ LOCAL(div_table_inv):
 	/* n1 < d, but n1 might be larger than d1.  */
 	.global GLOBAL(udiv_qrnnd_16)
 	.balign 8
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	5
+#endif		
 GLOBAL(udiv_qrnnd_16):
 	div0u
 	cmp/hi r6,r0
@@ -3931,3 +4020,8 @@ GLOBAL(udiv_qrnnd_16):
 	ENDFUNC(GLOBAL(udiv_qrnnd_16))
 #endif /* !__SHMEDIA__ */
 #endif /* L_udiv_qrnnd_16 */
+
+#ifndef L_div_table
+#include "ieee-754-sf.S"
+#include "ieee-754-df.S"
+#endif
--- gcc/libgcc/config/sh/lib1funcs-4-300.S	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libgcc/config/sh/lib1funcs-4-300.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -251,8 +251,8 @@ GLOBAL(udivsi3_i4i):
 	   don't get such an adjustment, it's OK to also compute one's -> two's
 	   complement adjustment suppression for a dividend of 0.  */
 	.balign 4
-GLOBAL(sdivsi3_i4i):
-	mov.l r6,@-r15
+GLOBAL(sdivsi3_i4i):	
+	mov.l r6,@-r15	
 	exts.b r5,r6
 	cmp/eq r5,r6
 	mov #-1,r1
@@ -403,12 +403,12 @@ LOCAL(le128_neg):
 	addc r6,r0
 	rotcr r0
 	mov.l @r15+,r6
-	shad r1,r0
+	shld r1,r0	
 	rts
 	neg r0,r0
 	ENDFUNC(GLOBAL(udivsi3_i4i))
 	ENDFUNC(GLOBAL(sdivsi3_i4i))
-
+	
 /* This table has been generated by divtab-sh4.c.  */
 	.balign 4
 	.byte	-7
--- gcc/libgcc/config/sh/ieee-754-sf.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/ieee-754-sf.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,703 @@
+/* Copyright (C) 2006, 2008, 2009 Free Software Foundation, Inc.
+   Copyright (c) 2010 STMicroelectronics.
+	
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 3, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   Under Section 7 of GPL version 3, you are granted additional
+   permissions described in the GCC Runtime Library Exception, version
+   3.1, as published by the Free Software Foundation.
+
+   You should have received a copy of the GNU General Public License and
+   a copy of the GCC Runtime Library Exception along with this program;
+   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+   <http://www.gnu.org/licenses/>.  */
+
+!! libgcc software floating-point routines for Renesas SH /
+!! STMicroelectronics ST40 CPUs
+!! Contributed by J"orn Rennecke joern.rennecke@st.com
+
+#ifndef __SH_FPU_ANY__
+
+#include "lib1funcs.h"
+#include "insn-constants.h"
+
+/* Single-precision floating-point emulation.
+   We handle NANs, +-infinity, and +-zero.
+   However, we assume that for NANs, the topmost bit of the fraction is set.  */
+#ifdef L_nesf2
+/* -fno-finite-math-only inline version, T := r4:SF == r5:SF
+	cmp/eq	r4,r5
+	mov	r4,r0
+	bt	0f
+	or	r5,r0
+	add	r0,r0
+	tst	r0,r0	! test for +0.0 == -0.0 ; -0.0 == +0.0
+	0:			*/
+	.balign 4
+	.global GLOBAL(nesf2)
+	HIDDEN_FUNC(GLOBAL(nesf2))
+GLOBAL(nesf2):
+        /* If the raw values are unequal, the result is unequal, unless
+	   both values are +-zero.
+	   If the raw values are equal, the result is equal, unless
+	   the values are NaN.  */
+	cmp/eq	r4,r5
+	mov.l   LOCAL(c_SF_NAN_MASK),r1
+	bt.s	LOCAL(check_nan)
+	not	r4,r0
+	mov	r4,r0
+	or	r5,r0
+	rts
+	add	r0,r0
+LOCAL(check_nan):
+	tst	r1,r0
+	bt.s 	LOCAL(nan)
+	mov	#96,r2
+	shll16  r2
+	xor 	r2,r1
+	tst	r1,r0	
+LOCAL(nan):		
+	rts
+	movt	r0
+	
+	.balign 4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+LOCAL(c_SF_SNAN_MASK):
+	ENDFUNC(GLOBAL(nesf2))
+#endif /* L_nesf2 */
+
+#ifdef L_unord_sf
+	.balign 4
+	.global GLOBAL(unordsf2)
+	HIDDEN_FUNC(GLOBAL(unordsf2))
+GLOBAL(unordsf2):
+	mov.l	LOCAL(c_SF_NAN_MASK),r1
+	not	r4,r0
+	tst	r1,r0
+	not	r5,r0
+	bt	LOCAL(unord)
+	tst	r1,r0
+LOCAL(unord):
+	rts
+	movt	r0
+	.balign	4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+	ENDFUNC(GLOBAL(unordsf2))
+#endif /* L_unord_sf */
+
+#if defined(L_gtsf2t) || defined(L_gtsf2t_trap)
+/* -fno-finite-math-only inline version, T := r4:SF > r5:SF ? 0 : 1
+	cmp/pz	r4
+	mov	r4,r0
+	bf/s	0f
+	 cmp/hs	r5,r4
+	cmp/ge	r4,r5
+	or	r5,r0
+	bt	0f
+	add	r0,r0
+	tst	r0,r0
+	0:			*/
+#ifdef L_gtsf2t
+#define fun_label GLOBAL(gtsf2t)
+#else
+#define fun_label GLOBAL(gtsf2t_trap)
+#endif
+	.balign 4
+	.global fun_label
+	HIDDEN_FUNC(fun_label)
+fun_label:
+	/* If the raw values compare greater, the result true, unless
+	   any of them is a nan (but infinity is fine), or both values are
+	   +- zero.  Otherwise, the result false.  */
+	mov.l	LOCAL(c_SF_NAN_MASK),r1
+	cmp/pz	r4
+	not	r5,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	r4,r0
+	bt	LOCAL(nan)
+	cmp/gt	r5,r4
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/gt	r4,r1)
+	bf	LOCAL(nan)
+	or	r5,r0
+	rts
+	add	r0,r0
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan)
+	not	r4,r0
+	tst	r1,r0
+	bt	LOCAL(nan)
+	cmp/hi	r4,r5
+#if defined(L_gtsf2t) && defined(DELAYED_BRANCHES)
+LOCAL(check_nan):
+#endif /* DELAYED_BRANCHES */
+	rts
+	movt	r0
+#ifdef L_gtsf2t
+LOCAL(check_nan):
+LOCAL(nan):
+	rts
+	mov	#0,r0
+#else /* ! L_gtsf2t */
+LOCAL(check_nan):
+	SLI(cmp/gt	r4,r1)
+	bf	LOCAL(nan)
+	rts
+	movt	r0
+LOCAL(nan):
+	mov	#0,r0
+	trapa	#0
+#endif /* ! L_gtsf2t */
+	.balign	4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+	ENDFUNC(fun_label)
+#endif /* L_gtsf2t */
+
+#if defined(L_gesf2f) || defined(L_gesf2f_trap)
+/* -fno-finite-math-only inline version, T := r4:SF >= r5:SF */
+	cmp/pz	r5
+	mov	r4,r0
+	bf/s	0f
+	 cmp/hs	r4,r5
+	cmp/ge	r5,r4
+	or	r5,r0
+	bt	0f
+	add	r0,r0
+	tst	r0,r0
+	0:
+#ifdef L_gesf2f
+#define fun_label GLOBAL(gesf2f)
+#else
+#define fun_label GLOBAL(gesf2f_trap)
+#endif
+	.balign 4
+	.global fun_label
+	HIDDEN_FUNC(fun_label)
+fun_label:
+	/* If the raw values compare greater or equal, the result is
+	   true, unless any of them is a nan.  If both are -+zero, the
+	   result is true; otherwise, it is false.
+	   We use 0 as true and nonzero as false for this function.  */
+	mov.l	LOCAL(c_SF_NAN_MASK),r1
+	cmp/pz	r5
+	not	r4,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	r4,r0
+	bt	LOCAL(nan)
+	cmp/gt	r4,r5
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/ge	r1,r5)
+	bt	LOCAL(nan)
+	or	r5,r0
+	rts
+	add	r0,r0
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan)
+	not	r5,r0
+	tst	r1,r0
+	bt	LOCAL(nan)
+	cmp/hi	r5,r4
+#if defined(L_gesf2f) && defined(DELAYED_BRANCHES)
+LOCAL(nan): LOCAL(check_nan):
+#endif
+	rts
+	movt	r0
+#if defined(L_gesf2f) && ! defined(DELAYED_BRANCHES)
+LOCAL(check_nan):
+	cmp/ge	r1,r5
+LOCAL(nan):
+	rts
+	movt	r0
+#endif /* ! DELAYED_BRANCHES */
+#ifdef L_gesf2f_trap
+LOCAL(check_nan):
+	SLI(cmp/ge	r1,r5)
+	bt	LOCAL(nan)
+	rts
+LOCAL(nan):
+	movt	r0
+	trapa	#0
+#endif /* L_gesf2f_trap */
+	.balign	4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+	ENDFUNC(GLOBAL(gesf2f))
+#endif /* L_gesf2f */
+
+#ifndef DYN_SHIFT /* SH1 / SH2 code */
+#ifdef L_addsub_sf
+#include "IEEE-754/addsf3.S"
+#endif /* _addsub_sf */
+
+#ifdef L_mul_sf
+#include "IEEE-754/mulsf3.S"
+#endif /* L_mul_sf */
+
+#ifdef L__fixunssfsi
+#include "IEEE-754/fixunssfsi.S"
+#endif /* L_fixunssfsi */
+
+#ifdef L_sf_to_si
+#include "IEEE-754/fixsfsi.S"
+#endif /* L_sf_to_si */
+
+#ifdef L_usi_to_sf
+#include "IEEE-754/floatunssisf.S"
+#endif /* L_usi_to_sf */
+
+#ifdef L_si_to_sf
+#include "IEEE-754/floatsisf.S"
+#endif /* L_si_to_sf */
+
+#ifdef L_div_sf
+#include "IEEE-754/divsf3.S"
+#endif /* L_div_sf */
+#endif /* ! DYN_SHIFT */
+
+/* The actual arithmetic uses dynamic shift.  Supporting SH1 / SH2 here would
+   make this code too hard to maintain, so if you want to add SH1 / SH2
+   support, do it in a separate copy.  */
+#ifdef DYN_SHIFT
+#ifdef L_addsub_sf
+#include "IEEE-754/m3/addsf3.S"
+#endif /* L_addsub_sf */
+
+#ifdef L_mul_sf
+#include "IEEE-754/m3/mulsf3.S"
+#endif /* L_mul_sf */
+
+#ifdef L_fixunssfsi
+	! What is a bit unusal about this implementation is that the
+	! sign bit influences the result for NANs: for cleared sign bit, you
+	! get UINT_MAX, for set sign bit, you get 0.
+	! However, since the result for NANs is undefined, this should be no
+	! problem.
+	! N.B. This is scheduled both for SH4-200 and SH4-300
+	.balign 4
+	.global GLOBAL(fixunssfsi)
+	FUNC(GLOBAL(fixunssfsi))
+GLOBAL(fixunssfsi):
+	mov.l	LOCAL(max),r2
+	mov	#-23,r1
+	mov	r4,r0
+	shad	r1,r4
+	mov.l	LOCAL(mask),r1
+	add	#-127,r4
+	cmp/ge	r2,r0
+	or	r2,r0
+	bt	LOCAL(retmax)
+	cmp/pz	r4
+	and	r1,r0
+	bf	LOCAL(ret0)
+	add	#-23,r4
+	rts
+	shld	r4,r0
+LOCAL(ret0):
+LOCAL(retmax):
+	rts
+	subc	r0,r0
+	.balign 4
+LOCAL(mask):
+	.long	0x00ffffff
+LOCAL(max):
+	.long	0x4f800000
+	ENDFUNC(GLOBAL(fixunssfsi))
+#endif /* L_fixunssfsi */
+
+#ifdef L_sf_to_si
+	! What is a bit unusal about this implementation is that the
+	! sign bit influences the result for NANs: for cleared sign bit, you
+	! get INT_MAX, for set sign bit, you get INT_MIN.
+	! However, since the result for NANs is undefined, this should be no
+	! problem.
+	! N.B. This is scheduled both for SH4-200 and SH4-300
+	.balign 4
+	.global GLOBAL(fixsfsi)
+	FUNC(GLOBAL(fixsfsi))
+	.balign	4
+GLOBAL(fixsfsi):
+	mov	r4,r0
+	shll	r4
+	mov	#-24,r1
+	bt	LOCAL(neg)
+	mov.l	LOCAL(max),r2
+	shld	r1,r4
+	mov.l	LOCAL(mask),r1
+	add	#-127,r4
+	cmp/pz	r4
+	add	#-23,r4
+	bf	LOCAL(ret0)
+	cmp/gt	r0,r2
+	bf	LOCAL(retmax)
+	and	r1,r0
+	addc	r1,r0
+	rts
+	shld	r4,r0
+
+	.balign	4
+LOCAL(neg):
+	mov.l	LOCAL(min),r2
+	shld	r1,r4
+	mov.l	LOCAL(mask),r1
+	add	#-127,r4
+	cmp/pz	r4
+	add	#-23,r4
+	bf	LOCAL(ret0)
+	cmp/gt	r0,r2
+	bf	LOCAL(retmin)
+	and	r1,r0
+	addc	r1,r0
+	shld	r4,r0	! SH4-200 will start this insn on a new cycle
+	rts
+	neg	r0,r0
+
+	.balign	4
+LOCAL(ret0):
+	rts
+	mov	#0,r0
+
+LOCAL(retmax):
+	mov	#-1,r0
+	rts
+	shlr	r0
+
+LOCAL(retmin):
+	mov	#1,r0
+	rts
+	rotr	r0
+
+	.balign 4
+LOCAL(mask):
+	.long	0x007fffff
+LOCAL(max):
+	.long	0x4f000000
+LOCAL(min):
+	.long	0xcf000000
+	ENDFUNC(GLOBAL(fixsfsi))
+#endif /* L_sf_to_si */
+
+#ifdef L_usi_to_sf
+#include "IEEE-754/m3/floatunssisf.S"
+#endif /* L_usi_to_sf */
+
+#ifdef L_si_to_sf
+#include "IEEE-754/m3/floatsisf.S"
+#endif /* L_si_to_sf */
+
+#ifdef L_div_sf
+#include "IEEE-754/m3/divsf3.S"
+#endif /* L_div_sf */
+
+#ifdef L_hypotf
+	.balign 4
+	.global GLOBAL(hypotf)
+	FUNC(GLOBAL(hypotf))
+GLOBAL(hypotf):
+/* This integer implementation takes 71 to 72 cycles in the main path.
+   This is a bit slower than the SH4 can do this computation using double
+   precision hardware floating point - 57 cycles, or 69 with mode switches.  */
+ /* First, calculate x (r4) as the sum of the square of the fractions -
+    the exponent is calculated separately in r3.
+    Then, calculate sqrt(x) for the fraction by reciproot iteration.
+    We get an 7.5 bit inital value using linear approximation with two slopes
+    that are powers of two.
+    x (- [1. .. 2.)  y0 := 1.25 - x/4 - tab(x)   y (- (0.8 .. 1.0)
+    x (- [2. .. 4.)  y0 := 1.   - x/8 - tab(x)   y (- (0.5 .. 0.8)
+ x is represented with two bits before the point,
+ y with 0 bits before the binary point.
+ Thus, to calculate y0 := 1. - x/8 - tab(x), all you have to do is to shift x
+ right by 1, negate it, and subtract tab(x).  */
+
+ /* y1 := 1.5*y0 - 0.5 * (x * y0) * (y0 * y0)
+    z0 := x * y1
+    z1 := z0 + 0.5 * (y1 - (y1*y1) * z0) */
+
+	mov.l	LOCAL(xff000000),r1
+	add	r4,r4
+	mov	r4,r0
+	add	r5,r5
+	cmp/hs	r5,r4
+	sub	r5,r0
+	mov	#-24,r2
+	bf/s	LOCAL(r5_large)
+	shad	r2,r0
+	mov	r4,r3
+	shll8	r4
+	rotcr	r4
+	tst	#0xe0,r0
+	neg	r0,r0
+	bt	LOCAL(ret_abs_r3)
+	tst	r1,r5
+	shll8	r5
+	bt/s	LOCAL(denorm_r5)
+	cmp/hi	r3,r1
+	dmulu.l	r4,r4
+	bf	LOCAL(inf_nan)
+	rotcr	r5
+	shld	r0,r5
+LOCAL(denorm_r5_done):
+	sts	mach,r4
+	dmulu.l	r5,r5
+	mov.l	r6,@-r15
+	mov	#20,r6
+
+	sts	mach,r5
+LOCAL(add_frac):
+	mova	LOCAL(tab)-32,r0
+	mov.l	r7,@-r15
+	mov.w	LOCAL(x1380),r7
+	and	r1,r3
+	addc	r5,r4
+	mov.w	LOCAL(m25),r2	! -25
+	bf	LOCAL(frac_ok)
+	sub	r1,r3
+	rotcr	r4
+	cmp/eq	r1,r3	! did we generate infinity ?
+	bt	LOCAL(inf_nan)
+	shlr	r4
+	mov	r4,r1
+	shld	r2,r1
+	mov.b	@(r0,r1),r0
+	mov	r4,r1
+	shld	r6,r1
+	bra	LOCAL(frac_low2)
+	sub	r1,r7
+
+LOCAL(frac_ok):
+	mov	r4,r1
+	shld	r2,r1
+	mov.b	@(r0,r1),r1
+	cmp/pz	r4
+	mov	r4,r0
+	bt/s	LOCAL(frac_low)
+	shld	r6,r0
+	mov.w	LOCAL(xf80),r7
+	shlr	r0
+LOCAL(frac_low):
+	sub	r0,r7
+LOCAL(frac_low2):
+	mov.l	LOCAL(x40000080),r0 ! avoid denorm results near 1. << r3
+	sub	r1,r7	! {0.12}
+	mov.l	LOCAL(xfffe0000),r5 ! avoid rounding overflow near 4. << r3
+	swap.w	r7,r1	! {0.28}
+	dmulu.l	r1,r4 /* two issue cycles */
+	mulu.w	r7,r7  /* two issue cycles */
+	sts	mach,r2	! {0.26}
+	mov	r1,r7
+	shlr	r1
+	sts	macl,r6	! {0.24}
+	cmp/hi	r0,r4
+	shlr2	r2
+	bf	LOCAL(near_one)
+	shlr	r2	! {0.23} systemic error of linear approximation keeps y1 < 1
+	dmulu.l	r2,r6
+	cmp/hs	r5,r4
+	add	r7,r1	! {1.28}
+	bt	LOCAL(near_four)
+	shlr2	r1	! {1.26}
+	sts	mach,r0	! {0.15} x*y0^3 == {0.16} 0.5*x*y0^3
+	shlr2	r1	! {1.24}
+	shlr8	r1	! {1.16}
+	sett		! compensate for truncation of subtrahend, keep y1 < 1
+	subc	r0,r1   ! {0.16} y1;  max error about 3.5 ulp
+	swap.w	r1,r0
+	dmulu.l	r0,r4	! { 1.30 }
+	mulu.w	r1,r1
+	sts	mach,r2
+	shlr2	r0
+	sts	macl,r1
+	add	r2,r0
+	mov.l	LOCAL(xff000000),r6
+	add	r2,r0
+	dmulu.l	r1,r2
+	add	#127,r0
+	add	r6,r3	! precompensation for adding leading 1
+	sts	mach,r1
+	shlr	r3
+	mov.l	@r15+,r7
+	sub	r1,r0	! {0.31} max error about 50 ulp (+127)
+	mov.l	@r15+,r6
+	shlr8	r0	! {0.23} max error about 0.7 ulp
+	rts
+	add	r3,r0
+	
+LOCAL(r5_large):
+	mov	r5,r3
+	mov	#-31,r2
+	cmp/ge	r2,r0
+	shll8	r5
+	bf	LOCAL(ret_abs_r3)
+	rotcr	r5
+	tst	r1,r4
+	shll8	r4
+	bt/s	LOCAL(denorm_r4)
+	cmp/hi	r3,r1
+	dmulu.l	r5,r5
+	bf	LOCAL(inf_nan)
+	rotcr	r4
+LOCAL(denorm_r4_done):
+	shld	r0,r4
+	sts	mach,r5
+	dmulu.l	r4,r4
+	mov.l	r6,@-r15
+	mov	#20,r6
+	bra	LOCAL(add_frac)
+	sts	mach,r4
+
+LOCAL(near_one):
+	bra	LOCAL(assemble_sqrt)
+	mov	#0,r0
+LOCAL(near_four):
+	! exact round-to-nearest would add 255.  We add 256 for speed & compactness.
+	mov	r4,r0
+	shlr8	r0
+	add	#1,r0
+	tst	r0,r0
+	addc	r0,r3	! might generate infinity.
+LOCAL(assemble_sqrt):
+	mov.l	@r15+,r7
+	shlr	r3
+	mov.l	@r15+,r6
+	rts
+	add	r3,r0
+LOCAL(inf_nan):
+LOCAL(ret_abs_r3):
+	mov	r3,r0
+	rts
+	shlr	r0
+LOCAL(denorm_r5):
+	bf	LOCAL(inf_nan)
+	tst	r1,r4
+	bt	LOCAL(denorm_both)
+	dmulu.l	r4,r4
+	bra	LOCAL(denorm_r5_done)
+	shld	r0,r5
+LOCAL(denorm_r4):
+	bf	LOCAL(inf_nan)
+	tst	r1,r5
+	dmulu.l	r5,r5
+	bf	LOCAL(denorm_r4_done)
+LOCAL(denorm_both):	! normalize according to r3.
+	extu.w	r3,r2
+	mov.l	LOCAL(c__clz_tab),r0
+	cmp/eq	r3,r2
+	mov	#-8,r2
+	bt	0f
+	tst	r1,r3
+	mov	#-16,r2
+	bt	0f
+	mov	#-24,r2
+0:
+	shld	r2,r3
+	mov.l	r7,@-r15
+#ifdef __pic__
+	add	r0,r3
+	mova	 LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r3),r0
+	add	#32,r2
+	sub	r0,r2
+	shld	r2,r4
+	mov	r2,r7
+	dmulu.l	r4,r4
+	sts.l	pr,@-r15
+	mov	#1,r3
+	bsr	LOCAL(denorm_r5_done)
+	shld	r2,r5
+	mov.l	LOCAL(x01000000),r1
+	neg	r7,r2
+	lds.l	@r15+,pr
+	tst	r1,r0
+	mov.l	@r15+,r7
+	bt	0f
+	add	#1,r2
+	sub	r1,r0
+0:
+	rts
+	shld	r2,r0
+
+LOCAL(m25):
+	.word	-25
+LOCAL(x1380):
+	.word	0x1380
+LOCAL(xf80):
+	.word	0xf80
+	.balign	4
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(x40000080):
+	.long	0x40000080
+LOCAL(xfffe0000):
+	.long	0xfffe0000
+LOCAL(x01000000):
+	.long	0x01000000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+
+/*
+double err(double x)
+{
+  return (x < 2. ? 1.25 - x/4. : 1. - x/8.) - 1./sqrt(x);
+}
+
+int
+main ()
+{
+  int i = 0;
+  double x, s, v;
+  double lx, hx;
+
+  s = 1./32.;
+  for (x = 1.; x < 4; x += s, i++)
+    {
+      lx = x;
+      hx = x + s - 1. / (1 << 30);
+      v = 0.5 * (err (lx) + err (hx));
+      printf ("%s% 4d%c",
+              (i & 7) == 0 ? "\t.byte\t" : "",
+              (int)(v * 4096 + 0.5) - 128,
+              (i & 7) == 7 ? '\n' : ',');
+    }
+  return 0;
+} */
+
+	.balign	4
+LOCAL(tab):
+	.byte	-113, -84, -57, -33, -11,   8,  26,  41
+	.byte	  55,  67,  78,  87,  94, 101, 106, 110
+	.byte	 113, 115, 115, 115, 114, 112, 109, 106
+	.byte	 101,  96,  91,  84,  77,  69,  61,  52
+	.byte	  51,  57,  63,  68,  72,  77,  80,  84
+	.byte	  87,  89,  91,  93,  95,  96,  97,  97
+	.byte	  97,  97,  97,  96,  95,  94,  93,  91
+	.byte	  89,  87,  84,  82,  79,  76,  72,  69
+	.byte	  65,  61,  57,  53,  49,  44,  39,  34
+	.byte	  29,  24,  19,  13,   8,   2,  -4, -10
+	.byte	 -17, -23, -29, -36, -43, -50, -57, -64
+	.byte	 -71, -78, -85, -93,-101,-108,-116,-124
+	ENDFUNC(GLOBAL(hypotf))
+#endif /* L_hypotf */
+#endif /* DYN_SHIFT */
+
+#endif /* __SH_FPU_ANY__ */
--- gcc/libgcc/config/sh/supervisor-atomic.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/supervisor-atomic.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,195 @@
+/* Copyright (C) 2006, 2008, 2009 Free Software Foundation, Inc.
+   Copyright (c) 2010 STMicroelectronics.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 3, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   Under Section 7 of GPL version 3, you are granted additional
+   permissions described in the GCC Runtime Library Exception, version
+   3.1, as published by the Free Software Foundation.
+
+   You should have received a copy of the GNU General Public License and
+   a copy of the GCC Runtime Library Exception along with this program;
+   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+   <http://www.gnu.org/licenses/>.  */
+
+!! Atomic routines for the Renesas / SuperH SH CPUs for applications
+!! executing in supervisor mode.
+
+/* http://gcc.gnu.org/onlinedocs/gcc/_005f_005fatomic-Builtins.html
+   Just create a working set of atomic routines: When implementing patterns for these built-in functions, the memory model parameter can be ignored as long as the pattern implements the most restrictive __ATOMIC_SEQ_CST model. Any of the other memory models execute correctly with this memory model but they may not execute as efficiently as they could with a more appropriate implementation of the relaxed requirements. */
+
+#include "lib1funcs.h"
+
+#if ! __SH5__
+
+#define ATOMIC_BOOL_COMPARE_AND_SWAP(N,T) \
+	.global	GLOBAL(atomic_compare_exchange_##N); \
+	.align	1; \
+GLOBAL(atomic_compare_exchange_##N):; \
+	stc	sr, r1; \
+	mov	r1, r0; \
+	or	#0xf0, r0; \
+	ldc	r0, sr; \
+	mov.##T	@r4, r2; \
+	mov.##T	@r5, r3; \
+	cmp/eq	r2, r3; \
+	bf/s	0f; \
+	movt	r0; \
+	mov.##T	r6, @r4; \
+	ldc	r1, sr; \
+	rts; \
+	nop; \
+0:	mov.##T	r2, @r5; \
+	ldc	r1, sr; \
+	rts; \
+	nop; \
+	ENDFUNC(GLOBAL(atomic_compare_exchange_##N))
+
+ATOMIC_BOOL_COMPARE_AND_SWAP(1,b)
+ATOMIC_BOOL_COMPARE_AND_SWAP(2,w)
+ATOMIC_BOOL_COMPARE_AND_SWAP(4,l)
+
+#define ATOMIC_EXCHANGE(N,T) \
+	.global	GLOBAL(atomic_exchange_##N); \
+	.align	1; \
+GLOBAL(atomic_exchange_##N):; \
+	stc	sr, r1; \
+	mov	r1, r0; \
+	or	#0xf0, r0; \
+	ldc	r0, sr; \
+	mov.##T	@r4, r2; \
+	mov.##T	r5, @r4; \
+	ldc	r1, sr; \
+	rts; \
+	mov	r2, r0; \
+	ENDFUNC(GLOBAL(atomic_exchange_##N))
+
+ATOMIC_EXCHANGE(1,b)
+ATOMIC_EXCHANGE(2,w)
+ATOMIC_EXCHANGE(4,l)
+
+#define SYNC_FETCH_AND_OP(OP,N,T) \
+	.global	GLOBAL(atomic_fetch_##OP##_##N); \
+	.align	1; \
+GLOBAL(atomic_fetch_##OP##_##N):; \
+	stc	sr, r1; \
+	mov	r1, r0; \
+	or	#0xf0, r0; \
+	ldc	r0, sr; \
+	mov.##T	@r4, r2; \
+	OP	r2, r5; \
+	mov.##T	r5, @r4; \
+	ldc	r1, sr; \
+	rts; \
+	mov	r2, r0; \
+	ENDFUNC(GLOBAL(atomic_fetch_##OP##_##N))
+
+SYNC_FETCH_AND_OP(add,1,b)
+SYNC_FETCH_AND_OP(add,2,w)
+SYNC_FETCH_AND_OP(add,4,l)
+
+SYNC_FETCH_AND_OP(or,1,b)
+SYNC_FETCH_AND_OP(or,2,w)
+SYNC_FETCH_AND_OP(or,4,l)
+
+SYNC_FETCH_AND_OP(and,1,b)
+SYNC_FETCH_AND_OP(and,2,w)
+SYNC_FETCH_AND_OP(and,4,l)
+
+SYNC_FETCH_AND_OP(xor,1,b)
+SYNC_FETCH_AND_OP(xor,2,w)
+SYNC_FETCH_AND_OP(xor,4,l)
+
+#define SYNC_FETCH_AND_COMBOP(OP,OP0,OP1,N,T) \
+	.global	GLOBAL(atomic_fetch_##OP##_##N); \
+	.align	1; \
+GLOBAL(atomic_fetch_##OP##_##N):; \
+	stc	sr, r1; \
+	mov	r1, r0; \
+	or	#0xf0, r0; \
+	ldc	r0, sr; \
+	mov.##T	@r4, r2; \
+	OP0	r2, r5; \
+	OP1	r5, r5; \
+	mov.##T	r5, @r4; \
+	ldc	r1, sr; \
+	rts; \
+	mov	r2, r0; \
+	ENDFUNC(GLOBAL(atomic_fetch_##OP##_##N))
+
+SYNC_FETCH_AND_COMBOP(sub,sub,neg,1,b)
+SYNC_FETCH_AND_COMBOP(sub,sub,neg,2,w)
+SYNC_FETCH_AND_COMBOP(sub,sub,neg,4,l)
+
+SYNC_FETCH_AND_COMBOP(nand,and,not,1,b)
+SYNC_FETCH_AND_COMBOP(nand,and,not,2,w)
+SYNC_FETCH_AND_COMBOP(nand,and,not,4,l)
+
+#define SYNC_OP_AND_FETCH(OP,N,T) \
+	.global	GLOBAL(atomic_##OP##_fetch_##N); \
+	.align	1; \
+GLOBAL(atomic_##OP##_fetch_##N):; \
+	stc	sr, r1; \
+	mov	r1, r0; \
+	or	#0xf0, r0; \
+	ldc	r0, sr; \
+	mov.##T	@r4, r2; \
+	OP	r5, r2; \
+	mov.##T	r2, @r4; \
+	ldc	r1, sr; \
+	rts; \
+	mov	r2, r0; \
+	ENDFUNC(GLOBAL(atomic_##OP##_fetch_##N))
+
+SYNC_OP_AND_FETCH(add,1,b)
+SYNC_OP_AND_FETCH(add,2,w)
+SYNC_OP_AND_FETCH(add,4,l)
+
+SYNC_OP_AND_FETCH(sub,1,b)
+SYNC_OP_AND_FETCH(sub,2,w)
+SYNC_OP_AND_FETCH(sub,4,l)
+
+SYNC_OP_AND_FETCH(or,1,b)
+SYNC_OP_AND_FETCH(or,2,w)
+SYNC_OP_AND_FETCH(or,4,l)
+
+SYNC_OP_AND_FETCH(and,1,b)
+SYNC_OP_AND_FETCH(and,2,w)
+SYNC_OP_AND_FETCH(and,4,l)
+
+SYNC_OP_AND_FETCH(xor,1,b)
+SYNC_OP_AND_FETCH(xor,2,w)
+SYNC_OP_AND_FETCH(xor,4,l)
+
+#define SYNC_COMBOP_AND_FETCH(OP,OP0,OP1,N,T) \
+	.global	GLOBAL(atomic_##OP##_fetch_##N); \
+	.align	1; \
+GLOBAL(atomic_##OP##_fetch_##N):; \
+	stc	sr, r1; \
+	mov	r1, r0; \
+	or	#0xf0, r0; \
+	ldc	r0, sr; \
+	mov.##T	@r4, r2; \
+	OP0	r2, r5; \
+	OP1	r5, r2; \
+	mov.##T	r2, @r4; \
+	ldc	r1, sr; \
+	rts; \
+	mov	r2, r0; \
+	ENDFUNC(GLOBAL(atomic_##OP##_fetch_##N))
+
+SYNC_COMBOP_AND_FETCH(nand,and,not,1,b)
+SYNC_COMBOP_AND_FETCH(nand,and,not,2,w)
+SYNC_COMBOP_AND_FETCH(nand,and,not,4,l)
+
+#endif /* ! __SH5__ */
--- gcc/libgcc/config/sh/supervisor-sync.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/supervisor-sync.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,211 @@
+/* Copyright (C) 2006, 2008, 2009 Free Software Foundation, Inc.
+   Copyright (c) 2010 STMicroelectronics.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 3, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   Under Section 7 of GPL version 3, you are granted additional
+   permissions described in the GCC Runtime Library Exception, version
+   3.1, as published by the Free Software Foundation.
+
+   You should have received a copy of the GNU General Public License and
+   a copy of the GCC Runtime Library Exception along with this program;
+   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+   <http://www.gnu.org/licenses/>.  */
+
+!! Atomic routines for the Renesas / SuperH SH CPUs for applications
+!! executing in supervisor mode.
+
+/* http://gcc.gnu.org/onlinedocs/gcc/_005f_005fatomic-Builtins.html
+   Just create a working set of atomic routines: When implementing patterns for these built-in functions, the memory model parameter can be ignored as long as the pattern implements the most restrictive __SYNC_SEQ_CST model. Any of the other memory models execute correctly with this memory model but they may not execute as efficiently as they could with a more appropriate implementation of the relaxed requirements. */
+
+#include "lib1funcs.h"
+
+#if ! __SH5__
+
+#define SYNC_TEST_AND_SET(N,T) \
+	.global	GLOBAL(sync_lock_test_and_set_##N); \
+	.align	1; \
+GLOBAL(sync_lock_test_and_set_##N):; \
+	stc	sr, r1; \
+	mov	r1, r0; \
+	or	#0xf0, r0; \
+	ldc	r0, sr; \
+	mov.##T	@r4, r2; \
+	mov.##T	r5, @r4; \
+	ldc	r1, sr; \
+	rts; \
+	mov	r2, r0; \
+	ENDFUNC(GLOBAL(sync_lock_test_and_set_##N)) 
+
+SYNC_TEST_AND_SET(1,b)
+SYNC_TEST_AND_SET(2,w)
+SYNC_TEST_AND_SET(4,l)
+
+#define SYNC_VAL_COMPARE_AND_SWAP(N,T) \
+	.global	GLOBAL(sync_val_compare_and_swap_##N); \
+	.align	1; \
+GLOBAL(sync_val_compare_and_swap_##N):; \
+	stc	sr, r1; \
+	mov	r1, r0; \
+	or	#0xf0, r0; \
+	ldc	r0, sr; \
+	mov.##T	@r4, r2; \
+	cmp/eq	r2, r5; \
+	bf	0f; \
+	mov.##T	r6, @r4; \
+0:	ldc	r1, sr; \
+	rts; \
+	mov	r2, r0; \
+	ENDFUNC(GLOBAL(sync_val_compare_and_swap_##N))
+
+SYNC_VAL_COMPARE_AND_SWAP(1,b)
+SYNC_VAL_COMPARE_AND_SWAP(2,w)
+SYNC_VAL_COMPARE_AND_SWAP(4,l)
+
+#define SYNC_BOOL_COMPARE_AND_SWAP(N,T) \
+	.global	GLOBAL(sync_bool_compare_and_swap_##N); \
+	.align	1; \
+GLOBAL(sync_bool_compare_and_swap_##N):; \
+	stc	sr, r1; \
+	mov	r1, r0; \
+	or	#0xf0, r0; \
+	ldc	r0, sr; \
+	mov.##T	@r4, r2; \
+	cmp/eq	r2, r5; \
+	bf/s	0f; \
+	movt	r0; \
+	mov.##T	r6, @r4; \
+0:	ldc	r1, sr; \
+	rts; \
+	nop; \
+	ENDFUNC(GLOBAL(sync_bool_compare_and_swap_##N))
+
+SYNC_BOOL_COMPARE_AND_SWAP(1,b)
+SYNC_BOOL_COMPARE_AND_SWAP(2,w)
+SYNC_BOOL_COMPARE_AND_SWAP(4,l)
+
+#define SYNC_FETCH_AND_OP(OP,N,T) \
+	.global	GLOBAL(sync_fetch_and_##OP##_##N); \
+	.align	1; \
+GLOBAL(sync_fetch_and_##OP##_##N):; \
+	stc	sr, r1; \
+	mov	r1, r0; \
+	or	#0xf0, r0; \
+	ldc	r0, sr; \
+	mov.##T	@r4, r2; \
+	OP	r2, r5; \
+	mov.##T	r5, @r4; \
+	ldc	r1, sr; \
+	rts; \
+	mov	r2, r0; \
+	ENDFUNC(GLOBAL(sync_fetch_and_##OP##_##N))
+
+SYNC_FETCH_AND_OP(add,1,b)
+SYNC_FETCH_AND_OP(add,2,w)
+SYNC_FETCH_AND_OP(add,4,l)
+
+SYNC_FETCH_AND_OP(or,1,b)
+SYNC_FETCH_AND_OP(or,2,w)
+SYNC_FETCH_AND_OP(or,4,l)
+
+SYNC_FETCH_AND_OP(and,1,b)
+SYNC_FETCH_AND_OP(and,2,w)
+SYNC_FETCH_AND_OP(and,4,l)
+
+SYNC_FETCH_AND_OP(xor,1,b)
+SYNC_FETCH_AND_OP(xor,2,w)
+SYNC_FETCH_AND_OP(xor,4,l)
+
+#define SYNC_FETCH_AND_COMBOP(OP,OP0,OP1,N,T) \
+	.global	GLOBAL(sync_fetch_and_##OP##_##N); \
+	.align	1; \
+GLOBAL(sync_fetch_and_##OP##_##N):; \
+	stc	sr, r1; \
+	mov	r1, r0; \
+	or	#0xf0, r0; \
+	ldc	r0, sr; \
+	mov.##T	@r4, r2; \
+	OP0	r2, r5; \
+	OP1	r5, r5; \
+	mov.##T	r5, @r4; \
+	ldc	r1, sr; \
+	rts; \
+	mov	r2, r0; \
+	ENDFUNC(GLOBAL(sync_fetch_and_##OP##_##N))
+
+SYNC_FETCH_AND_COMBOP(sub,sub,neg,1,b)
+SYNC_FETCH_AND_COMBOP(sub,sub,neg,2,w)
+SYNC_FETCH_AND_COMBOP(sub,sub,neg,4,l)
+
+SYNC_FETCH_AND_COMBOP(nand,and,not,1,b)
+SYNC_FETCH_AND_COMBOP(nand,and,not,2,w)
+SYNC_FETCH_AND_COMBOP(nand,and,not,4,l)
+
+#define SYNC_OP_AND_FETCH(OP,N,T) \
+	.global	GLOBAL(sync_##OP##_and_fetch_##N); \
+	.align	1; \
+GLOBAL(sync_##OP##_and_fetch_##N):; \
+	stc	sr, r1; \
+	mov	r1, r0; \
+	or	#0xf0, r0; \
+	ldc	r0, sr; \
+	mov.##T	@r4, r2; \
+	OP	r5, r2; \
+	mov.##T	r2, @r4; \
+	ldc	r1, sr; \
+	rts; \
+	mov	r2, r0; \
+	ENDFUNC(GLOBAL(sync_##OP##_and_fetch_##N))
+
+SYNC_OP_AND_FETCH(add,1,b)
+SYNC_OP_AND_FETCH(add,2,w)
+SYNC_OP_AND_FETCH(add,4,l)
+
+SYNC_OP_AND_FETCH(sub,1,b)
+SYNC_OP_AND_FETCH(sub,2,w)
+SYNC_OP_AND_FETCH(sub,4,l)
+
+SYNC_OP_AND_FETCH(or,1,b)
+SYNC_OP_AND_FETCH(or,2,w)
+SYNC_OP_AND_FETCH(or,4,l)
+
+SYNC_OP_AND_FETCH(and,1,b)
+SYNC_OP_AND_FETCH(and,2,w)
+SYNC_OP_AND_FETCH(and,4,l)
+
+SYNC_OP_AND_FETCH(xor,1,b)
+SYNC_OP_AND_FETCH(xor,2,w)
+SYNC_OP_AND_FETCH(xor,4,l)
+
+#define SYNC_COMBOP_AND_FETCH(OP,OP0,OP1,N,T) \
+	.global	GLOBAL(sync_##OP##_and_fetch_##N); \
+	.align	1; \
+GLOBAL(sync_##OP##_and_fetch_##N):; \
+	stc	sr, r1; \
+	mov	r1, r0; \
+	or	#0xf0, r0; \
+	ldc	r0, sr; \
+	mov.##T	@r4, r2; \
+	OP0	r2, r5; \
+	OP1	r5, r2; \
+	mov.##T	r2, @r4; \
+	ldc	r1, sr; \
+	rts; \
+	mov	r2, r0; \
+	ENDFUNC(GLOBAL(sync_##OP##_and_fetch_##N))
+
+SYNC_COMBOP_AND_FETCH(nand,and,not,1,b)
+SYNC_COMBOP_AND_FETCH(nand,and,not,2,w)
+SYNC_COMBOP_AND_FETCH(nand,and,not,4,l)
+
+#endif /* ! __SH5__ */
--- gcc/libgcc/config/sh/t-linux	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libgcc/config/sh/t-linux	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,8 +1,12 @@
-LIB1ASMFUNCS_CACHE = _ic_invalidate _ic_invalidate_array
+# Use a syscall for i-cache flushing so that all SoCs are supported by default
+# In future we will change this to a vsyscall for efficiency
+LIB1ASMFUNCS_CACHE=_ic_invalidate_syscall
 
+LIB1ASMFUNCS_DIVTABLE= _div_table 
+
 LIB2ADD = $(srcdir)/config/sh/linux-atomic.S
 
-HOST_LIBGCC2_CFLAGS += -mieee -DNO_FPSCR_VALUES
+HOST_LIBGCC2_CFLAGS += -fpic -DNO_FPSCR_VALUES
 
 # Override t-slibgcc-elf-ver to export some libgcc symbols with
 # the symbol versions that glibc used, and hide some lib1func
@@ -12,3 +16,6 @@ SHLIB_MAPFILES = \
 	libgcc-std.ver \
 	$(srcdir)/config/sh/libgcc-excl.ver \
 	$(srcdir)/config/sh/libgcc-glibc.ver
+
+
+
--- gcc/libgcc/config/sh/trap-handler.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/trap-handler.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,10 @@
+extern void exit(int) __attribute__ ((noreturn));
+
+void
+_superh_trap_handler (unsigned int trap_reason)
+{
+  exit(*(int*)0xff000024);  /* return EXPEVT */
+
+  /* in case exit returns ... */
+  while(1);
+}
--- gcc/libgcc/config/sh/crti.S	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libgcc/config/sh/crti.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -44,8 +44,8 @@ see the files COPYING3 and COPYING.RUNTIME respect
 #else
 	.p2align 1
 #endif
-	.global	 _init
-_init:
+	.global	 __init
+__init:
 #if __SHMEDIA__
 	addi	r15, -16, r15
 	st.q	r15, 8, r14
@@ -89,8 +89,8 @@ see the files COPYING3 and COPYING.RUNTIME respect
 #else
 	.p2align 1
 #endif
-	.global  _fini
-_fini:	
+	.global  __fini
+__fini:
 #if __SHMEDIA__
 	addi	r15, -16, r15
 	st.q	r15, 8, r14
--- gcc/libgcc/config/sh/lib1funcs-Os-4-200.S	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libgcc/config/sh/lib1funcs-Os-4-200.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,4 +1,5 @@
 /* Copyright (C) 2006, 2009 Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is free software; you can redistribute it and/or modify it
 under the terms of the GNU General Public License as published by the
@@ -39,7 +40,7 @@ see the files COPYING3 and COPYING.RUNTIME respect
 	.global GLOBAL(udivsi3_i4i)
 	FUNC(GLOBAL(udivsi3_i4i))
 GLOBAL(udivsi3_i4i):
-	mova L1,r0
+	mova LOCAL(L1),r0
 	cmp/pz r5
 	sts fpscr,r1
 	lds.l @r0+,fpscr
@@ -106,7 +107,7 @@ LOCAL(huge_divisor):
 	movt r0
 
 	.p2align 2
-L1:
+LOCAL(L1):
 #ifndef FMOVD_WORKS
 	.long 0x80000
 #else
@@ -272,7 +273,7 @@ LOCAL(negate_result):
 GLOBAL(sdivsi3_i4i):
 	sts.l fpscr,@-r15
 	sts fpul,r1
-	mova L1,r0
+	mova LOCAL(L1),r0
 	lds.l @r0+,fpscr
 	lds r4,fpul
 #ifdef FMOVD_WORKS
@@ -309,7 +310,7 @@ GLOBAL(sdivsi3_i4i):
 	lds r1,fpul
 
 	.p2align 2
-L1:
+LOCAL(L1):
 #ifndef FMOVD_WORKS
 	.long 0x80000
 #else
@@ -320,3 +321,4 @@ GLOBAL(sdivsi3_i4i):
 #endif /* __SH_FPU_DOUBLE__ */
 #endif /* L_sdivsi3_i4i */
 #endif /* !__SHMEDIA__ */
+	
--- gcc/libgcc/config/sh/crtn.S	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libgcc/config/sh/crtn.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -39,9 +39,15 @@ see the files COPYING3 and COPYING.RUNTIME respect
 	rts
 	add	#8,r15
 #else
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif	
 	mov	r14,r15
 	lds.l	@r15+,pr
 	mov.l	@r15+,r14
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	nop
+#endif		
 	rts
 #ifdef __ELF__
 	mov.l	@r15+,r12
@@ -65,9 +71,15 @@ see the files COPYING3 and COPYING.RUNTIME respect
 	rts
 	add	#8,r15
 #else
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	.align	2
+#endif		
 	mov	r14,r15
 	lds.l	@r15+,pr
 	mov.l	@r15+,r14
+#ifdef	DB_ST40300_BUG_WORKAROUND
+	nop
+#endif			
 	rts
 #ifdef __ELF__
 	mov.l	@r15+,r12
--- gcc/libgcc/config/sh/t-superh	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libgcc/config/sh/t-superh	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,3 +1,5 @@
+LIB2ADD = $(srcdir)/config/sh/supervisor-atomic.S $(srcdir)/config/sh/supervisor-sync.S
+
 # Compile crt1-mmu.o as crt1.o with -DMMU_SUPPORT
 crt1-mmu.o: $(srcdir)/config/sh/crt1.S
 	$(gcc_compile) -c -DMMU_SUPPORT $<
@@ -9,3 +11,6 @@ gcrt1-mmu.o: $(srcdir)/config/sh/crt1.S
 # For sh4-400: Compile gcrt1.o as crt1.o with -DPROFILE
 gcrt1.o: $(srcdir)/config/sh/crt1.S
 	$(gcc_compile) -c -DPROFILE $<
+
+trap-handler.o: $(srcdir)/config/sh/trap-handler.c
+	$(gcc_compile) $(MULTILIB_CFLAGS) -c $<
--- gcc/libgcc/config/sh/crt1.S	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libgcc/config/sh/crt1.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -2,6 +2,8 @@
    Free Software Foundation, Inc.
    This file was pretty much copied from newlib.
 
+   Copyright (c) 2006  STMicroelectronics.
+
 This file is part of GCC.
 
 GCC is free software; you can redistribute it and/or modify it
@@ -34,7 +36,7 @@ __ST_VBR:
 /* Label at the highest stack address where the stack grows from */
 __timer_stack:
 #endif /* MMU_SUPPORT */
-	
+
 	/* ;----------------------------------------
 	Normal newlib crt1.S */
 
@@ -421,7 +423,7 @@ start:
 #endif /* MMU_SUPPORT */
 
 	pt/l	.Lzero_bss_loop, tr0
-	pt/l	_init, tr5
+	pt/l	__init, tr5
 	pt/l	___setup_argv_and_call_main, tr6
 	pt/l	_exit, tr7
 
@@ -453,7 +455,7 @@ start:
 
 	! arrange for exit to call fini
 	pt/l	_atexit, tr1
-	LOAD_ADDR (_fini, r2)
+	LOAD_ADDR (__fini, r2)
 	blink	tr1, r18
 
 	! call init
@@ -851,9 +853,9 @@ exit_k:
 atexit_k:
 	.long	_atexit
 init_k:
-	.long	_init
+	.long	__init
 fini_k:
-	.long	_fini
+	.long	__fini
 #ifdef VBR_SETUP
 old_vbr_k:
 	.long	old_vbr
@@ -1169,201 +1171,5 @@ superh_trap_handler_k:
 handler_exit_k:
 	.long _exit
 	.align 2
-! Simulated compile of trap handler.
-	.section	.debug_abbrev,"",@progbits
-.Ldebug_abbrev0:
-	.section	.debug_info,"",@progbits
-.Ldebug_info0:
-	.section	.debug_line,"",@progbits
-.Ldebug_line0:
-	.text
-.Ltext0:
-	.align 5
-	.type	__superh_trap_handler,@function
-__superh_trap_handler:
-.LFB1:
-	mov.l	r14,@-r15
-.LCFI0:
-	add	#-4,r15
-.LCFI1:
-	mov	r15,r14
-.LCFI2:
-	mov.l	r4,@r14
-	lds	r1, pr
-	add	#4,r14
-	mov	r14,r15
-	mov.l	@r15+,r14
-	rts	
-	nop
-.LFE1:
-.Lfe1:
-	.size	__superh_trap_handler,.Lfe1-__superh_trap_handler
-	.section	.debug_frame,"",@progbits
-.Lframe0:
-	.ualong	.LECIE0-.LSCIE0
-.LSCIE0:
-	.ualong	0xffffffff
-	.byte	0x1
-	.string	""
-	.uleb128 0x1
-	.sleb128 -4
-	.byte	0x11
-	.byte	0xc
-	.uleb128 0xf
-	.uleb128 0x0
-	.align 2
-.LECIE0:
-.LSFDE0:
-	.ualong	.LEFDE0-.LASFDE0
-.LASFDE0:
-	.ualong	.Lframe0
-	.ualong	.LFB1
-	.ualong	.LFE1-.LFB1
-	.byte	0x4
-	.ualong	.LCFI0-.LFB1
-	.byte	0xe
-	.uleb128 0x4
-	.byte	0x4
-	.ualong	.LCFI1-.LCFI0
-	.byte	0xe
-	.uleb128 0x8
-	.byte	0x8e
-	.uleb128 0x1
-	.byte	0x4
-	.ualong	.LCFI2-.LCFI1
-	.byte	0xd
-	.uleb128 0xe
-	.align 2
-.LEFDE0:
-	.text
-.Letext0:
-	.section	.debug_info
-	.ualong	0xb3
-	.uaword	0x2
-	.ualong	.Ldebug_abbrev0
-	.byte	0x4
-	.uleb128 0x1
-	.ualong	.Ldebug_line0
-	.ualong	.Letext0
-	.ualong	.Ltext0
-	.string	"trap_handler.c"
-	.string	"xxxxxxxxxxxxxxxxxxxxxxxxxxxx"
-	.string	"GNU C 3.2 20020529 (experimental)"
-	.byte	0x1
-	.uleb128 0x2
-	.ualong	0xa6
-	.byte	0x1
-	.string	"_superh_trap_handler"
-	.byte	0x1
-	.byte	0x2
-	.byte	0x1
-	.ualong	.LFB1
-	.ualong	.LFE1
-	.byte	0x1
-	.byte	0x5e
-	.uleb128 0x3
-	.string	"trap_reason"
-	.byte	0x1
-	.byte	0x1
-	.ualong	0xa6
-	.byte	0x2
-	.byte	0x91
-	.sleb128 0
-	.byte	0x0
-	.uleb128 0x4
-	.string	"unsigned int"
-	.byte	0x4
-	.byte	0x7
-	.byte	0x0
-	.section	.debug_abbrev
-	.uleb128 0x1
-	.uleb128 0x11
-	.byte	0x1
-	.uleb128 0x10
-	.uleb128 0x6
-	.uleb128 0x12
-	.uleb128 0x1
-	.uleb128 0x11
-	.uleb128 0x1
-	.uleb128 0x3
-	.uleb128 0x8
-	.uleb128 0x1b
-	.uleb128 0x8
-	.uleb128 0x25
-	.uleb128 0x8
-	.uleb128 0x13
-	.uleb128 0xb
-	.byte	0x0
-	.byte	0x0
-	.uleb128 0x2
-	.uleb128 0x2e
-	.byte	0x1
-	.uleb128 0x1
-	.uleb128 0x13
-	.uleb128 0x3f
-	.uleb128 0xc
-	.uleb128 0x3
-	.uleb128 0x8
-	.uleb128 0x3a
-	.uleb128 0xb
-	.uleb128 0x3b
-	.uleb128 0xb
-	.uleb128 0x27
-	.uleb128 0xc
-	.uleb128 0x11
-	.uleb128 0x1
-	.uleb128 0x12
-	.uleb128 0x1
-	.uleb128 0x40
-	.uleb128 0xa
-	.byte	0x0
-	.byte	0x0
-	.uleb128 0x3
-	.uleb128 0x5
-	.byte	0x0
-	.uleb128 0x3
-	.uleb128 0x8
-	.uleb128 0x3a
-	.uleb128 0xb
-	.uleb128 0x3b
-	.uleb128 0xb
-	.uleb128 0x49
-	.uleb128 0x13
-	.uleb128 0x2
-	.uleb128 0xa
-	.byte	0x0
-	.byte	0x0
-	.uleb128 0x4
-	.uleb128 0x24
-	.byte	0x0
-	.uleb128 0x3
-	.uleb128 0x8
-	.uleb128 0xb
-	.uleb128 0xb
-	.uleb128 0x3e
-	.uleb128 0xb
-	.byte	0x0
-	.byte	0x0
-	.byte	0x0
-	.section	.debug_pubnames,"",@progbits
-	.ualong	0x27
-	.uaword	0x2
-	.ualong	.Ldebug_info0
-	.ualong	0xb7
-	.ualong	0x67
-	.string	"_superh_trap_handler"
-	.ualong	0x0
-	.section	.debug_aranges,"",@progbits
-	.ualong	0x1c
-	.uaword	0x2
-	.ualong	.Ldebug_info0
-	.byte	0x4
-	.byte	0x0
-	.uaword	0x0
-	.uaword	0x0
-	.ualong	.Ltext0
-	.ualong	.Letext0-.Ltext0
-	.ualong	0x0
-	.ualong	0x0
 #endif /* VBR_SETUP */
 #endif /* ! __SH5__ */
--- gcc/libgcc/config/sh/t-sh	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libgcc/config/sh/t-sh	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -17,16 +17,53 @@
 # along with GCC; see the file COPYING3.  If not see
 # <http://www.gnu.org/licenses/>.
 
+# _nesf2f _nedf2f _gtsf2t _gtdf2t _gesf2f _gedf2f: no-finite-math-only optimized
+# versions of _ne_sf _ne_df _gt_sf _gt_df _ge_sf _ge_df
+# _div_table 
 LIB1ASMSRC = sh/lib1funcs.S
 LIB1ASMFUNCS = _ashiftrt _ashiftrt_n _ashiftlt _lshiftrt _movmem \
   _movmem_i4 _mulsi3 _sdivsi3 _sdivsi3_i4 _udivsi3 _udivsi3_i4 _set_fpscr \
-  _div_table _udiv_qrnnd_16 \
+  _udiv_qrnnd_16 \
+  _nesf2 _nedf2 _gtsf2t _gtdf2t _gesf2f _gedf2f \
+  _addsub_sf _mul_sf _addsub_df _mul_df \
+  _hypotf \
+  _sf_to_df _df_to_sf \
+  _fixunssfsi _sf_to_si _usi_to_sf _si_to_sf \
+  _usi_to_df _si_to_df \
+  _div_sf \
+  $(LIB1ASMFUNCS_DIVTABLE) \
   $(LIB1ASMFUNCS_CACHE)
 LIB1ASMFUNCS_CACHE = _ic_invalidate _ic_invalidate_array
 
+# notyet. move above and remove bellow!
+# unord_sf/unord_df
+# _div_df \
+# _df_to_usi \
+# _df_to_si \
+
+FPBIT_FUNCS = _pack_sf _unpack_sf \
+    _fpcmp_parts_sf _compare_sf _eq_sf _gt_sf _ge_sf \
+    _lt_sf _le_sf _unord_sf _negate_sf _make_sf \
+    _sf_to_tf _thenan_sf
+
+DPBIT_FUNCS = _pack_df _unpack_df _div_df \
+    _fpcmp_parts_df _compare_df _eq_df _gt_df _ge_df \
+    _lt_df _le_df _unord_df _negate_df _make_df \
+    _df_to_tf _thenan_df _df_to_usi _df_to_si
+
 crt1.o: $(srcdir)/config/sh/crt1.S
 	$(gcc_compile) -c $<
 
+ic_invalidate.o: $(srcdir)/config/sh/lib1funcs.S
+	$(gcc_compile) -c -DL_ic_invalidate $<
+libic_invalidate.a: ic_invalidate.o
+	$(AR_CREATE_FOR_TARGET) $@ $<
+
+ic_invalidate_4a.o: $(srcdir)/config/sh/lib1funcs.S
+	$(gcc_compile) -c -DL_ic_invalidate -D__FORCE_SH4A__ -Wa,-isa=st40-300 $<
+libic_invalidate_4a.a: ic_invalidate_4a.o
+	$(AR_CREATE_FOR_TARGET) $@ $<
+
 ic_invalidate_array_4-100.o: $(srcdir)/config/sh/lib1funcs.S
 	$(gcc_compile) -c -DL_ic_invalidate_array -DWAYS=1 -DWAY_SIZE=0x2000 $<
 libic_invalidate_array_4-100.a: ic_invalidate_array_4-100.o
@@ -53,11 +90,20 @@ OBJS_Os_4_200=sdivsi3_i4i-Os-4-200.o udivsi3_i4i-O
 libgcc-Os-4-200.a: $(OBJS_Os_4_200)
 	$(AR_CREATE_FOR_TARGET) $@ $(OBJS_Os_4_200)
 
+div_table-4-200.o: $(srcdir)/config/sh/lib1funcs.S
+	$(gcc_compile) -c -DL_div_table $<
+
+libgcc-4-200.a: div_table-4-200.o $(GCC_PASSES)
+	$(AR_CREATE_FOR_TARGET) $@ div_table-4-200.o
+
 div_table-4-300.o: $(srcdir)/config/sh/lib1funcs-4-300.S
 	$(gcc_compile) -c -DL_div_table $<
 
 libgcc-4-300.a: div_table-4-300.o
 	$(AR_CREATE_FOR_TARGET) $@ div_table-4-300.o
 
-HOST_LIBGCC2_CFLAGS += -mieee
+# HOST_LIBGCC2_CFLAGS += -mieee
 
+# Local Variables:
+# mode: Makefile
+# End:
--- gcc/libgcc/config/sh/lib1funcs.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libgcc/config/sh/lib1funcs.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,6 +1,7 @@
 /* Copyright (C) 1994, 1995, 1997, 1998, 1999, 2000, 2001, 2002, 2003,
    2004, 2005, 2006, 2009
    Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is free software; you can redistribute it and/or modify it
 under the terms of the GNU General Public License as published by the
@@ -64,13 +65,152 @@ see the files COPYING3 and COPYING.RUNTIME respect
 #endif /* !__LITTLE_ENDIAN__ */
 
 #ifdef __sh1__
+/* branch with two-argument delay slot insn */
 #define SL(branch, dest, in_slot, in_slot_arg2) \
 	in_slot, in_slot_arg2; branch dest
+/* branch with one-argument delay slot insn */
 #define SL1(branch, dest, in_slot) \
 	in_slot; branch dest
+/* branch with comparison in delay slot */
+#define SLC(branch, dest, in_slot, in_slot_arg2) \
+        branch dest; in_slot, in_slot_arg2
+/* comparison in a delay slot, at branch destination */
+#define SLI(in_slot, in_slot_arg2) in_slot, in_slot_arg2
+#define SLCMP(branch, cmp1, cmp1arg2, cmp2, cmp2arg2) \
+	branch .+6; bra .+6; cmp2, cmp2arg2; cmp1, cmp1arg2
+#define DMULU_SAVE \
+ mov.l r10,@-r15; \
+ mov.l r11,@-r15; \
+ mov.l r12,@-r15; \
+ mov.l r13,@-r15
+#define DMULUL(m1, m2, rl) \
+ swap.w m1,r12; \
+ mulu.w r12,m2; \
+ swap.w m2,r13; \
+ sts macl,r10; \
+ mulu.w r13,m1; \
+ clrt; \
+ sts macl,r11; \
+ mulu.w r12,r13; \
+ addc r11,r10; \
+ sts macl,r12; \
+ mulu.w m1,m2; \
+ movt r11; \
+ sts macl,rl; \
+ mov r10,r13; \
+ shll16 r13; \
+ addc r13,rl; \
+ xtrct r11,r10; \
+ addc r10,r12 \
+/* N.B. the carry is cleared here.  */
+#define DMULUH(rh) mov r12,rh
+#define DMULU_RESTORE \
+ mov.l @r15+,r13; \
+ mov.l @r15+,r12; \
+ mov.l @r15+,r11; \
+ mov.l @r15+,r10
 #else /* ! __sh1__ */
+/* branch with two-argument delay slot insn */
 #define SL(branch, dest, in_slot, in_slot_arg2) \
-	branch##.s dest; in_slot, in_slot_arg2
+	branch##/s dest; in_slot, in_slot_arg2
+/* branch with one-argument delay slot insn */
 #define SL1(branch, dest, in_slot) \
 	branch##/s dest; in_slot
+/* branch with comparison in delay slot */
+#define SLC(branch, dest, in_slot, in_slot_arg2) \
+        branch##/s dest; in_slot, in_slot_arg2
+/* comparison in a delay slot, at branch destination */
+#define SLI(in_slot, in_slot_arg)
+#define SLCMP(branch, cmp1, cmp1arg2, cmp2, cmp2arg2) \
+	branch##/s .+6; cmp1, cmp1arg2; cmp2, cmp2arg2
+#define DMULU_SAVE
+#define DMULUL(m1, m2, rl) dmulu.l m1,m2; sts macl,rl
+#define DMULUH(rh) sts mach,rh
+#define DMULU_RESTORE
 #endif /* !__sh1__ */
+
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+/* don't #define DYN_SHIFT */
+  #define SHLL4(REG)	\
+	shll2	REG;	\
+	shll2	REG
+
+  #define SHLR4(REG)	\
+	shlr2	REG;	\
+	shlr2	REG
+
+  #define SHLL6(REG)	\
+	shll2	REG;	\
+	shll2	REG;	\
+	shll2	REG
+
+  #define SHLR6(REG)	\
+	shlr2	REG;	\
+	shlr2	REG;	\
+	shlr2	REG
+
+  #define SHLL12(REG)	\
+	shll8	REG;	\
+	SHLL4 (REG)
+
+  #define SHLR12(REG)	\
+	shlr8	REG;	\
+	SHLR4 (REG)
+
+  #define SHLR19(REG)	\
+	shlr16	REG;	\
+	shlr2	REG;	\
+	shlr	REG
+
+  #define SHLL23(REG)	\
+	shll16	REG;	\
+	shlr	REG;	\
+	shll8	REG
+
+  #define SHLR24(REG)	\
+	shlr16	REG;	\
+	shlr8	REG
+
+  #define SHLR21(REG)	\
+	shlr16	REG;	\
+	shll2	REG;	\
+	add	REG,REG;\
+	shlr8	REG
+
+  #define SHLL21(REG)	\
+	shll16	REG;	\
+	SHLL4 (REG);	\
+	add	REG,REG
+
+  #define SHLR11(REG)	\
+	shlr8	REG;	\
+	shlr2	REG;	\
+	shlr	REG
+
+  #define SHLR22(REG)	\
+	shlr16	REG;	\
+	shll2	REG;	\
+	shlr8	REG
+
+  #define SHLR23(REG)	\
+	shlr16	REG;	\
+	add	REG,REG;\
+	shlr8	REG
+
+  #define SHLR20(REG)	\
+	shlr16	REG;	\
+	SHLR4 (REG)
+
+  #define SHLL20(REG)	\
+	shll16	REG;	\
+	SHLL4 (REG)
+#define SHLD_COUNT(N,COUNT)
+#define SHLRN(N,COUNT,REG) SHLR##N(REG)
+#define SHLLN(N,COUNT,REG) SHLL##N(REG)
+#else
+#define SHLD_COUNT(N,COUNT) mov #N,COUNT
+#define SHLRN(N,COUNT,REG) shld COUNT,REG
+#define SHLLN(N,COUNT,REG) shld COUNT,REG
+#define DYN_SHIFT 1
+#endif
+
--- gcc/libgcc/config/sh/ieee-754-df.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/ieee-754-df.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,794 @@
+/* Copyright (C) 2006, 2008, 2009 Free Software Foundation, Inc.
+   Copyright (c) 2010 STMicroelectronics.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 3, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   Under Section 7 of GPL version 3, you are granted additional
+   permissions described in the GCC Runtime Library Exception, version
+   3.1, as published by the Free Software Foundation.
+
+   You should have received a copy of the GNU General Public License and
+   a copy of the GCC Runtime Library Exception along with this program;
+   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+   <http://www.gnu.org/licenses/>.  */
+
+!! libgcc software floating-point routines for Renesas SH /
+!! STMicroelectronics ST40 CPUs
+!! Contributed by J"orn Rennecke joern.rennecke@st.com
+
+#ifndef __SH_FPU_DOUBLE__
+
+#include "lib1funcs.h"
+#include "insn-constants.h"
+
+/* Double-precision floating-point emulation.
+   We handle NANs, +-infinity, and +-zero.
+   However, we assume that for NANs, the topmost bit of the fraction is set.  */
+
+#ifdef __LITTLE_ENDIAN__
+#define DBL0L r4
+#define DBL0H r5
+#define DBL1L r6
+#define DBL1H r7
+#define DBLRL r0
+#define DBLRH r1
+#else
+#define DBL0L r5
+#define DBL0H r4
+#define DBL1L r7
+#define DBL1H r6
+#define DBLRL r1
+#define DBLRH r0
+#endif
+
+#ifdef __SH_FPU_ANY__
+#define RETURN_R0_MAIN
+#define RETURN_R0 bra LOCAL(return_r0)
+#define RETURN_FR0 \
+LOCAL(return_r0): \
+ lds r0,fpul; \
+ rts; \
+ fsts fpul,fr0
+#define ARG_TO_R4 \
+ flds fr4,fpul; \
+ sts fpul,r4
+#else /* ! __SH_FPU_ANY__ */
+#define RETURN_R0_MAIN rts
+#define RETURN_R0 rts
+#define RETURN_FR0
+#define ARG_TO_R4
+#endif /* ! __SH_FPU_ANY__ */
+
+#ifdef L_nedf2
+/* -fno-finite-math-only -mb inline version, T := r4:DF == r6:DF
+	cmp/eq	r5,r7
+	mov	r4,r0
+	bf	0f
+	cmp/eq	r4,r6
+	bt	0f
+	or	r6,r0
+	add	r0,r0
+	or	r5,r0
+	tst	r0,r0
+	0:			*/
+	.balign 4
+	.global GLOBAL(nedf2)
+	HIDDEN_FUNC(GLOBAL(nedf2))
+GLOBAL(nedf2):
+	cmp/eq	DBL0L,DBL1L
+	bf.s 	LOCAL(ne)
+	mov     #1,r0
+	cmp/eq	DBL0H,DBL1H
+	mov.l   LOCAL(c_DF_NAN_MASK),r1
+	bt.s	LOCAL(check_nan)
+	not	DBL0H,r0
+	mov	DBL0H,r0
+	or	DBL1H,r0
+	add	r0,r0
+	rts
+	or	DBL0L,r0
+LOCAL(check_nan):
+	tst	r1,r0
+	bt.s 	LOCAL(nan)
+	mov	#12,r2
+	shll16  r2
+	xor 	r2,r1
+	tst 	r1,r0
+LOCAL(nan):
+	movt	r0
+LOCAL(ne):
+	rts
+	nop
+
+	.balign 4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(GLOBAL(nedf2))
+#endif /* L_nedf2 */
+
+#ifdef L_unord_df
+	.balign 4
+	.global GLOBAL(unorddf2)
+	HIDDEN_FUNC(GLOBAL(unorddf2))
+GLOBAL(unorddf2):
+	mov.l	LOCAL(c_DF_NAN_MASK),r1
+	not	DBL0H,r0
+	tst	r1,r0
+	not	r6,r0
+	bt	LOCAL(unord)
+	tst	r1,r0
+LOCAL(unord):
+	rts
+	movt	r0
+	.balign	4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(GLOBAL(unorddf2))
+#endif /* L_unord_df */
+
+#if defined(L_gtdf2) || defined(L_gtdf2_trap)
+/* -fno-finite-math-only version of _gt_df */
+#ifdef L_gtdf2
+#define fun_label GLOBAL(gtdf2)
+#else
+#define fun_label GLOBAL(gtdf2_trap)
+#endif
+	.balign 4
+	.global fun_label
+	HIDDEN_FUNC(fun_label)
+fun_label:
+	/* If the raw values compare greater, the result true, unless
+	   any of them is a nan (but infinity is fine), or both values are
+	   +- zero.  Otherwise, the result false.  */
+	mov.l	LOCAL(c_DF_NAN_MASK),r1
+	cmp/pz	DBL0H
+	not	DBL1H,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	DBL0H,r0
+	bt	LOCAL(nan) /* return zero if DBL1 is NAN.  */
+	cmp/eq	DBL1H,DBL0H
+	bt	LOCAL(cmp_low)
+	cmp/gt	DBL1H,DBL0H
+	or	DBL1H,r0
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/gt	DBL0H,r1)
+	add	r0,r0
+	bf	LOCAL(nan) /* return zero if DBL0 is NAN.  */
+	or	DBL0L,r0
+	rts
+	or	DBL1L,r0 /* non-zero unless both DBL0 and DBL1 are +-zero.  */
+LOCAL(cmp_low):
+	cmp/hi	DBL1L,DBL0L
+	rts
+	movt	r0
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan) /* return zero if DBL1 is NAN.  */
+	cmp/eq	DBL1H,DBL0H
+	SLC(bt,	LOCAL(neg_cmp_low),
+	 cmp/hi	DBL0L,DBL1L)
+	not	DBL0H,r0
+	tst	r1,r0
+	bt	LOCAL(nan) /* return zero if DBL0 is NAN.  */
+	cmp/hi	DBL0H,DBL1H
+	SLI(rts	!,)
+	SLI(movt r0 !,)
+LOCAL(neg_cmp_low):
+	SLI(cmp/hi	DBL0L,DBL1L)
+	rts
+	movt	r0
+LOCAL(check_nan):
+#ifdef L_gtdf2
+LOCAL(nan):
+	rts
+	mov	#0,r0
+#else
+	SLI(cmp/gt DBL0H,r1)
+	bf	LOCAL(nan) /* return zero if DBL0 is NAN.  */
+	rts
+	mov	#0,r0
+LOCAL(nan):
+	mov	#0,r0
+	trapa	#0
+#endif
+	.balign	4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(fun_label)
+#endif /* defined(L_gtdf2) || defined(L_gtdf2_trap) */
+
+#ifdef L_gedf2
+	.balign 4
+	.global GLOBAL(gedf2)
+	HIDDEN_FUNC(GLOBAL(gedf2))
+GLOBAL(gedf2):
+	/* -fno-finite-math-only version of _ge_df */
+	/* If the raw values compare greater or equal, the result is
+	   true, unless any of them is a nan, or both are the
+	   same infinity.  If both are -+zero, the result is true;
+	   otherwise, it is false.
+	   We use 0 as true and nonzero as false for this function.  */
+	mov.l	LOCAL(c_DF_NAN_MASK),r1
+	cmp/pz	DBL1H
+	not	DBL0H,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	DBL0H,r0
+	bt	LOCAL(nan)
+	cmp/eq	DBL0H,DBL1H
+	bt	LOCAL(cmp_low)
+	cmp/gt	DBL0H,DBL1H
+	or	DBL1H,r0
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/ge	r1,DBL1H)
+	add	r0,r0
+	bt	LOCAL(nan)
+	or	DBL0L,r0
+	rts
+	or	DBL1L,r0
+LOCAL(cmp_low):
+	cmp/hi	DBL0L,DBL1L
+#if defined(L_gedf2) && defined(DELAYED_BRANCHES)
+LOCAL(nan): LOCAL(check_nan):
+#endif
+	rts
+	movt	r0
+#if defined(L_gedf2) && ! defined(DELAYED_BRANCHES)
+LOCAL(check_nan):
+	SLI(cmp/ge	r1,DBL1H)
+LOCAL(nan):
+	rts
+	movt	r0
+#elif defined(L_gedf2_trap)
+LOCAL(check_nan):
+	SLI(cmp/ge	r1,DBL1H)
+	bt	LOCAL(nan)
+	rts
+LOCAL(nan):
+	movt	r0
+	trapa	#0
+#endif /* L_gedf2_trap */
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan)
+	cmp/eq	DBL0H,DBL1H
+	not	DBL1H,r0
+	SLC(bt,	LOCAL(neg_cmp_low),
+	 cmp/hi	DBL1L,DBL0L)
+	tst	r1,r0
+	bt	LOCAL(nan)
+	cmp/hi	DBL1H,DBL0H
+	SLI(rts !,)
+	SLI(movt	r0 !,)
+LOCAL(neg_cmp_low):
+	SLI(cmp/hi	DBL1L,DBL0L)
+	rts
+	movt	r0
+	.balign	4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(GLOBAL(gedf2))
+#endif /* L_gedf2 */
+
+#ifndef DYN_SHIFT /* SH1 / SH2 code */
+#ifdef L_sf_to_df
+	.balign 4
+	.global GLOBAL(extendsfdf2)
+	FUNC(GLOBAL(extendsfdf2))
+GLOBAL(extendsfdf2):
+	ARG_TO_R4
+	mov.l	LOCAL(x7f800000),r3
+	mov	r4,DBLRL
+	tst	r3,r4
+	bt	LOCAL(zero_denorm)
+	mov.l	LOCAL(xe0000000),r2
+	rotr	DBLRL
+	rotr	DBLRL
+	rotr	DBLRL
+	and	r2,DBLRL
+	mov	r4,DBLRH
+	not	r4,r2
+	tst	r3,r2
+	mov.l	LOCAL(x38000000),r2
+	bf	0f
+	add	r2,r2	! infinity / NaN adjustment
+0:	shll	DBLRH
+	shlr2	DBLRH
+	shlr2	DBLRH
+	add	DBLRH,DBLRH
+	rotcr	DBLRH
+	rts
+	add	r2,DBLRH
+LOCAL(zero_denorm):
+	mov.l	r4,@-r15
+	add	r4,r4
+	tst	r4,r4
+	bt	LOCAL(zero)
+	shlr8	r3	/* 0x007f8000 */
+	mov.w	LOCAL(x389),r2
+LOCAL(shift_byte):
+	tst	r3,r4
+	shll8	r4
+	SL(bt,	LOCAL(shift_byte),
+	 add	#-8,r2)
+LOCAL(shift_bit):
+	shll	r4
+	SL(bf,	LOCAL(shift_bit),
+	 add	#-1,r2)
+	mov	#0,DBLRL
+	mov	r4,DBLRH
+	mov.l	@r15+,r4
+	shlr8	DBLRH
+	shlr2	DBLRH
+	shlr	DBLRH
+	rotcr	DBLRL
+	cmp/gt	r4,DBLRH	! get sign
+	rotcr	DBLRH
+	rotcr	DBLRL
+	shll16	r2
+	shll8	r2
+	rts
+	add	r2,DBLRH
+LOCAL(zero):
+	mov.l	@r15+,DBLRH
+	rts
+	mov	#0,DBLRL
+LOCAL(x389):	.word 0x389
+	.balign	4
+LOCAL(x7f800000):
+	.long	0x7f800000
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(xe0000000):
+	.long	0xe0000000
+	ENDFUNC(GLOBAL(extendsfdf2))
+#endif /* L_sf_to_df */
+
+#ifdef L_df_to_sf
+	.balign 4
+	.global GLOBAL(truncdfsf2)
+	FUNC(GLOBAL(truncdfsf2))
+GLOBAL(truncdfsf2):
+	mov.l	LOCAL(x38000000),r3	! exponent adjustment DF -> SF
+	mov	DBL0H,r1
+	mov.l	LOCAL(x70000000),r2	! mask for out-of-range exponent bits
+	mov	DBL0H,r0
+	mov.l	DBL0L,@-r15
+	sub	r3,r1
+	tst	r2,r1
+	shll8	r0			!
+	shll2	r0			! Isolate highpart fraction.
+	shll2	r0			!
+	bf	LOCAL(ill_exp)
+	shll2	r1
+	mov.l	LOCAL(x2fffffff),r2 /* Fraction lsb | lower guard bits.  */
+	shll2	r1
+	mov.l	LOCAL(xff000000),r3
+	shlr8	r0
+	tst	r2,DBL0L /* Check if msb guard bit wants rounding up.  */
+	shlr16	DBL0L
+	shlr8	DBL0L
+	shlr2	DBL0L
+	SL1(bt,	LOCAL(add_frac),
+	 shlr2	DBL0L)
+	add	#1,DBL0L
+LOCAL(add_frac):
+	add	DBL0L,r0
+	mov.l	LOCAL(x01000000),r2
+	and	r3,r1
+	mov.l	@r15+,DBL0L
+	add	r1,r0
+	tst	r3,r0
+	bt	LOCAL(inf_denorm0)
+	cmp/hs	r3,r0
+LOCAL(denorm_noup_sh1):
+	bt	LOCAL(inf)
+	div0s	DBL0H,r2	/* copy orig. sign into T.  */
+RETURN_R0_MAIN
+	rotcr	r0
+RETURN_FR0
+LOCAL(inf_denorm0):	!  We might need to undo previous rounding.
+	mov.l	LOCAL(x2fffffff),r3 /* Old fraction lsb | lower guard bits.  */
+	tst	r1,r1
+	bf	LOCAL(inf)
+	add	#-1,r0
+	tst	r3,DBL0L /* Check if msb guard bit was rounded up.  */
+	mov.l	LOCAL(x5fffffff),r3 /* Fraction lsb | lower guard bits.  */
+	addc	r2,r0
+	shlr	r0
+	tst	r3,DBL0L /* Check if msb guard bit wants rounding up.  */
+#ifdef DELAYED_BRANCHES
+	bt/s	LOCAL(denorm_noup)
+#else
+	bt	LOCAL(denorm_noup_sh1)
+#endif
+	div0s	DBL0H,r2	/* copy orig. sign into T.  */
+	add	#1,r0
+LOCAL(denorm_noup):
+	RETURN_R0
+	rotcr	r0
+LOCAL(ill_exp):
+	div0s	DBL0H,r1
+	mov.l	LOCAL(x7ff80000),r2
+	add	r1,r1
+	bf	LOCAL(inf_nan)
+	mov.w	LOCAL(m32),r3 /* Handle denormal or zero.  */
+	shlr16	r1
+	exts.w	r1,r1
+	shll2	r1
+	add	r1,r1
+	shlr8	r1
+	exts.w	r1,r1
+	add	#-8,r1	/* Go from 9 to 1 guard bit in MSW.  */
+	cmp/gt	r3,r1
+	mov.l	@r15+,r3 /* DBL0L */
+	bf	LOCAL(zero)
+	mov.l	DBL0L, @-r15
+	shll8	DBL0L
+	rotcr	r0	/* Insert leading 1.  */
+	shlr16	r3
+	shll2	r3
+	add	r3,r3
+	shlr8	r3
+	cmp/pl	DBL0L	/* Check lower 23 guard bits if guard bit 23 is 0.  */
+	addc	r3,r0	/* Assemble fraction with compressed guard bits.  */
+	mov.l	@r15+,DBL0L
+	mov	#0,r2
+	neg	r1,r1
+LOCAL(denorm_loop):
+	shlr	r0
+	rotcl	r2
+	dt	r1
+	bf	LOCAL(denorm_loop)
+	tst	#2,r0
+	rotcl	r0
+	tst	r2,r2
+	rotcl	r0
+	xor	#3,r0
+	add	#3,r0	/* Even overflow gives the correct result.  */
+	shlr2	r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(zero):
+	mov	#0,r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(inf_nan):
+	not	DBL0H,r0
+	tst	r2,r0
+	mov.l	@r15+,DBL0L
+	bf	LOCAL(inf)
+	RETURN_R0
+	mov	#-1,r0	/* NAN */
+LOCAL(inf):	/* r2 must be positive here.  */
+	mov.l	LOCAL(xff000000),r0
+	div0s	r2,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(m32):
+	.word	-32
+	.balign	4
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(x70000000):
+	.long	0x70000000
+LOCAL(x2fffffff):
+	.long	0x2fffffff
+LOCAL(x01000000):
+	.long	0x01000000
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(x5fffffff):
+	.long	0x5fffffff
+LOCAL(x7ff80000):
+	.long	0x7ff80000
+	ENDFUNC(GLOBAL(truncdfsf2))
+#endif /*  L_df_to_sf */
+#ifdef L_addsub_df
+#include "IEEE-754/adddf3.S"
+#endif /* _addsub_df */
+
+#ifdef L_mul_df
+#include "IEEE-754/muldf3.S"
+#endif /* L_mul_df */
+
+#ifdef L_df_to_usi
+#include "IEEE-754/fixunsdfsi.S"
+#endif /* L_df_to_usi */
+
+#ifdef L_df_to_si
+#include "IEEE-754/fixdfsi.S"
+#endif /* L_df_to_si */
+
+#ifdef L_usi_to_df
+#include "IEEE-754/floatunssidf.S"
+#endif /* L_usi_to_df */
+
+#ifdef L_si_to_df
+#include "IEEE-754/floatsidf.S"
+#endif /* L_si_df */
+
+#ifdef L_div_df
+#include "IEEE-754/divdf3.S"
+#endif /* L_div_df */
+#endif /* ! DYN_SHIFT */
+
+/* The actual arithmetic uses dynamic shift.  Supporting SH1 / SH2 here would
+   make this code too hard to maintain, so if you want to add SH1 / SH2
+   support, do it in a separate copy.  */
+#ifdef DYN_SHIFT
+#ifdef L_sf_to_df
+	.balign 4
+	.global GLOBAL(extendsfdf2)
+	FUNC(GLOBAL(extendsfdf2))
+GLOBAL(extendsfdf2):
+	ARG_TO_R4
+	mov.l	LOCAL(x7f800000),r2
+	mov	#29,r3
+	mov	r4,DBLRL
+	not	r4,DBLRH
+	tst	r2,r4
+	shld	r3,DBLRL
+	bt	LOCAL(zero_denorm)
+	mov	#-3,r3
+	tst	r2,DBLRH
+	mov	r4,DBLRH
+	mov.l	LOCAL(x38000000),r2
+	bt/s	LOCAL(inf_nan)
+	 shll	DBLRH
+	shld	r3,DBLRH
+	rotcr	DBLRH
+	rts
+	add	r2,DBLRH
+	.balign	4
+LOCAL(inf_nan):
+	shld	r3,DBLRH
+	add	r2,r2
+	rotcr	DBLRH
+	rts
+	add	r2,DBLRH
+LOCAL(zero_denorm):
+	mov.l	r4,@-r15
+	add	r4,r4
+	tst	r4,r4
+	extu.w	r4,r2
+	bt	LOCAL(zero)
+	cmp/eq	r4,r2
+	extu.b	r4,r1
+	mov.l	LOCAL(c__clz_tab),r0
+	bf	LOCAL(three_bytes)
+	nop
+	cmp/eq	r4,r1
+	mov	#22,DBLRH
+	bt	LOCAL(one_byte)
+	shlr8	r2
+	mov	#14,DBLRH
+LOCAL(one_byte):
+#ifdef __pic__
+	add	r0,r2
+	mova  LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r2),r2
+	mov	#21,r3
+	mov.w	LOCAL(x0),DBLRL
+	sub	r2,DBLRH
+LOCAL(norm_shift):
+	shld	DBLRH,r4
+	neg	DBLRH,DBLRH
+	mov.l	@r15+,r2
+	shld	r3,DBLRH
+	mov.l	LOCAL(x6fa00000),r3
+	add	r4,DBLRH
+	mov r2,r4
+	add	r3,DBLRH
+
+	div0s	r3,r4
+	rts
+	rotcr	DBLRH
+LOCAL(three_bytes):
+	mov	r4,r2
+	shlr16	r2
+#ifdef __pic__
+	add	r0,r2
+	mova  LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r2),r2
+	mov	#21,r3
+	mov	#6+10,DBLRH
+	sub	r2,DBLRH
+	mov	r4,DBLRL
+	shld	r3,DBLRL
+	shld	DBLRH,DBLRL
+	bra	LOCAL(norm_shift)
+	add	#-10,DBLRH
+LOCAL(zero):
+	rts	/* DBLRL has already been zeroed above.  */
+	mov.l @r15+,DBLRH
+LOCAL(x0):
+	.word 0
+	.balign	4
+LOCAL(x7f800000):
+	.long	0x7f800000
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(x6fa00000):
+	/* Flip sign back, do exponent adjustment, and remove leading one.  */
+	.long 0x6fa00000 
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+	ENDFUNC(GLOBAL(extendsfdf2))
+#endif /* L_sf_to_df */
+
+#ifdef L_df_to_sf
+	.balign 4
+	.global GLOBAL(truncdfsf2)
+	FUNC(GLOBAL(truncdfsf2))
+GLOBAL(truncdfsf2):
+	mov.l	LOCAL(x38000000),r3
+	mov	DBL0H,r1
+	mov.l	LOCAL(x70000000),r2
+	mov	DBL0H,r0
+	sub	r3,r1
+	mov.l	DBL0L,@-r15
+	tst	r2,r1
+	mov	#12,r3
+	shld	r3,r0			! Isolate highpart fraction.
+	bf	LOCAL(ill_exp)
+	shll2	r1
+	mov.l	LOCAL(x2fffffff),r2 /* Fraction lsb | lower guard bits.  */
+	shll2	r1
+	mov.l	LOCAL(xff000000),r3
+	shlr8	r0
+	tst	r2,DBL0L /* Check if msb guard bit wants rounding up.  */
+	mov	#-28,r2
+	bt/s	LOCAL(add_frac)
+	 shld	r2,DBL0L
+	add	#1,DBL0L
+LOCAL(add_frac):
+	add	DBL0L,r0
+	mov.l	LOCAL(x01000000),r2
+	and	r3,r1
+	mov.l	@r15+,DBL0L
+	add	r1,r0
+	tst	r3,r0
+	bt	LOCAL(inf_denorm0)
+	cmp/hs	r3,r0
+	bt	LOCAL(inf)
+	div0s	DBL0H,r2	/* copy orig. sign into T.  */
+	RETURN_R0_MAIN
+	rotcr	r0
+RETURN_FR0
+LOCAL(inf_denorm0):	! We might need to undo previous rounding.
+	mov.l	LOCAL(x2fffffff),r3 /* Old fraction lsb | lower guard bits.  */
+	tst	r1,r1
+	bf	LOCAL(inf)
+	add	#-1,r0
+	tst	r3,DBL0L /* Check if msb guard bit was rounded up.  */
+	mov.l	LOCAL(x5fffffff),r3 /* Fraction lsb | lower guard bits.  */
+	addc	r2,r0
+	shlr	r0
+	tst	r3,DBL0L /* Check if msb guard bit wants rounding up.  */
+	bt/s	LOCAL(denorm_noup)
+	 div0s	DBL0H,r2	/* copy orig. sign into T.  */
+	add	#1,r0
+LOCAL(denorm_noup):
+	RETURN_R0
+	rotcr	r0
+LOCAL(ill_exp):
+	div0s	DBL0H,r1
+	mov.l	LOCAL(x7ff80000),r2
+	add	r1,r1
+	bf	LOCAL(inf_nan)
+	mov.w	LOCAL(m32),r3 /* Handle denormal or zero.  */
+	mov	#-21,r2
+	shad	r2,r1
+	add	#-8,r1	/* Go from 9 to 1 guard bit in MSW.  */
+	cmp/gt	r3,r1
+	mov.l	@r15+,r3 /* DBL0L */
+	bf	LOCAL(zero)
+	mov.l	DBL0L, @-r15
+	shll8	DBL0L
+	rotcr	r0	/* Insert leading 1.  */
+	shld	r2,r3
+	cmp/pl	DBL0L	/* Check lower 23 guard bits if guard bit 23 is 0.  */
+	addc	r3,r0	/* Assemble fraction with compressed guard bits.  */
+	mov	r0,r2
+	shld	r1,r0
+	mov.l	@r15+,DBL0L
+	add	#32,r1
+	shld	r1,r2
+	tst	#2,r0
+	rotcl	r0
+	tst	r2,r2
+	rotcl	r0
+	xor	#3,r0
+	add	#3,r0	/* Even overflow gives the correct result.  */
+	shlr2	r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(zero):
+	mov	#0,r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(inf_nan):
+	not	DBL0H,r0
+	tst	r2,r0
+	mov.l	@r15+,DBL0L
+	bf	LOCAL(inf)
+	RETURN_R0
+	mov	#-1,r0	/* NAN */
+LOCAL(inf):	/* r2 must be positive here.  */
+	mov.l	LOCAL(xff000000),r0
+	div0s	r2,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(m32):
+	.word	-32
+	.balign	4
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(x70000000):
+	.long	0x70000000
+LOCAL(x2fffffff):
+	.long	0x2fffffff
+LOCAL(x01000000):
+	.long	0x01000000
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(x5fffffff):
+	.long	0x5fffffff
+LOCAL(x7ff80000):
+	.long	0x7ff80000
+	ENDFUNC(GLOBAL(truncdfsf2))
+#endif /* L_df_to_sf */
+
+
+#ifdef L_addsub_df
+#include "IEEE-754/m3/adddf3.S"
+#endif /* _addsub_df */
+
+#ifdef L_mul_df
+#include "IEEE-754/m3/muldf3.S"
+#endif /* L_mul_df */
+
+#ifdef L_df_to_usi
+#include "IEEE-754/m3/fixunsdfsi.S"
+#endif /* L_df_to_usi */
+
+#ifdef L_df_to_si
+#include "IEEE-754/m3/fixdfsi.S"
+#endif /* L_df_to_si */
+
+#ifdef L_usi_to_df
+#include "IEEE-754/m3/floatunssidf.S"
+#endif /* L_usi_to_df */
+
+#ifdef L_si_to_df
+#include "IEEE-754/m3/floatsidf.S"
+#endif /* L_si_to_df */
+
+#ifdef L_div_df
+#include "IEEE-754/m3/divdf3.S"
+#endif /* L_div_df */
+#endif /* DYN_SHIFT */
+
+#endif /* __SH_FPU_DOUBLE__ */
--- gcc/libgcc/config/sh/t-generic	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/config/sh/t-generic	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,2 @@
+# Provide dummy generic threads functions
+LIB2ADD += $(srcdir)/gthr-generic.c
--- gcc/libgcc/configure	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libgcc/configure	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -4479,6 +4479,7 @@ case $target_thread_file in
     single)	thread_header=gthr-single.h ;;
     tpf)	thread_header=config/s390/gthr-tpf.h ;;
     vxworks)	thread_header=config/gthr-vxworks.h ;;
+    generic)	thread_header=gthr-generic.h ;;
     win32)	thread_header=config/i386/gthr-win32.h ;;
 esac
 
--- gcc/libgcc/ChangeLog.STM	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libgcc/ChangeLog.STM	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,23 @@
+2013-02-12  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/supervisor-atomic.S: New file.
+	* config/sh/supervisor-sync.S: Rename.
+	* config/sh/t-superh (LIB2ADD): Add supervisor-sync.S
+
+2012-11-13  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/lib1funcs.S (movmem): Remove strmem aliases.
+
+2012-04-04  Antony King  <antony.king@st.com>
+	Christian Bruel  <christian.bruel@st.com>
+
+	* gthr-generic.c (__generic_gxx_mutex_destroy): New function.
+	(__generic_gxx_recursive_mutex_destroy): Likewise.
+	* gthr-generic.h (__generic_gxx_mutex_destroy): Declare.
+	(__generic_gxx_recursive_mutex_destroy): Likewise.
+
+2009-07-27  Christian Bruel  <christian.bruel@st.com>
+
+        * config.host (extra_parts): Set for sh*-*-linux.
+	(tmake_file): Add t-extra for sh*-*-linux.
+	* config/sh/t-extra: New file to build optimized libgcc objects.
--- gcc/ChangeLog.STM	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/ChangeLog.STM	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,41 @@
+2010-04-12  Christian Bruel  <christian.bruel@st.com>
+
+	* config/mh-mingw (LDFLAGS): Remove wrap rename.
+
+2009-12-07  Yvan Roux  <yvan.roux@st.com>
+
+	* config/mh-mingw (LDFLAGS): Wrap syscall for cygwin path support.
+
+2009-10-22  Christian Bruel  <christian.bruel@st.com>
+
+	* configure.ac: Use include-fixed header path for canadian cross build.
+	* configure: Regenerate.
+
+2009-02-03  Christian Bruel  <christian.bruel@st.com>
+
+	* config/mt-ospace: Don't overwrite CFLAGS_FOR_TARGET.
+	* config/mt-relax: Not supported for c++.
+
+2008-05-26  Christian Bruel  <christian.bruel@st.com>
+
+	* config/mt-relax: New file.
+	* Makefile.in: Add relax fragment.
+	* Makefile.tpl: Likewise
+	* configure.ac: Likewise.
+	* configure: Regenerate.
+
+2008-09-30  Christian Bruel  <christian.bruel@st.com>
+
+	* configure.in: Allow libgloss configure for sh.
+	* configure: Regenerate.
+
+2008-05-26  Christian Bruel  <christian.bruel@st.com>
+
+	* config/mt-alphaieee: Removed.
+	* config/mt-ieee: Renamed from mt-alphaieee.
+	* Makefile.in: alphaieee_frag renamed ieee_frag.
+	ieee_frag must be included after ospace_frag.
+	* Makefile.tpl: Likewise
+	* configure.ac: Likewise. Enable for sh.
+	* configure: Regenerate.
+
--- gcc/gcc/fwprop.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/fwprop.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -664,7 +664,12 @@ propagate_rtx (rtx x, enum machine_mode mode, rtx
     return NULL_RTX;
 
   flags = 0;
-  if (REG_P (new_rtx) || CONSTANT_P (new_rtx))
+  if (REG_P (new_rtx)
+      || CONSTANT_P (new_rtx)
+      || (GET_CODE (new_rtx) == SUBREG
+	  && REG_P (SUBREG_REG (new_rtx))
+	  && (GET_MODE_SIZE (mode)
+	      <= GET_MODE_SIZE (GET_MODE (SUBREG_REG (new_rtx))))))
     flags |= PR_CAN_APPEAR;
   if (!for_each_rtx (&new_rtx, varying_mem_p, NULL))
     flags |= PR_HANDLE_MEM;
--- gcc/gcc/doc/tm.texi	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/doc/tm.texi	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -3434,6 +3434,13 @@ info to be given comdat linkage.  Define it to be
 linkage is necessary.  The default is @code{0}.
 @end defmac
 
+@defmac TARGET_USES_LEB128
+A C expression that evaluates to true if the target requires leb128
+to be used for dwarf compression.  Define it to be @code{1} if leb128
+linkage is necessary.  The default is @code{1} if @code{HAVE_AS_LEB128}
+is defined.
+@end defmac
+
 @node Stack Checking
 @subsection Specifying How Stack Checking is Done
 
@@ -9776,10 +9783,10 @@ for @var{entity}.  For any fixed @var{entity}, @co
 @code{num_modes_for_mode_switching[@var{entity}] - 1}.
 @end defmac
 
-@defmac EMIT_MODE_SET (@var{entity}, @var{mode}, @var{hard_regs_live})
+@defmac EMIT_MODE_SET (@var{entity}, @var{mode}, @var{flip}, @var{hard_regs_live})
 Generate one or more insns to set @var{entity} to @var{mode}.
 @var{hard_reg_live} is the set of hard registers live at the point where
-the insn(s) are to be inserted.
+the insn(s) are to be inserted. @var{flip} is a boolean to indicate that current mode can be flipped.
 @end defmac
 
 @node Target Attributes
--- gcc/gcc/doc/tm.texi.in	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/doc/tm.texi.in	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -3414,6 +3414,13 @@ info to be given comdat linkage.  Define it to be
 linkage is necessary.  The default is @code{0}.
 @end defmac
 
+@defmac TARGET_USES_LEB128
+A C expression that evaluates to true if the target requires leb128
+to be used for dwarf compression.  Define it to be @code{1} if leb128
+linkage is necessary.  The default is @code{1} if @code{HAVE_AS_LEB128}
+is defined.
+@end defmac
+
 @node Stack Checking
 @subsection Specifying How Stack Checking is Done
 
@@ -9665,10 +9672,10 @@ for @var{entity}.  For any fixed @var{entity}, @co
 @code{num_modes_for_mode_switching[@var{entity}] - 1}.
 @end defmac
 
-@defmac EMIT_MODE_SET (@var{entity}, @var{mode}, @var{hard_regs_live})
+@defmac EMIT_MODE_SET (@var{entity}, @var{mode}, @var{flip}, @var{hard_regs_live})
 Generate one or more insns to set @var{entity} to @var{mode}.
 @var{hard_reg_live} is the set of hard registers live at the point where
-the insn(s) are to be inserted.
+the insn(s) are to be inserted. @var{flip} is a boolean to indicate that current mode can be flipped.
 @end defmac
 
 @node Target Attributes
--- gcc/gcc/doc/invoke.texi	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/doc/invoke.texi	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,6 +1,7 @@
 @c Copyright (C) 1988, 1989, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,
 @c 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012
 @c Free Software Foundation, Inc.
+@c Copyright (c) 2011  STMicroelectronics.
 @c This is part of the GCC manual.
 @c For copying conditions, see the file gcc.texi.
 
@@ -236,6 +237,7 @@ Objective-C and Objective-C++ Dialects}.
 @gccoptlist{-fsyntax-only  -fmax-errors=@var{n}  -pedantic @gol
 -pedantic-errors @gol
 -w  -Wextra  -Wall  -Waddress  -Waggregate-return  -Warray-bounds @gol
+-Wbranch-probabilities-computation
 -Wno-attributes -Wno-builtin-macro-redefined @gol
 -Wc++-compat -Wc++11-compat -Wcast-align  -Wcast-qual  @gol
 -Wchar-subscripts -Wclobbered  -Wcomment @gol
@@ -256,7 +258,7 @@ Objective-C and Objective-C++ Dialects}.
 -Wmain -Wmaybe-uninitialized -Wmissing-braces  -Wmissing-field-initializers @gol
 -Wmissing-format-attribute  -Wmissing-include-dirs @gol
 -Wno-mudflap @gol
--Wno-multichar  -Wnonnull  -Wno-overflow @gol
+-Wno-multichar -Wnonnull -Wnon-finite-math -Wno-overflow @gol
 -Woverlength-strings  -Wpacked  -Wpacked-bitfield-compat  -Wpadded @gol
 -Wparentheses  -Wpedantic-ms-format -Wno-pedantic-ms-format @gol
 -Wpointer-arith  -Wno-pointer-to-int-cast @gol
@@ -399,6 +401,7 @@ Objective-C and Objective-C++ Dialects}.
 -fschedule-insns -fschedule-insns2 -fsection-anchors @gol
 -fselective-scheduling -fselective-scheduling2 @gol
 -fsel-sched-pipelining -fsel-sched-pipelining-outer-loops @gol
+-fcse-sincos @gol
 -fshrink-wrap -fsignaling-nans -fsingle-precision-constant @gol
 -fsplit-ivs-in-unroller -fsplit-wide-types -fstack-protector @gol
 -fstack-protector-all -fstrict-aliasing -fstrict-overflow @gol
@@ -877,16 +880,17 @@ See RS/6000 and PowerPC Options.
 -m2a-nofpu -m2a-single-only -m2a-single -m2a @gol
 -m3  -m3e @gol
 -m4-nofpu  -m4-single-only  -m4-single  -m4 @gol
+-m4-300-nofpu  -m4-300-single-only  -m4-300-single  -m4-300 @gol
 -m4a-nofpu -m4a-single-only -m4a-single -m4a -m4al @gol
 -m5-64media  -m5-64media-nofpu @gol
 -m5-32media  -m5-32media-nofpu @gol
 -m5-compact  -m5-compact-nofpu @gol
--mb  -ml  -mdalign  -mrelax @gol
+-mb  -ml  -mdalign  -mrelax -malign-small-blocks=@var{block-size} @gol
 -mbigtable -mfmovd -mhitachi -mrenesas -mno-renesas -mnomacsave @gol
--mieee -mno-ieee -mbitops  -misize  -minline-ic_invalidate -mpadstruct @gol
--mspace -mprefergot  -musermode -multcost=@var{number} -mdiv=@var{strategy} @gol
+-mieee -mno-ieee  -mbitops  -misize  -minline-ic_invalidate -mpadstruct @gol
+-mprefergot  -musermode -multcost=@var{number} -mdiv=@var{strategy} @gol
 -mdivsi3_libfunc=@var{name} -mfixed-range=@var{register-range} @gol
--madjust-unroll -mindexed-addressing -mgettrcost=@var{number} -mpt-fixed @gol
+-mindexed-addressing -mgettrcost=@var{number} -mpt-fixed @gol
 -maccumulate-outgoing-args -minvalid-symbols -msoft-atomic @gol
 -mbranch-cost=@var{num} -mcbranchdi -mcmpeqdi -mfused-madd -mpretend-cmove}
 
@@ -2836,6 +2840,11 @@ that methods and selectors must be declared before
 Generate C header describing the largest structure that is passed by
 value, if any.
 
+@item -Wbranch-probabilities-computation
+@opindex Wbranch-probabilities-computation
+Warn if edge and/or basic block counts computation is not consistent
+when using the @option{-fbranch-probabilities} option.
+
 @end table
 
 @node Language Independent Options
@@ -3043,6 +3052,7 @@ Options} and @ref{Objective-C and Objective-C++ Di
 -Wmaybe-uninitialized @gol
 -Wmissing-braces  @gol
 -Wnonnull  @gol
+-Wnon-finite-math @gol
 -Wparentheses  @gol
 -Wpointer-sign  @gol
 -Wreorder   @gol
@@ -3286,6 +3296,11 @@ requiring a non-null value by the @code{nonnull} f
 @option{-Wnonnull} is included in @option{-Wall} and @option{-Wformat}.  It
 can be disabled with the @option{-Wno-nonnull} option.
 
+@item -Wnon-finite-math
+@opindex Wnon-finite-math
+@opindex Wno-non-finite-math
+Warn if non-finite builtins are used with -ffinite-math-only.
+
 @item -Winit-self @r{(C, C++, Objective-C and Objective-C++ only)}
 @opindex Winit-self
 @opindex Wno-init-self
@@ -6987,6 +7002,10 @@ This option has no effect until one of @option{-fs
 When pipelining loops during selective scheduling, also pipeline outer loops.
 This option has no effect until @option{-fsel-sched-pipelining} is turned on.
 
+@item -fcse-sincos
+@opindex fcse-sincos
+Disable merging of the @samp{sin}, @samp{cos} operations.
+
 @item -fshrink-wrap
 @opindex fshrink-wrap
 Emit function prologues only before parts of the function that need it,
@@ -9951,6 +9970,10 @@ Marks the argument containing or following the @sa
 designated output file of this compilation.  This puts the argument
 into the sequence of arguments that @samp{%o} will substitute later.
 
+@item %M
+If the target supports multilibs substitute the current multilib directory
+otherwise substitute @samp{.}.
+
 @item %o
 Substitutes the names of all the output files, with spaces
 automatically placed around them.  You should write spaces
@@ -10089,11 +10112,11 @@ The following built-in spec functions are provided
 
 @table @code
 @item @code{getenv}
-The @code{getenv} spec function takes two arguments: an environment
-variable name and a string.  If the environment variable is not
-defined, a fatal error is issued.  Otherwise, the return value is the
-value of the environment variable concatenated with the string.  For
-example, if @env{TOPDIR} is defined as @file{/path/to/top}, then:
+The @code{getenv} spec function takes two or more arguments: an environment
+variable name and a list of strings.  If the environment variable is not
+defined, a fatal error is issued.  Otherwise, the return value is the value
+of the environment variable concatenated with the strings.  For example, if
+@env{TOPDIR} is defined as @file{/path/to/top}, then:
 
 @smallexample
 %:getenv(TOPDIR /include)
@@ -17893,6 +17916,24 @@ single-precision mode by default.
 @opindex m4
 Generate code for the SH4.
 
+@item -m4-300-nofpu
+@opindex m4-300-nofpu
+Generate code for the ST40-300 without a floating-point unit.
+
+@item -m4-300-single-only
+@opindex m4-300-single-only
+Generate code for the ST40-300 with a floating-point unit that only
+supports single-precision arithmetic.
+
+@item -m4-300-single
+@opindex m4-300-single
+Generate code for the ST40-300 assuming the floating-point unit is in
+single-precision mode by default.
+
+@item -m4-300
+@opindex m4-300
+Generate code for the ST40-300.
+
 @item -m4a-nofpu
 @opindex m4a-nofpu
 Generate code for the SH4al-dsp, or for a SH4a in such a way that the
@@ -17932,6 +17973,12 @@ Align doubles at 64-bit boundaries.  Note that thi
 conventions, and thus some functions from the standard C library will
 not work unless you recompile it first with @option{-mdalign}.
 
+@item -mtas
+@itemx -mno-tas
+@opindex mtas
+@opindex mno-tas
+Allow or disallow the @code{tas.b} instruction.
+
 @item -mrelax
 @opindex mrelax
 Shorten some address references at link time, when possible; uses the
@@ -17946,6 +17993,16 @@ Use 32-bit offsets in @code{switch} tables.  The d
 @opindex mbitops
 Enable the use of bit manipulation instructions on SH2A.
 
+@item -mdead-delay
+@opindex mdead-delay
+Try to eliminate dead delay slot instructions.
+
+@item -mfldi
+@itemx -mno-fldfi
+@opindex mfldi
+@opindex mno-fldi
+Enable or disable the use of the instruction @code{fldi0} and @code{fldi1}. When disabled floating point zero/one constants are loaded from the constant pool. Default is enabled.
+
 @item -mfmovd
 @opindex mfmovd
 Enable the use of the instruction @code{fmovd}.  Check @option{-mdalign} for
@@ -18012,10 +18069,6 @@ single-core systems.  They will not perform correc
 This option is enabled by default when the target is @code{sh-*-linux*}.
 For details on the atomic built-in functions see @ref{__atomic Builtins}.
 
-@item -mspace
-@opindex mspace
-Optimize for space instead of speed.  Implied by @option{-Os}.
-
 @item -mprefergot
 @opindex mprefergot
 When generating position-independent code, emit function calls using
@@ -18033,15 +18086,15 @@ Set the cost to assume for a multiply insn.
 
 @item -mdiv=@var{strategy}
 @opindex mdiv=@var{strategy}
-Set the division strategy to use for SHmedia code.  @var{strategy} must be
-one of: call, call2, fp, inv, inv:minlat, inv20u, inv20l, inv:call,
-inv:call2, inv:fp .
-"fp" performs the operation in floating point.  This has a very high latency,
+Set the division strategy to use. For SHmedia code, @var{strategy} must be
+one of: @var{call}, @var{call2}, @var{fp}, @var{inv}, @var{inv:minlat},
+@var{inv20u}, @var{inv20l}, @var{inv:call}, @var{inv:call2}, @var{inv:fp}.
+@samp{fp} performs the operation in floating point.  This has a very high
 but needs only a few instructions, so it might be a good choice if
 your code has enough easily-exploitable ILP to allow the compiler to
 schedule the floating-point instructions together with other instructions.
 Division by zero causes a floating-point exception.
-"inv" uses integer operations to calculate the inverse of the divisor,
+@samp{inv} uses integer operations to calculate the inverse of the divisor,
 and then multiplies the dividend with the inverse.  This strategy allows
 cse and hoisting of the inverse calculation.  Division by zero calculates
 an unspecified result, but does not trap.
@@ -18051,10 +18104,10 @@ place, the last stages of the inverse calculation
 final multiply to reduce the overall latency, at the expense of using a few
 more instructions, and thus offering fewer scheduling opportunities with
 other code.
-"call" calls a library function that usually implements the inv:minlat
+@samp{call} calls a library function that usually implements the inv:minlat
 strategy.
 This gives high code density for m5-*media-nofpu compilations.
-"call2" uses a different entry point of the same library function, where it
+@samp{call2} uses a different entry point of the same library function, where it
 assumes that a pointer to a lookup table has already been set up, which
 exposes the pointer load to cse / code hoisting optimizations.
 "inv:call", "inv:call2" and "inv:fp" all use the "inv" algorithm for initial
@@ -18070,6 +18123,8 @@ up division where the dividend fits into 20 bits (
 by inserting a test to skip a number of operations in this case; this test
 slows down the case of larger dividends.  inv20u assumes the case of a such
 a small dividend to be unlikely, and inv20l assumes it to be likely.
+@samp{call-pre1} optimizes return 1 cases with divisor greater than dividend
+before calling the library function.
 
 @item -maccumulate-outgoing-args
 @opindex maccumulate-outgoing-args
@@ -18092,11 +18147,10 @@ useful when compiling kernel code.  A register ran
 two registers separated by a dash.  Multiple register ranges can be
 specified separated by a comma.
 
-@item -madjust-unroll
-@opindex madjust-unroll
-Throttle unrolling to avoid thrashing target registers.
-This option only has an effect if the gcc code base supports the
-TARGET_ADJUST_UNROLL_MAX target hook.
+@item -malign-small-blocks=@var{number}
+@opindex align-small-blocks=@var{number}
+Set the size among which basic blocks are aligned on cache line boundaries.
+Default is 16 bytes. 0 means default alignment.
 
 @item -mindexed-addressing
 @opindex mindexed-addressing
--- gcc/gcc/doc/md.texi	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/doc/md.texi	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -3211,6 +3211,37 @@ Vector zero
 
 @end table
 
+@item SH---@file{config/sh/constraints.md}
+@table @code
+@item c
+FPSCR. Floating-point status/control register
+
+@item d
+Any 64-bit Floating-point register
+
+@item f
+Any 32-bit Floating-point register
+
+@item l
+PR register
+
+@item t
+T bit from SR. Status register
+
+@item w
+FR0 floating-point register
+
+@item x
+MAC register (MACH and MACL)
+
+@item y
+FPUL register
+
+@item z
+R0 register
+
+@end table
+
 @item SPU---@file{config/spu/spu.h}
 @table @code
 @item a
--- gcc/gcc/dwarf2asm.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/dwarf2asm.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -591,31 +591,33 @@ dw2_asm_output_data_uleb128 (unsigned HOST_WIDE_IN
 
   va_start (ap, comment);
 
-#ifdef HAVE_AS_LEB128
-  fputs ("\t.uleb128 ", asm_out_file);
-  fprint_whex (asm_out_file, value);
+  if (TARGET_USES_LEB128)
+    {
+      fputs ("\t.uleb128 ", asm_out_file);
+      fprint_whex (asm_out_file, value);
 
-  if (flag_debug_asm && comment)
+      if (flag_debug_asm && comment)
+	{
+	  fprintf (asm_out_file, "\t%s ", ASM_COMMENT_START);
+	  vfprintf (asm_out_file, comment, ap);
+	}
+    }
+  else
     {
-      fprintf (asm_out_file, "\t%s ", ASM_COMMENT_START);
-      vfprintf (asm_out_file, comment, ap);
-    }
-#else
-  {
-    unsigned HOST_WIDE_INT work = value;
-    const char *byte_op = targetm.asm_out.byte_op;
+      unsigned HOST_WIDE_INT work = value;
+      const char *byte_op = targetm.asm_out.byte_op;
 
-    if (byte_op)
-      fputs (byte_op, asm_out_file);
-    do
-      {
-	int byte = (work & 0x7f);
-	work >>= 7;
-	if (work != 0)
-	  /* More bytes to follow.  */
-	  byte |= 0x80;
+      if (byte_op)
+	fputs (byte_op, asm_out_file);
+      do
+	{
+	  int byte = (work & 0x7f);
+	  work >>= 7;
+	  if (work != 0)
+	    /* More bytes to follow.  */
+	    byte |= 0x80;
 
-	if (byte_op)
+	  if (byte_op)
 	  {
 	    fprintf (asm_out_file, "%#x", byte);
 	    if (work != 0)
@@ -623,21 +625,21 @@ dw2_asm_output_data_uleb128 (unsigned HOST_WIDE_IN
 	  }
 	else
 	  assemble_integer (GEN_INT (byte), 1, BITS_PER_UNIT, 1);
-      }
-    while (work != 0);
+	}
+      while (work != 0);
 
-  if (flag_debug_asm)
-    {
-      fprintf (asm_out_file, "\t%s uleb128 " HOST_WIDE_INT_PRINT_HEX,
-	       ASM_COMMENT_START, value);
-      if (comment)
+      if (flag_debug_asm)
 	{
-	  fputs ("; ", asm_out_file);
-	  vfprintf (asm_out_file, comment, ap);
+	  fprintf (asm_out_file, "\t%s uleb128 " HOST_WIDE_INT_PRINT_HEX,
+		   ASM_COMMENT_START, value);
+	  if (comment)
+	    {
+	      fputs ("; ", asm_out_file);
+	      vfprintf (asm_out_file, comment, ap);
+	    }
 	}
     }
-  }
-#endif
+
   putc ('\n', asm_out_file);
 
   va_end (ap);
@@ -739,14 +741,15 @@ dw2_asm_output_delta_uleb128 (const char *lab1 ATT
 
   va_start (ap, comment);
 
-#ifdef HAVE_AS_LEB128
-  fputs ("\t.uleb128 ", asm_out_file);
-  assemble_name (asm_out_file, lab1);
-  putc ('-', asm_out_file);
-  assemble_name (asm_out_file, lab2);
-#else
-  gcc_unreachable ();
-#endif
+  if (TARGET_USES_LEB128)
+    {
+      fputs ("\t.uleb128 ", asm_out_file);
+      assemble_name (asm_out_file, lab1);
+      putc ('-', asm_out_file);
+      assemble_name (asm_out_file, lab2);
+    }
+  else
+    gcc_unreachable ();
 
   if (flag_debug_asm && comment)
     {
--- gcc/gcc/tree-ssa-tail-merge.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/tree-ssa-tail-merge.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1437,6 +1437,8 @@ replace_block_by (basic_block bb1, basic_block bb2
     bb2->frequency = BB_FREQ_MAX;
   bb1->frequency = 0;
 
+  bb2->count += bb1->count;
+
   /* Do updates that use bb1, before deleting bb1.  */
   release_last_vdef (bb1);
   same_succ_flush_bb (bb1);
--- gcc/gcc/c-family/c.opt	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/c-family/c.opt	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -998,7 +998,7 @@ C++ ObjC++ Optimization Var(flag_rtti) Init(1)
 Generate run time type descriptor information
 
 fshort-double
-C ObjC C++ ObjC++ Optimization Var(flag_short_double)
+C ObjC C++ ObjC++ Optimization LTO Var(flag_short_double)
 Use the same size for double as for float
 
 fshort-enums
--- gcc/gcc/DATESTAMP	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/DATESTAMP	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1 +1 @@
-20120920
+20130218
--- gcc/gcc/defaults.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/defaults.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -3,6 +3,7 @@
    2005, 2007, 2008, 2009, 2010, 2011
    Free Software Foundation, Inc.
    Contributed by Ron Guilmette (rfg@monkeys.com)
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -311,6 +312,15 @@ see the files COPYING3 and COPYING.RUNTIME respect
 #define TARGET_USES_WEAK_UNWIND_INFO 0
 #endif
 
+/* Use leb128 encoding based on command line options.  */
+#ifndef TARGET_USES_LEB128
+#ifdef HAVE_AS_LEB128
+#define TARGET_USES_LEB128 1
+#else
+#define TARGET_USES_LEB128 0
+#endif
+#endif
+
 /* By default, there is no prefix on user-defined symbols.  */
 #ifndef USER_LABEL_PREFIX
 #define USER_LABEL_PREFIX ""
@@ -882,6 +892,10 @@ see the files COPYING3 and COPYING.RUNTIME respect
 #define ASM_PREFERRED_EH_DATA_FORMAT(CODE,GLOBAL)  DW_EH_PE_absptr
 #endif
 
+#ifndef ASM_ALIGN_FUNCTION_LOG 
+#define ASM_ALIGN_FUNCTION_LOG(DECL) align_functions_log
+#endif
+
 /* By default, the C++ compiler will use the lowest bit of the pointer
    to function to indicate a pointer-to-member-function points to a
    virtual member function.  However, if FUNCTION_BOUNDARY indicates
--- gcc/gcc/tree.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/tree.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1576,7 +1576,10 @@ build_one_cst (tree type)
 {
   switch (TREE_CODE (type))
     {
-    case INTEGER_TYPE: case ENUMERAL_TYPE: case BOOLEAN_TYPE:
+    case BOOLEAN_TYPE:
+      return boolean_true_node;
+    
+    case INTEGER_TYPE: case ENUMERAL_TYPE:
     case POINTER_TYPE: case REFERENCE_TYPE:
     case OFFSET_TYPE:
       return build_int_cst (type, 1);
@@ -1615,7 +1618,10 @@ build_zero_cst (tree type)
 {
   switch (TREE_CODE (type))
     {
-    case INTEGER_TYPE: case ENUMERAL_TYPE: case BOOLEAN_TYPE:
+    case BOOLEAN_TYPE:
+      return boolean_false_node;
+    
+    case INTEGER_TYPE: case ENUMERAL_TYPE:
     case POINTER_TYPE: case REFERENCE_TYPE:
     case OFFSET_TYPE: case NULLPTR_TYPE:
       return build_int_cst (type, 0);
--- gcc/gcc/configure	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/configure	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -11302,7 +11302,7 @@ case ${enable_threads} in
     target_thread_file='single'
     ;;
   aix | dce | lynx | mipssde | posix | rtems | \
-  single | tpf | vxworks | win32)
+  single | tpf | vxworks | win32 | generic)
     target_thread_file=${enable_threads}
     ;;
   *)
@@ -11829,6 +11829,16 @@ then
 	CROSS="-DCROSS_DIRECTORY_STRUCTURE"
 	ALL=all.cross
 	SYSTEM_HEADER_DIR=$build_system_header_dir
+
+ 	# For builds with an in-tree newlib, then the headers are not
+ 	# copied to build_system_header_dir, so things like limits.h
+ 	# won't work unless we point at the real headers.
+ 	if test "$with_newlib" = yes \
+ 		&& (test -z "$with_headers" || test "$with_headers" = yes) \
+ 		&& test -d $srcdir/../newlib/libc/include; then
+ 	  SYSTEM_HEADER_DIR="\$(abs_srcdir)/../newlib/libc/include"
+ 	fi
+
 	case "$host","$target" in
 	# Darwin crosses can use the host system's libraries and headers,
 	# because of the fat library support.  Of course, it must be the
@@ -18010,7 +18020,7 @@ else
   lt_dlunknown=0; lt_dlno_uscore=1; lt_dlneed_uscore=2
   lt_status=$lt_dlunknown
   cat > conftest.$ac_ext <<_LT_EOF
-#line 18013 "configure"
+#line 18023 "configure"
 #include "confdefs.h"
 
 #if HAVE_DLFCN_H
@@ -18116,7 +18126,7 @@ else
   lt_dlunknown=0; lt_dlno_uscore=1; lt_dlneed_uscore=2
   lt_status=$lt_dlunknown
   cat > conftest.$ac_ext <<_LT_EOF
-#line 18119 "configure"
+#line 18129 "configure"
 #include "confdefs.h"
 
 #if HAVE_DLFCN_H
@@ -26729,6 +26739,8 @@ if test x$host != x$target || test "x$TARGET_SYSTE
   else
     target_header_dir="${with_sysroot}${native_system_header_dir}"
   fi
+elif test x$host != x$build && test "x$with_build_sysroot" != "x"; then
+  target_header_dir="${with_build_sysroot}${native_system_header_dir}"
 else
   target_header_dir=${native_system_header_dir}
 fi
--- gcc/gcc/final.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/final.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -3,6 +3,7 @@
    1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,
    2010, 2011
    Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -374,6 +375,22 @@ init_insn_lengths (void)
     }
 }
 
+#ifdef ADJUST_INSN_LENGTH
+static void realloc_insn_lengths (int uid, char **varying_length)
+{
+  int max_uid = get_max_uid ();
+
+  gcc_assert (insn_lengths);
+
+  insn_lengths = XRESIZEVEC (int, insn_lengths, max_uid);
+  insn_lengths[uid] = 0;
+  insn_lengths_max_uid = max_uid;
+  insn_lengths[uid] = 0;
+  *varying_length = XRESIZEVEC (char, *varying_length, max_uid);
+  (*varying_length)[uid] = 0;
+}
+#endif
+
 /* Obtain the current length of an insn.  If branch shortening has been done,
    get its actual length.  Otherwise, use FALLBACK_FN to calculate the
    length.  */
@@ -657,13 +674,12 @@ int
 insn_current_reference_address (rtx branch)
 {
   rtx dest, seq;
-  int seq_uid;
 
   if (! INSN_ADDRESSES_SET_P ())
     return 0;
 
   seq = NEXT_INSN (PREV_INSN (branch));
-  seq_uid = INSN_UID (seq);
+
   if (!JUMP_P (branch))
     /* This can happen for example on the PA; the objective is to know the
        offset to address something in front of the start of the function.
@@ -678,7 +694,7 @@ insn_current_reference_address (rtx branch)
   if (INSN_SHUID (seq) < INSN_SHUID (dest))
     {
       /* Forward branch.  */
-      return (insn_last_address + insn_lengths[seq_uid]
+      return (insn_last_address + insn_min_length (branch)
 	      - align_fuzz (seq, dest, length_unit_log, ~0));
     }
   else
@@ -976,6 +992,11 @@ shorten_branches (rtx first ATTRIBUTE_UNUSED)
     }
 #ifdef HAVE_ATTR_length
 
+  gcc_assert (insn_lengths == 0);
+
+  /* New insn might have been created by insn_length_adjustment.  */
+  max_uid = get_max_uid ();
+
   /* Allocate the rest of the arrays.  */
   insn_lengths = XNEWVEC (int, max_uid);
   insn_lengths_max_uid = max_uid;
@@ -1107,7 +1128,10 @@ shorten_branches (rtx first ATTRIBUTE_UNUSED)
 	  /* Alignment is handled by ADDR_VEC_ALIGN.  */
 	}
       else if (GET_CODE (body) == ASM_INPUT || asm_noperands (body) >= 0)
+	{
 	insn_lengths[uid] = asm_insn_count (body) * insn_default_length (insn);
+	  varying_length[uid] = 1;
+	}
       else if (GET_CODE (body) == SEQUENCE)
 	{
 	  int i;
@@ -1150,12 +1174,23 @@ shorten_branches (rtx first ATTRIBUTE_UNUSED)
       else if (GET_CODE (body) != USE && GET_CODE (body) != CLOBBER)
 	{
 	  insn_lengths[uid] = insn_default_length (insn);
+
+#ifdef VARYING_INSN_P
+	  if (VARYING_INSN_P (insn))
+	    varying_length[uid] = 1;
+	  else
+#endif
 	  varying_length[uid] = insn_variable_length_p (insn);
 	}
 
       /* If needed, do any adjustment.  */
 #ifdef ADJUST_INSN_LENGTH
       ADJUST_INSN_LENGTH (insn, insn_lengths[uid]);
+      if (max_uid != get_max_uid ())
+	{
+	  realloc_insn_lengths (max_uid, &varying_length);
+	  max_uid = get_max_uid ();
+	}
       if (insn_lengths[uid] < 0)
 	fatal_insn ("negative insn length", insn);
 #endif
@@ -1184,7 +1219,7 @@ shorten_branches (rtx first ATTRIBUTE_UNUSED)
 	  if (LABEL_P (insn))
 	    {
 	      int log = LABEL_TO_ALIGNMENT (insn);
-	      if (log > insn_current_align)
+	      if (log >= insn_current_align)
 		{
 		  int align = 1 << log;
 		  int new_address= (insn_current_address + align - 1) & -align;
@@ -1335,10 +1370,10 @@ shorten_branches (rtx first ATTRIBUTE_UNUSED)
 		}
 	      else
 		insn_current_address += insn_lengths[uid];
-
 	      continue;
 	    }
 
+	  /* Varying_length.  */
 	  if (NONJUMP_INSN_P (insn) && GET_CODE (PATTERN (insn)) == SEQUENCE)
 	    {
 	      int i;
@@ -1360,6 +1395,16 @@ shorten_branches (rtx first ATTRIBUTE_UNUSED)
 		  else
 		    inner_length = insn_current_length (inner_insn);
 
+#ifdef ADJUST_INSN_LENGTH
+		  /* If needed, do any adjustment.  */
+		  ADJUST_INSN_LENGTH (inner_insn, inner_length);
+		  if (max_uid != get_max_uid ())
+		    {
+		      realloc_insn_lengths (max_uid, &varying_length);
+		      max_uid = get_max_uid ();
+		    }
+#endif
+
 		  if (inner_length != insn_lengths[inner_uid])
 		    {
 		      insn_lengths[inner_uid] = inner_length;
@@ -1371,7 +1416,20 @@ shorten_branches (rtx first ATTRIBUTE_UNUSED)
 	    }
 	  else
 	    {
+	      rtx body = PATTERN (insn);
+
+	      if (GET_CODE (body) == ASM_INPUT || asm_noperands (body) >= 0)
+		new_length = asm_insn_count (body) * insn_default_length (insn);
+	      else  
+		{
+#ifdef VARYING_INSN_P 
+		  if (VARYING_INSN_P (insn))
+		    new_length = insn_lengths[uid];
+		  else
+#endif
 	      new_length = insn_current_length (insn);
+		}
+
 	      insn_current_address += new_length;
 	    }
 
@@ -1380,6 +1438,11 @@ shorten_branches (rtx first ATTRIBUTE_UNUSED)
 	  tmp_length = new_length;
 	  ADJUST_INSN_LENGTH (insn, new_length);
 	  insn_current_address += (new_length - tmp_length);
+	  if (max_uid != get_max_uid ())
+	    {
+	      realloc_insn_lengths (max_uid, &varying_length);
+	      max_uid = get_max_uid ();
+	    }
 #endif
 
 	  if (new_length != insn_lengths[uid])
@@ -1388,9 +1451,6 @@ shorten_branches (rtx first ATTRIBUTE_UNUSED)
 	      something_changed = 1;
 	    }
 	}
-      /* For a non-optimizing compile, do only a single pass.  */
-      if (!optimize)
-	break;
     }
 
   free (varying_length);
@@ -1427,10 +1487,14 @@ asm_str_count (const char *templ)
   if (!*templ)
     return 0;
 
+#ifdef TARGET_ASM_COUNT
+  count = TARGET_ASM_COUNT (templ, 0);
+#else
   for (; *templ; templ++)
     if (IS_ASM_LOGICAL_LINE_SEPARATOR (*templ, templ)
 	|| *templ == '\n')
       count++;
+#endif
 
   return count;
 }
@@ -1488,6 +1552,8 @@ remap_debug_filename (const char *filename)
   const char *name;
   size_t name_len;
 
+  CYGPATH (filename);
+
   for (map = debug_prefix_maps; map; map = map->next)
     if (filename_ncmp (filename, map->old_prefix, map->old_len) == 0)
       break;
@@ -2114,6 +2180,10 @@ final_scan_insn (rtx insn, FILE *file, int optimiz
 
 	  if (align && NEXT_INSN (insn))
 	    {
+#ifdef FINAL_PRESCAN_INSN
+	FINAL_PRESCAN_INSN (insn, recog_data.operand, recog_data.n_operands);
+#endif
+
 #ifdef ASM_OUTPUT_MAX_SKIP_ALIGN
 	      ASM_OUTPUT_MAX_SKIP_ALIGN (file, align, max_skip);
 #else
@@ -4558,3 +4628,12 @@ struct rtl_opt_pass pass_clean_state =
   0                                     /* todo_flags_finish */
  }
 };
+
+int
+print_address (int uid)
+{
+  if (! INSN_ADDRESSES_SET_P ())
+    return 0;
+
+  return INSN_ADDRESSES (uid);
+}
--- gcc/gcc/builtins.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/builtins.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -10025,7 +10025,12 @@ fold_builtin_classify (location_t loc, tree fndecl
     {
     case BUILT_IN_ISINF:
       if (!HONOR_INFINITIES (TYPE_MODE (TREE_TYPE (arg))))
+	{
+	  if (warn_non_finite_math)
+	    warning (OPT_Wnon_finite_math,
+		     "non-finite operation %q+F always returns 0", fndecl);
 	return omit_one_operand_loc (loc, type, integer_zero_node, arg);
+	}
 
       if (TREE_CODE (arg) == REAL_CST)
 	{
@@ -10074,7 +10079,11 @@ fold_builtin_classify (location_t loc, tree fndecl
     case BUILT_IN_ISFINITE:
       if (!HONOR_NANS (TYPE_MODE (TREE_TYPE (arg)))
 	  && !HONOR_INFINITIES (TYPE_MODE (TREE_TYPE (arg))))
+	{
+	  warning (OPT_Wnon_finite_math, "non-finite operation %q+D not honored" ,
+		   fndecl);
 	return omit_one_operand_loc (loc, type, integer_one_node, arg);
+	}
 
       if (TREE_CODE (arg) == REAL_CST)
 	{
@@ -10086,7 +10095,11 @@ fold_builtin_classify (location_t loc, tree fndecl
 
     case BUILT_IN_ISNAN:
       if (!HONOR_NANS (TYPE_MODE (TREE_TYPE (arg))))
+	{
+	  warning (OPT_Wnon_finite_math, "non-finite operation %q+D not honored",
+		   fndecl);
 	return omit_one_operand_loc (loc, type, integer_zero_node, arg);
+	}
 
       if (TREE_CODE (arg) == REAL_CST)
 	{
@@ -10210,7 +10223,12 @@ fold_builtin_unordered_cmp (location_t loc, tree f
   if (unordered_code == UNORDERED_EXPR)
     {
       if (!HONOR_NANS (TYPE_MODE (TREE_TYPE (arg0))))
+	{
+	  if (warn_non_finite_math)
+	    warning (OPT_Wnon_finite_math,
+		     "non-finite operation %q+F always returns 0", fndecl);
 	return omit_two_operands_loc (loc, type, integer_zero_node, arg0, arg1);
+	}
       return fold_build2_loc (loc, UNORDERED_EXPR, type, arg0, arg1);
     }
 
--- gcc/gcc/gcc.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/gcc.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -3,6 +3,7 @@
    1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,
    2010, 2011, 2012
    Free Software Foundation, Inc.
+   Copyright (c) 2009, 2012  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -190,8 +191,8 @@ static void init_spec (void);
 static void store_arg (const char *, int, int);
 static void insert_wrapper (const char *);
 static char *load_specs (const char *);
-static void read_specs (const char *, int);
-static void set_spec (const char *, const char *);
+static void read_specs (const char *, bool, bool);
+static void set_spec (const char *, const char *, bool);
 static struct compiler *lookup_compiler (const char *, size_t, const char *);
 static char *build_search_list (const struct path_prefix *, const char *,
 				bool, bool);
@@ -227,9 +228,9 @@ static void do_option_spec (const char *, const ch
 static void do_self_spec (const char *);
 static const char *find_file (const char *);
 static int is_directory (const char *, bool);
-static const char *validate_switches (const char *);
+static const char *validate_switches (const char *, bool);
 static void validate_all_switches (void);
-static inline void validate_switches_from_spec (const char *);
+static inline void validate_switches_from_spec (const char *, bool);
 static void give_switch (int, int);
 static int used_arg (const char *, int);
 static int default_arg (const char *, int);
@@ -340,6 +341,7 @@ or with constant text in a single argument.
  %W{...}
 	like %{...} but mark last argument supplied within
 	as a file to be deleted on failure.
+ %M	substitue the current multilib directory.
  %o	substitutes the names of all the output files, with spaces
 	automatically placed around them.  You should write spaces
 	around the %o as well or the results are undefined.
@@ -677,7 +679,7 @@ proper position among the other output files.  */
     %{fopenmp|ftree-parallelize-loops=*:%:include(libgomp.spec)%(link_gomp)}\
     %{fgnu-tm:%:include(libitm.spec)%(link_itm)}\
     %(mflib) " STACK_SPLIT_SPEC "\
-    %{fprofile-arcs|fprofile-generate*|coverage:-lgcov}\
+    %{!nostdlib:%{fprofile-arcs|fprofile-generate*|coverage:-lgcov}}\
     %{!nostdlib:%{!nodefaultlibs:%(link_ssp) %(link_gcc_c_sequence)}}\
     %{!nostdlib:%{!nostartfiles:%E}} %{T*} }}}}}}"
 #endif
@@ -1170,11 +1172,12 @@ struct spec_list
   const char **ptr_spec;	/* pointer to the spec itself.  */
   struct spec_list *next;	/* Next spec in linked list.  */
   int name_len;			/* length of the name */
-  int alloc_p;			/* whether string was allocated */
+  bool user_p;			/* whether string come from file spec.  */
+  bool alloc_p;			/* whether string was allocated */
 };
 
 #define INIT_STATIC_SPEC(NAME,PTR) \
-{ NAME, NULL, PTR, (struct spec_list *) 0, sizeof (NAME) - 1, 0 }
+  { NAME, NULL, PTR, (struct spec_list *) 0, sizeof (NAME) - 1, false, false }
 
 /* List of statically defined specs.  */
 static struct spec_list static_specs[] =
@@ -1478,7 +1481,7 @@ init_spec (void)
    current spec.  */
 
 static void
-set_spec (const char *name, const char *spec)
+set_spec (const char *name, const char *spec, bool user_p)
 {
   struct spec_list *sl;
   const char *old_spec;
@@ -1530,7 +1533,8 @@ static void
   if (old_spec && sl->alloc_p)
     free (CONST_CAST(char *, old_spec));
 
-  sl->alloc_p = 1;
+  sl->user_p = user_p;
+  sl->alloc_p = true;
 }
 
 /* Accumulate a command (program name and args), and run it.  */
@@ -1686,7 +1690,7 @@ load_specs (const char *filename)
    Anything invalid in the file is a fatal error.  */
 
 static void
-read_specs (const char *filename, int main_p)
+read_specs (const char *filename, bool main_p, bool user_p)
 {
   char *buffer;
   char *p;
@@ -1735,7 +1739,7 @@ static void
 
 	      p[-2] = '\0';
 	      new_filename = find_a_file (&startfile_prefixes, p1, R_OK, true);
-	      read_specs (new_filename ? new_filename : p1, FALSE);
+	      read_specs (new_filename ? new_filename : p1, false, user_p);
 	      continue;
 	    }
 	  else if (!strncmp (p1, "%include_noerr", sizeof "%include_noerr" - 1)
@@ -1756,7 +1760,7 @@ static void
 	      p[-2] = '\0';
 	      new_filename = find_a_file (&startfile_prefixes, p1, R_OK, true);
 	      if (new_filename)
-		read_specs (new_filename, FALSE);
+		read_specs (new_filename, false, user_p);
 	      else if (verbose_flag)
 		fnotice (stderr, "could not find specs file %s\n", p1);
 	      continue;
@@ -1833,7 +1837,7 @@ static void
 #endif
 		}
 
-	      set_spec (p2, *(sl->ptr_spec));
+	      set_spec (p2, *(sl->ptr_spec), user_p);
 	      if (sl->alloc_p)
 		free (CONST_CAST (char *, *(sl->ptr_spec)));
 
@@ -1899,7 +1903,7 @@ static void
 	  if (! strcmp (suffix, "*link_command"))
 	    link_command_spec = spec;
 	  else
-	    set_spec (suffix + 1, spec);
+	    set_spec (suffix + 1, spec, user_p);
 	}
       else
 	{
@@ -2100,15 +2104,17 @@ for_each_path (const struct path_prefix *paths,
     {
       size_t multi_dir_len = 0;
       size_t multi_os_dir_len = 0;
-      size_t suffix_len;
-      size_t just_suffix_len;
+      size_t suffix_len = 0;
+      size_t just_suffix_len = 0;
       size_t len;
 
       if (multi_dir)
 	multi_dir_len = strlen (multi_dir);
       if (multi_os_dir)
 	multi_os_dir_len = strlen (multi_os_dir);
+      if (multi_suffix)
       suffix_len = strlen (multi_suffix);
+      if (just_multi_suffix)
       just_suffix_len = strlen (just_multi_suffix);
 
       if (path == NULL)
@@ -2127,7 +2133,7 @@ for_each_path (const struct path_prefix *paths,
 	  memcpy (path, pl->prefix, len);
 
 	  /* Look first in MACHINE/VERSION subdirectory.  */
-	  if (!skip_multi_dir)
+	  if (!skip_multi_dir && multi_suffix)
 	    {
 	      memcpy (path + len, multi_suffix, suffix_len + 1);
 	      ret = callback (path, callback_info);
@@ -2137,7 +2143,7 @@ for_each_path (const struct path_prefix *paths,
 
 	  /* Some paths are tried with just the machine (ie. target)
 	     subdir.  This is used for finding as, ld, etc.  */
-	  if (!skip_multi_dir
+	  if (!skip_multi_dir && just_multi_suffix
 	      && pl->require_machine_suffix == 2)
 	    {
 	      memcpy (path + len, just_multi_suffix, just_suffix_len + 1);
@@ -2819,8 +2825,9 @@ struct switchstr
   const char *part1;
   const char **args;
   unsigned int live_cond;
-  unsigned char validated;
-  unsigned char ordering;
+  bool known;
+  bool validated;
+  bool ordering;
 };
 
 static struct switchstr *switches;
@@ -3086,11 +3093,11 @@ alloc_switch (void)
 }
 
 /* Save an option OPT with N_ARGS arguments in array ARGS, marking it
-   as validated if VALIDATED.  */
+   as validated if VALIDATED and KNOWN if it is an internal switch.  */
 
 static void
 save_switch (const char *opt, size_t n_args, const char *const *args,
-	     bool validated)
+	     bool validated, bool known)
 {
   alloc_switch ();
   switches[n_switches].part1 = opt + 1;
@@ -3105,6 +3112,7 @@ save_switch (const char *opt, size_t n_args, const
 
   switches[n_switches].live_cond = 0;
   switches[n_switches].validated = validated;
+  switches[n_switches].known = known;
   switches[n_switches].ordering = 0;
   n_switches++;
 }
@@ -3123,9 +3131,17 @@ driver_unknown_option_callback (const struct cl_de
 	 diagnosed only if there are warnings.  */
       save_switch (decoded->canonical_option[0],
 		   decoded->canonical_option_num_elements - 1,
-		   &decoded->canonical_option[1], false);
+		   &decoded->canonical_option[1], false, true);
       return false;
     }
+  if (decoded->opt_index == OPT_SPECIAL_unknown)
+    {
+      /* Give it a chance to define it a a spec file.  */
+      save_switch (decoded->canonical_option[0],
+		   decoded->canonical_option_num_elements - 1,
+		   &decoded->canonical_option[1], false, false);
+      return false;
+    }
   else
     return true;
 }
@@ -3150,7 +3166,7 @@ driver_wrong_lang_callback (const struct cl_decode
   else
     save_switch (decoded->canonical_option[0],
 		 decoded->canonical_option_num_elements - 1,
-		 &decoded->canonical_option[1], false);
+		 &decoded->canonical_option[1], false, true);
 }
 
 static const char *spec_lang = 0;
@@ -3293,7 +3309,7 @@ driver_handle_option (struct gcc_options *opts,
 	compare_debug_opt = NULL;
       else
 	compare_debug_opt = arg;
-      save_switch (compare_debug_replacement_opt, 0, NULL, validated);
+      save_switch (compare_debug_replacement_opt, 0, NULL, validated, true);
       return true;
 
     case OPT_Wa_:
@@ -3378,12 +3394,12 @@ driver_handle_option (struct gcc_options *opts,
     case OPT_L:
       /* Similarly, canonicalize -L for linkers that may not accept
 	 separate arguments.  */
-      save_switch (concat ("-L", arg, NULL), 0, NULL, validated);
+      save_switch (concat ("-L", arg, NULL), 0, NULL, validated, true);
       return true;
 
     case OPT_F:
       /* Likewise -F.  */
-      save_switch (concat ("-F", arg, NULL), 0, NULL, validated);
+      save_switch (concat ("-F", arg, NULL), 0, NULL, validated, true);
       return true;
 
     case OPT_save_temps:
@@ -3426,7 +3442,7 @@ driver_handle_option (struct gcc_options *opts,
 	  user_specs_head = user;
 	user_specs_tail = user;
       }
-      do_save = false;
+      validated = true;
       break;
 
     case OPT__sysroot_:
@@ -3455,8 +3471,11 @@ driver_handle_option (struct gcc_options *opts,
 
     case OPT_B:
       {
-	size_t len = strlen (arg);
+	size_t len;
 
+	CYGPATH (arg);
+	len = strlen (arg);
+
 	/* Catch the case where the user has forgotten to append a
 	   directory separator to the path.  Note, they may be using
 	   -B to add an executable name prefix, eg "i386-elf-", in
@@ -3505,7 +3524,7 @@ driver_handle_option (struct gcc_options *opts,
       save_temps_prefix = xstrdup (arg);
       /* On some systems, ld cannot handle "-o" without a space.  So
 	 split the option from its argument.  */
-      save_switch ("-o", 1, &arg, validated);
+      save_switch ("-o", 1, &arg, validated, true);
       return true;
 
     case OPT_static_libgcc:
@@ -3528,7 +3547,7 @@ driver_handle_option (struct gcc_options *opts,
   if (do_save)
     save_switch (decoded->canonical_option[0],
 		 decoded->canonical_option_num_elements - 1,
-		 &decoded->canonical_option[1], validated);
+		 &decoded->canonical_option[1], validated, true);
   return true;
 }
 
@@ -3582,21 +3601,8 @@ process_command (unsigned int decoded_options_coun
 	}
     }
 
-  /* Handle any -no-canonical-prefixes flag early, to assign the function
-     that builds relative prefixes.  This function creates default search
-     paths that are needed later in normal option handling.  */
+  get_relative_prefix = make_relative_prefix_ignore_links;
 
-  for (j = 1; j < decoded_options_count; j++)
-    {
-      if (decoded_options[j].opt_index == OPT_no_canonical_prefixes)
-	{
-	  get_relative_prefix = make_relative_prefix_ignore_links;
-	  break;
-	}
-    }
-  if (! get_relative_prefix)
-    get_relative_prefix = make_relative_prefix;
-
   /* Set up the default search paths.  If there is no GCC_EXEC_PREFIX,
      see if we can create it from the pathname specified in
      decoded_options[0].arg.  */
@@ -3821,7 +3827,7 @@ process_command (unsigned int decoded_options_coun
 	    }
 	  else
 	    fname = xstrdup (arg);
- 
+
           if (strcmp (fname, "-") != 0 && access (fname, F_OK) < 0)
 	    perror_with_name (fname);
           else
@@ -3955,7 +3961,8 @@ process_command (unsigned int decoded_options_coun
 					   NULL);
       switches[n_switches].args = 0;
       switches[n_switches].live_cond = 0;
-      switches[n_switches].validated = 0;
+      switches[n_switches].validated = false;
+      switches[n_switches].known = false;
       switches[n_switches].ordering = 0;
       n_switches++;
       compare_debug = 1;
@@ -4330,7 +4337,7 @@ do_self_spec (const char *spec)
 	      save_switch (decoded_options[j].canonical_option[0],
 			   (decoded_options[j].canonical_option_num_elements
 			    - 1),
-			   &decoded_options[j].canonical_option[1], false);
+			   &decoded_options[j].canonical_option[1], false, true);
 	      break;
 
 	    default:
@@ -4936,6 +4943,14 @@ do_spec_1 (const char *spec, int inswitch, const c
 	    }
 	    break;
 
+	  case 'M':
+	    {
+	      const char *mlib = multilib_dir ? multilib_dir : ".";
+	      obstack_grow (&obstack, mlib, strlen (mlib));
+	      arg_going = 1;
+	    }
+	    break;
+
 	  case 'o':
 	    {
 	      int max = n_infiles;
@@ -5195,7 +5210,11 @@ do_spec_1 (const char *spec, int inswitch, const c
 		    && (have_wildcard || switches[i].part1[len] == '\0'))
 		  {
 		    switches[i].live_cond |= switch_option;
-		    switches[i].validated = 1;
+		    /* User switch be validated from validate_all_switches.
+		       when the definition is seen from the spec file.
+		       If not defined anywhere, will be rejected.  */
+		    if (switches[i].known)
+		      switches[i].validated = true;
 		  }
 
 	      p += len;
@@ -5791,7 +5810,7 @@ check_live_switch (int switchnum, int prefix_lengt
       for (i = switchnum + 1; i < n_switches; i++)
 	if (switches[i].part1[0] == 'O')
 	  {
-	    switches[switchnum].validated = 1;
+	    switches[switchnum].validated = true;
 	    switches[switchnum].live_cond = SWITCH_FALSE;
 	    return 0;
 	  }
@@ -5805,7 +5824,9 @@ check_live_switch (int switchnum, int prefix_lengt
 	    if (switches[i].part1[0] == name[0]
 		&& ! strcmp (&switches[i].part1[1], &name[4]))
 	      {
-		switches[switchnum].validated = 1;
+		/* --specs are validated with the validate_switches mechanism.  */
+		if (switches[switchnum].known)
+		  switches[switchnum].validated = true;
 		switches[switchnum].live_cond = SWITCH_FALSE;
 		return 0;
 	      }
@@ -5820,7 +5841,9 @@ check_live_switch (int switchnum, int prefix_lengt
 		&& switches[i].part1[3] == '-'
 		&& !strcmp (&switches[i].part1[4], &name[1]))
 	      {
-		switches[switchnum].validated = 1;
+		/* --specs are validated with the validate_switches mechanism.  */
+		if (switches[switchnum].known)
+		  switches[switchnum].validated = true;
 		switches[switchnum].live_cond = SWITCH_FALSE;
 		return 0;
 	      }
@@ -5884,7 +5907,7 @@ give_switch (int switchnum, int omit_first_word)
     }
 
   do_spec_1 (" ", 0, NULL);
-  switches[switchnum].validated = 1;
+  switches[switchnum].validated = true;
 }
 
 /* Search for a file named NAME trying various prefixes including the
@@ -6256,7 +6279,7 @@ main (int argc, char **argv)
   specs_file = find_a_file (&startfile_prefixes, "specs", R_OK, true);
   /* Read the specs file unless it is a default one.  */
   if (specs_file != 0 && strcmp (specs_file, "specs"))
-    read_specs (specs_file, TRUE);
+    read_specs (specs_file, true, false);
   else
     init_spec ();
 
@@ -6269,7 +6292,7 @@ main (int argc, char **argv)
   strcat (specs_file, just_machine_suffix);
   strcat (specs_file, "specs");
   if (access (specs_file, R_OK) == 0)
-    read_specs (specs_file, TRUE);
+    read_specs (specs_file, true, false);
 
   /* Process any configure-time defaults specified for the command line
      options, via OPTION_DEFAULT_SPECS.  */
@@ -6313,7 +6336,7 @@ main (int argc, char **argv)
     {
       obstack_grow (&obstack, "%(sysroot_spec) ", strlen ("%(sysroot_spec) "));
       obstack_grow0 (&obstack, link_spec, strlen (link_spec));
-      set_spec ("link", XOBFINISH (&obstack, const char *));
+      set_spec ("link", XOBFINISH (&obstack, const char *), false);
     }
 #endif
 
@@ -6389,7 +6412,7 @@ main (int argc, char **argv)
     {
       char *filename = find_a_file (&startfile_prefixes, uptr->filename,
 				    R_OK, true);
-      read_specs (filename ? filename : uptr->filename, FALSE);
+      read_specs (filename ? filename : uptr->filename, false, true);
     }
 
   /* Process any user self specs.  */
@@ -6484,11 +6507,11 @@ main (int argc, char **argv)
       xputenv (XOBFINISH (&collect_obstack, char *));
     }
 
-  /* Warn about any switches that no pass was interested in.  */
+  /* Reject switches that no pass was interested in.  */
 
   for (i = 0; (int) i < n_switches; i++)
     if (! switches[i].validated)
-      error ("unrecognized option %<-%s%>", switches[i].part1);
+      error ("unrecognized command line option %<-%s%>", switches[i].part1);
 
   /* Obey some of the options.  */
 
@@ -7028,14 +7051,14 @@ perror_with_name (const char *name)
 }
 
 static inline void
-validate_switches_from_spec (const char *spec)
+validate_switches_from_spec (const char *spec, bool user)
 {
   const char *p = spec;
   char c;
   while ((c = *p++))
     if (c == '%' && (*p == '{' || *p == '<' || (*p == 'W' && *++p == '{')))
       /* We have a switch spec.  */
-      p = validate_switches (p + 1);
+      p = validate_switches (p + 1, user);
 }
 
 static void
@@ -7045,20 +7068,20 @@ validate_all_switches (void)
   struct spec_list *spec;
 
   for (comp = compilers; comp->spec; comp++)
-    validate_switches_from_spec (comp->spec);
+    validate_switches_from_spec (comp->spec, false);
 
   /* Look through the linked list of specs read from the specs file.  */
   for (spec = specs; spec; spec = spec->next)
-    validate_switches_from_spec (*spec->ptr_spec);
+    validate_switches_from_spec (*spec->ptr_spec, spec->user_p);
 
-  validate_switches_from_spec (link_command_spec);
+  validate_switches_from_spec (link_command_spec, false);
 }
 
 /* Look at the switch-name that comes after START
    and mark as valid all supplied switches that match it.  */
 
 static const char *
-validate_switches (const char *start)
+validate_switches (const char *start, bool user_spec)
 {
   const char *p = start;
   const char *atom;
@@ -7095,8 +7118,9 @@ next_member:
       /* Mark all matching switches as valid.  */
       for (i = 0; i < n_switches; i++)
 	if (!strncmp (switches[i].part1, atom, len)
-	    && (starred || switches[i].part1[len] == 0))
-	  switches[i].validated = 1;
+	    && (starred || switches[i].part1[len] == '\0')
+	    && (switches[i].known || user_spec))
+	      switches[i].validated = true;
     }
 
   if (*p) p++;
@@ -7111,9 +7135,9 @@ next_member:
 	    {
 	      p++;
 	      if (*p == '{' || *p == '<')
-		p = validate_switches (p+1);
+		p = validate_switches (p+1, user_spec);
 	      else if (p[0] == 'W' && p[1] == '{')
-		p = validate_switches (p+2);
+		p = validate_switches (p+2, user_spec);
 	    }
 	  else
 	    p++;
@@ -7798,7 +7822,7 @@ print_multilib_info (void)
 /* getenv built-in spec function.
 
    Returns the value of the environment variable given by its first
-   argument, concatenated with the second argument.  If the
+   argument, concatenated with the remaining arguments.  If the
    environment variable is not defined, a fatal error is issued.  */
 
 static const char *
@@ -7808,8 +7832,9 @@ getenv_spec_function (int argc, const char **argv)
   char *result;
   char *ptr;
   size_t len;
+  int i;
 
-  if (argc != 2)
+  if (argc < 2)
     return NULL;
 
   value = getenv (argv[0]);
@@ -7820,7 +7845,9 @@ getenv_spec_function (int argc, const char **argv)
      they are not interpreted as active spec characters.  A
      particularly painful case is when we are reading a variable
      holding a windows path complete with \ separators.  */
-  len = strlen (value) * 2 + strlen (argv[1]) + 1;
+  len = strlen (value) * 2 + 1;
+  for (i = 1; i < argc; i++)
+    len += strlen (argv[i]);
   result = XNEWVAR (char, len);
   for (ptr = result; *value; ptr += 2)
     {
@@ -7828,7 +7855,11 @@ getenv_spec_function (int argc, const char **argv)
       ptr[1] = *value++;
     }
 
-  strcpy (ptr, argv[1]);
+  for (i = 1; i < argc; i++)
+    {
+      strcpy (ptr, argv[i]);
+      ptr += strlen (argv[i]);
+    }
 
   return result;
 }
@@ -8043,7 +8074,7 @@ include_spec_function (int argc, const char **argv
     abort ();
 
   file = find_a_file (&startfile_prefixes, argv[0], R_OK, true);
-  read_specs (file ? file : argv[0], FALSE);
+  read_specs (file ? file : argv[0], false, false);
 
   return NULL;
 }
--- gcc/gcc/fold-const.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/fold-const.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -7069,7 +7069,7 @@ fold_plusminus_mult_expr (location_t loc, enum tre
 			  tree arg0, tree arg1)
 {
   tree arg00, arg01, arg10, arg11;
-  tree alt0 = NULL_TREE, alt1 = NULL_TREE, same;
+  tree alt0 = NULL_TREE, alt1 = NULL_TREE, same = NULL_TREE;
 
   /* (A * C) +- (B * C) -> (A+-B) * C.
      (A * C) +- A -> A * (C+-1).
@@ -7123,7 +7123,6 @@ fold_plusminus_mult_expr (location_t loc, enum tre
       arg10 = arg1;
       arg11 = build_one_cst (type);
     }
-  same = NULL_TREE;
 
   if (operand_equal_p (arg01, arg11, 0))
     same = arg01, alt0 = arg00, alt1 = arg10;
@@ -7134,10 +7133,18 @@ fold_plusminus_mult_expr (location_t loc, enum tre
   else if (operand_equal_p (arg01, arg10, 0))
     same = arg01, alt0 = arg00, alt1 = arg11;
 
+  if (same)
+    {
+      /* Catch base+index gimple trees.  */
+      if (host_integerp (same, 1) && exact_log2 (TREE_INT_CST_LOW (same)) > 0)
+	return NULL_TREE;
+    }
+  else 
+
   /* No identical multiplicands; see if we can find a common
      power-of-two factor in non-power-of-two multiplies.  This
      can help in multi-dimensional array access.  */
-  else if (host_integerp (arg01, 0)
+  if (host_integerp (arg01, 0)
 	   && host_integerp (arg11, 0))
     {
       HOST_WIDE_INT int01, int11, tmp;
@@ -7174,11 +7181,16 @@ fold_plusminus_mult_expr (location_t loc, enum tre
     }
 
   if (same)
+    {
+      if (! (host_integerp (alt1, 0) &&
+	     host_integerp (same, 1) &&
+	     exact_log2 (TREE_INT_CST_LOW (same)) > 0))
     return fold_build2_loc (loc, MULT_EXPR, type,
 			fold_build2_loc (loc, code, type,
 				     fold_convert_loc (loc, type, alt0),
 				     fold_convert_loc (loc, type, alt1)),
 			fold_convert_loc (loc, type, same));
+    }
 
   return NULL_TREE;
 }
--- gcc/gcc/cfg.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/cfg.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -715,6 +715,9 @@ dump_edge_info (FILE *file, edge e, int do_succ)
       fprintf (file, HOST_WIDEST_INT_PRINT_DEC, e->count);
     }
 
+  if (e->goto_locus)
+    fprintf (file, "locus = %d ", e->goto_locus);
+
   if (e->flags)
     {
       static const char * const bitnames[] = {
--- gcc/gcc/toplev.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/toplev.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -619,6 +619,7 @@ compile_file (void)
       process_pending_assemble_externals ();
    }
 
+#ifndef HAVE_LTO_PLUGIN 
   /* Emit LTO marker if LTO info has been previously emitted.  This is
      used by collect2 to determine whether an object file contains IL.
      We used to emit an undefined reference here, but this produces
@@ -656,6 +657,7 @@ compile_file (void)
 #endif
         }
     }
+#endif
 
   /* Attach a special .ident directive to the end of the file to identify
      the version of GCC which compiled this code.  The format of the .ident
--- gcc/gcc/reorg.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/reorg.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -2035,6 +2035,26 @@ get_label_before (rtx insn)
   return label;
 }
 
+static bool
+insn_conflict_latency (rtx trial, rtx insn)
+{
+  int cost = insn_default_latency (trial) + 1;
+  rtx set = single_set (trial);
+
+  if (set)
+    {
+      while (insn && cost--)
+	{
+	  if (reg_referenced_p (SET_DEST (set), PATTERN (insn)))
+	    return true;
+
+	  insn = next_active_insn (insn);
+	}
+    }
+
+  return false;
+}
+
 /* Scan a function looking for insns that need a delay slot and find insns to
    put into the delay slot.
 
@@ -2191,6 +2211,7 @@ fill_simple_delay_slots (int non_jumps_p)
 	      if (! insn_references_resource_p (trial, &set, true)
 		  && ! insn_sets_resource_p (trial, &set, true)
 		  && ! insn_sets_resource_p (trial, &needed, true)
+		  && ! insn_conflict_latency (trial, next_active_insn (insn))
 #ifdef HAVE_cc0
 		  /* Can't separate set of cc0 from its use.  */
 		  && ! (reg_mentioned_p (cc0_rtx, pat) && ! sets_cc0_p (pat))
--- gcc/gcc/DEV-PHASE	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/DEV-PHASE	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1 @@
+
--- gcc/gcc/testsuite/gcc.c-torture/execute/st14098.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.c-torture/execute/st14098.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,21 @@
+extern void abort (void);
+
+unsigned long long int i = 0x100000000LL;
+
+int
+foo(void)
+{
+  if( i < 127LL ) return 1;
+
+  if( i == 0x8000 ) return 5;
+
+  return 0;
+}
+
+main()
+{
+  if (foo())
+    abort();
+
+  return 0;
+}
--- gcc/gcc/testsuite/gcc.c-torture/execute/lrintf.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.c-torture/execute/lrintf.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,53 @@
+#include <math.h>
+
+extern void abort (void);
+extern void exit (int);
+
+void __attribute__ ((noinline))
+test(float a)
+{
+  int a1 = __builtin_lrintf (a);
+  int a3 = lrintf (a);
+
+  if (a1 != a3)
+    exit (1);
+}
+
+int main()
+{
+  test (__builtin_nanf(""));
+  test (__builtin_inff());
+
+  test (0);
+  test (1);
+
+  test (2.5);
+  test (3.5);
+  test (-2.5);
+  test (-3.5);
+
+  test (2.4);
+  test (3.4);
+  test (-2.4);
+  test (-3.4);
+
+  test (2.0);
+  test (3.0);
+  test (-2.0);
+  test (-3.0);
+
+  test (2.9);
+  test (3.9);
+  test (-2.9);
+  test (-3.9);
+
+  test (2.1);
+  test (3.1);
+  test (-2.1);
+  test (-3.1);
+
+  test (0.1);
+  test (-0.1);
+
+  return 0;
+}
--- gcc/gcc/testsuite/gcc.c-torture/execute/lrintf.x	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.c-torture/execute/lrintf.x	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,3 @@
+set additional_flags "-fno-math-errno -std=c99 -fno-builtin -fno-finite-math-only"
+
+return 0
--- gcc/gcc/testsuite/gcc.c-torture/execute/builtin-prefetch-6.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/gcc.c-torture/execute/builtin-prefetch-6.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,3 +1,6 @@
+/* can trap for SH with RADDERR on MMU with 31th bit set.  */
+/* { dg-do run { xfail sh*-*linux* } } */
+
 /* Test that __builtin_prefetch does no harm.
 
    Data prefetch should not fault if used with an invalid address.  */
--- gcc/gcc/testsuite/gcc.c-torture/execute/lroundf.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.c-torture/execute/lroundf.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,53 @@
+#include <math.h>
+
+extern void abort (void);
+extern void exit (int);
+
+void __attribute__ ((noinline))
+test(float a)
+{
+  int a1 = __builtin_lroundf (a);
+  int a3 = lroundf (a);
+
+  if (a1 != a3)
+    exit (1);
+}
+
+int main()
+{
+  test (__builtin_nanf(""));
+  test (__builtin_inff());
+
+  test (0);
+  test (1);
+
+  test (2.5);
+  test (3.5);
+  test (-2.5);
+  test (-3.5);
+
+  test (2.4);
+  test (3.4);
+  test (-2.4);
+  test (-3.4);
+
+  test (2.0);
+  test (3.0);
+  test (-2.0);
+  test (-3.0);
+
+  test (2.9);
+  test (3.9);
+  test (-2.9);
+  test (-3.9);
+
+  test (2.1);
+  test (3.1);
+  test (-2.1);
+  test (-3.1);
+
+  test (0.1);
+  test (-0.1);
+
+  return 0;
+}
--- gcc/gcc/testsuite/gcc.c-torture/execute/lroundf.x	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.c-torture/execute/lroundf.x	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,3 @@
+set additional_flags "-fno-math-errno -std=c99 -fno-builtin -fno-finite-math-only"
+
+return 0
--- gcc/gcc/testsuite/gcc.c-torture/unsorted/dump-noaddr.x	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/gcc.c-torture/unsorted/dump-noaddr.x	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -10,10 +10,10 @@ proc dump_compare { src options } {
     # loop through all the options
     foreach option $option_list {
 	file delete -force dump1
-	file mkdir dump1
+	file mkdir $tmpdir/dump1
 	c-torture-compile $src "$option $options -dumpbase dump1/$dumpbase -DMASK=1 -x c --param ggc-min-heapsize=1 -fdump-ipa-all -fdump-rtl-all -fdump-tree-all -fdump-noaddr"
 	file delete -force dump2
-	file mkdir dump2
+	file mkdir $tmpdir/dump2
 	c-torture-compile $src "$option $options -dumpbase dump2/$dumpbase -DMASK=2 -x c -fdump-ipa-all -fdump-rtl-all -fdump-tree-all -fdump-noaddr"
 	foreach dump1 [lsort [glob -nocomplain dump1/*]] {
 	    regsub dump1/ $dump1 dump2/ dump2
--- gcc/gcc/testsuite/gcc.target/sh/jump_compact_1.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.target/sh/jump_compact_1.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,17 @@
+/* { dg-do compile { target "sh*-*-*" } } */
+/* { dg-options "-O2" } */
+/* { dg-final { scan-assembler-not "braf"} }  */
+
+/* Check that no braf instruction is used to emit a far jump.  */
+
+int main(int argc,char **argv)
+{
+  if( argc < 2 )
+    goto label;
+
+  asm(".fill 16383,2,0x09");
+
+ label:
+  puts("after label");
+  return 0;
+}
--- gcc/gcc/testsuite/gcc.target/sh/jump_compact_2.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.target/sh/jump_compact_2.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,17 @@
+/* { dg-do compile { target "sh*-*-*" } } */
+/* { dg-options "-O2" } */
+/* { dg-final { scan-assembler "braf"} }  */
+
+/* Check that a braf instruction is used to emit a medium jump.  */
+
+int main(int argc,char **argv)
+{
+  if( argc < 2 )
+    goto label;
+
+  asm(".fill 16382,2,0x09");
+
+ label:
+  puts("after label");
+  return 0;
+}
--- gcc/gcc/testsuite/gcc.target/sh/fpchg1.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.target/sh/fpchg1.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,24 @@
+/* { dg-do run } */
+/* { dg-options "-O1 -m4-300" } */
+
+/* Check that the fpchg instruction is not moved in a delay slot if the
+   fallthru block uses the mode.  */
+
+__attribute__ ((weak))
+void barrier(void)
+{
+}
+
+float f;
+int i;
+double d;
+
+int main()
+{
+  i = 4;
+
+  barrier();
+
+  i = (f + (i && f && d));
+  return i;
+}
--- gcc/gcc/testsuite/gcc.target/sh/fpchg2.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.target/sh/fpchg2.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,16 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -m4-300" } */
+
+/* Make sure that fpchg is preferred over lfd.s fpscr.  */
+/* { dg-final { scan-assembler "fpchg" } } */
+/* { dg-final { scan-assembler-not "fpscr" } } */
+
+extern float c;
+
+void
+foo(int j)
+{
+  while (j--)
+    c++;
+
+}
--- gcc/gcc/testsuite/gcc.target/sh/sp-switch.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.target/sh/sp-switch.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,10 @@
+/* { dg-do compile { target "sh-*-*" } } */
+/* { dg-final { scan-assembler "mov\tr0,r15" } } */
+/* { dg-final { scan-assembler ".long\t_alt_stack" } } */
+
+void *alt_stack;
+void f() __attribute__ ((interrupt_handler, sp_switch ("alt_stack")));
+
+void f()
+{
+}
--- gcc/gcc/testsuite/gcc.target/sh/muladd.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.target/sh/muladd.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,20 @@
+/* Check that sequences of n r=r+b arecombined into r=r*nb
+   instructions.  */
+/* { dg-do compile { target "sh*-*-*" } } */
+/* { dg-options "-O2" } */
+/* { dg-final { scan-assembler-times "shl[dl]" 1 } } */
+
+int
+plus_9(int dest_y, int dc)
+{
+  dc+= dest_y;
+  dc+= dest_y;
+  dc+= dest_y;
+  dc+= dest_y;
+  dc+= dest_y;
+  dc+= dest_y;
+  dc+= dest_y;
+  dc+= dest_y;
+
+  return dc;
+}
--- gcc/gcc/testsuite/gcc.target/sh/sh-trapa.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.target/sh/sh-trapa.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,8 @@
+/* { dg-do compile { target "sh-superh-elf" } } */
+/* { dg-final { scan-assembler "trapa\t#42" } } */
+
+main()
+{
+  __builtin_trap();
+}
+
--- gcc/gcc/testsuite/gcc.dg/nested-func-4.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/gcc.dg/nested-func-4.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,4 +1,3 @@
-/* { dg-do run } */
 /* { dg-options "-pg" } */
 /* { dg-options "-pg -static" { target hppa*-*-hpux* } } */
 /* { dg-require-profiling "-pg" } */
--- gcc/gcc/testsuite/gcc.dg/fail_always_inline.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/gcc.dg/fail_always_inline.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -6,6 +6,5 @@ extern __attribute__ ((always_inline)) void
 void
 f()
 {
-  bar(); 
+  bar();
 }
-
--- gcc/gcc/testsuite/gcc.dg/builtins-nan.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/builtins-nan.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,54 @@
+/* { dg-do run } */
+/* { dg-options "-mieee" { target sh*-*-* } } */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <math.h>
+
+static int lisnan(double v)
+{
+  return (v != v);
+}
+
+static int lisnanf(float v)
+{
+  return (v != v);
+}
+
+int main(void)
+{
+  double d;
+  float f;
+
+  /* double */
+  d = __builtin_nans("");
+  if (! lisnan(d))
+    abort();
+
+  if (! __builtin_isnan(d))
+    abort();
+
+  d = __builtin_nan("");
+  if (! lisnan(d))
+    abort();
+
+  if (! __builtin_isnan(d))
+    abort();
+
+  /* float */
+  f = __builtin_nansf("");
+  if (! lisnanf(f))
+    abort();
+
+  if (! __builtin_isnanf(f))
+    abort();
+
+  f = __builtin_nanf("");
+  if (! lisnanf(f))
+    abort();
+
+  if (! __builtin_isnanf(f))
+    abort();
+
+  exit (0);
+}
--- gcc/gcc/testsuite/gcc.dg/array-index.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/array-index.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,15 @@
+/* PR tree-optimization/39423 */
+/* { dg-do compile } */
+/* { dg-options "-O2 -fdump-tree-original" } */
+
+int
+foo (int tab[], int index)
+{
+  return tab [index + 1];
+} 
+
+/* { dg-final { scan-tree-dump-times "\\+ 4" 1 "original" } } */
+/* { dg-final { scan-tree-dump-times "\\+ 1" 0 "original" } } */
+/* { dg-final { scan-tree-dump-times "index \\* 4" 1 "original" } } */
+/* { dg-final { cleanup-tree-dump "original" } } */
+
--- gcc/gcc/testsuite/gcc.dg/spec-options.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/spec-options.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,18 @@
+/* Check that -mfoo is accepted if defined in a user spec
+   and that it is not passed on the command line.  */
+/* { dg-do compile } */
+
+/* Must be processed in EXTRA_SPECS to run.  */
+/* { dg-do run { target sh*-*-* } } */
+/* { dg-options "-B${srcdir}/gcc.dg --specs=foo.specs -tfoo" } */
+
+extern void abort(void);
+
+int main(void)
+{
+#ifdef FOO
+  return 0;
+#else
+  abort();
+#endif
+}
--- gcc/gcc/testsuite/gcc.dg/const-weak.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/const-weak.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,16 @@
+/* weak constants can be replaced at link time. */
+
+/* { dg-do compile } */
+/* { dg-options "-O2 -fdump-tree-pre" } */
+
+const int wconst __attribute__((weak)) = 2;
+
+int f(void)
+{
+  return wconst;
+}
+
+/* { dg-final { scan-tree-dump-not "return 2" "pre"} } */
+/* { dg-final { cleanup-tree-dump "pre*]" } } */
+
+	
--- gcc/gcc/testsuite/gcc.dg/pr36998.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/gcc.dg/pr36998.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -2,7 +2,7 @@
 /* { dg-do compile } */
 /* { dg-options "-Os -fasynchronous-unwind-tables" } */
 /* { dg-options "-Os -mpreferred-stack-boundary=2 -fasynchronous-unwind-tables" { target { { i?86-*-* x86_64-*-* } && ia32 } } } */
-/* { dg-options "-fno-omit-frame-pointer" { target { avr-*-* } } } */
+/* { dg-options "-fno-omit-frame-pointer" { target { avr-*-* sh-*-* } } } */
 
 void foo (const char *, ...) __attribute__ ((noreturn));
 int bar (const char *, ...);
--- gcc/gcc/testsuite/gcc.dg/cpp/_Pragma3.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/gcc.dg/cpp/_Pragma3.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,6 +1,6 @@
 /* Copyright (C) 2002 Free Software Foundation, Inc.  */
 
-/* { dg-do preprocess } */
+/* { dg-do preprocess  { xfail *-*-* } } */
 
 /* Pragma buffers have a NULL "inc" member, which we would dereference
    when getting a file's date and time.
--- gcc/gcc/testsuite/gcc.dg/cpp/trad/include.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/gcc.dg/cpp/trad/include.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -7,6 +7,5 @@
    Newlib uses ## when including stdlib.h as of 2007-09-07.  */
 /* { dg-do preprocess { target { { ! vxworks_kernel } && { ! newlib } } } } */
 
-#define __STDC__ 1		/* Stop complaints about non-ISO compilers.  */
 #define stdlib 1
 #include <stdlib.h>		/* { dg-bogus "o such file or directory" } */
--- gcc/gcc/testsuite/gcc.dg/ssp.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/ssp.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,13 @@
+/* { dg-do compile { target fpic } } */
+/* { dg-options "-Os -fpic -fstack-protector-all -fnon-call-exceptions " } */
+/* Causes error: unable to find a register to spill in class 'R0_REGS' on SH4 */
+
+int
+foo (int d, int rp)
+{
+  if (d == 0)
+    if (rp == 0)
+      return 1;
+
+  return 0;
+}
--- gcc/gcc/testsuite/gcc.dg/case-const-1.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/gcc.dg/case-const-1.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,9 +1,11 @@
 /* Test for case labels not integer constant expressions but folding
-   to integer constants (used in Linux kernel, PR 39613).  */
+   to integer constants (used in Linux kernel, PR 39613, 52283).  */
 /* { dg-do compile } */
 /* { dg-options "" } */
 
 extern int i;
+extern unsigned int u;
+
 void
 f (int c)
 {
@@ -13,3 +15,13 @@ f (int c)
       ;
     }
 }
+
+void
+b (int c)
+{
+  switch (c)
+    {
+    case (int) (2  | ((4 < 8) ? 8 : u)):
+      ;
+    }
+}
--- gcc/gcc/testsuite/gcc.dg/muladdsi3.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/muladdsi3.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,19 @@
+/* { dg-do run } */
+
+int test_status=1;
+__attribute__((noinline)) void bar(int a, int b, int c)
+{
+  if (a==33 && b==44 && c==55) 
+     test_status=0;
+}
+
+void foo(int a, int b)
+{
+  bar(11 + 22 * a, 44 * b, 55);
+}
+
+int main()
+{
+  foo(1, 1);
+  return test_status;
+}
--- gcc/gcc/testsuite/gcc.dg/shlrtst.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/shlrtst.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,25 @@
+/* { dg-do run } */
+
+extern void abort (void);
+unsigned char test_char[2] ={0x80,0x40};
+
+void Funtion_test(unsigned char *buf, unsigned int num)
+{
+    unsigned int byte = num / 2;
+
+    if(!num)
+      buf[ byte] |= 0x80;
+    else
+      buf[ byte] |= 0x7f;      
+}
+
+main()
+{
+  Funtion_test(test_char, 0);
+
+  if (test_char[0] != 0x80)
+    abort();
+
+  return 0;
+}
+
--- gcc/gcc/testsuite/gcc.dg/profiling/branch-probabilities/main.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/profiling/branch-probabilities/main.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,22 @@
+/* { dg-do link } */
+/* { dg-options { -O2 -fno-inline -fprofile-arcs } } */
+/* { dg-additional-sources "others.c others3.c" } */
+/* { dg-do run { target native } } */
+
+main()
+{
+  int i = 0;
+
+  puts ("foo");
+
+  for (i=0; i < 100000; i++)
+    {
+      freq();
+      freq1();
+    }
+
+  unfreq1();
+  unfreq2();
+
+  return 0;
+}
--- gcc/gcc/testsuite/gcc.dg/profiling/branch-probabilities/others.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/profiling/branch-probabilities/others.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,52 @@
+/* { dg-do assemble } */
+/* { dg-options { -O2 -fno-inline -fbranch-probabilities } } */
+
+void unfreq1() __attribute__ ((cold)) ;
+void freq1() __attribute__ ((hot)) ;
+void notfreq1() __attribute__ ((hot)) ;
+
+void
+unfreq1()
+{
+}
+
+void
+freq1()
+{
+}
+
+void
+notfreq1()
+{
+}
+
+
+freq()
+{
+}
+
+
+never()
+{
+}
+
+unfreq2()
+{
+}
+
+never2()
+{
+}
+
+__attribute__ ((weak)) wnever()
+{
+}
+
+/* { dg-final { scan-obj { <_unfreq1> }   { section .text: }          { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_unfreq2> }   { section .text: }	      { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_freq1> }	  { section .text.hot: }      { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_freq> }	  { section .text.hot: }      { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_notfreq1> }  { section .text.unlikely: } { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_never> }	  { section .text.unlikely: } { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_never2> }    { section .text.unlikely: } { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_wnever> }    { section .text.unlikely: } { Disassembly of section } { others.o } } } */
--- gcc/gcc/testsuite/gcc.dg/profiling/branch-probabilities/others3.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/profiling/branch-probabilities/others3.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,9 @@
+/* { dg-do assemble } */
+/* { dg-options { -O2 -fno-inline -fbranch-probabilities } } */
+
+never3()
+{
+}
+
+/* { dg-final { scan-obj { <_never3> }    { section .text.unlikely: }          { Disassembly of section } { others3.o } } } */
+/* { dg-final { cleanup-saved-temps ".gcda" } } */
--- gcc/gcc/testsuite/gcc.dg/profiling/default/main.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/profiling/default/main.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,22 @@
+/* { dg-do link } */
+/* { dg-options { -O2 -fno-inline -fprofile-arcs } } */
+/* { dg-additional-sources "others.c others3.c" } */
+/* { dg-do run { target native } } */
+
+main()
+{
+  int i = 0;
+
+  puts ("foo");
+
+  for (i=0; i < 100000; i++)
+    {
+      freq();
+      freq1();
+    }
+
+  unfreq1();
+  unfreq2();
+
+  return 0;
+}
--- gcc/gcc/testsuite/gcc.dg/profiling/default/others.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/profiling/default/others.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,52 @@
+/* { dg-do assemble } */
+/* { dg-options { -O2 -fno-inline } } */
+
+void unfreq1() __attribute__ ((cold)) ;
+void freq1() __attribute__ ((hot)) ;
+void notfreq1() __attribute__ ((hot)) ;
+
+void
+unfreq1()
+{
+}
+
+void
+freq1()
+{
+}
+
+void
+notfreq1()
+{
+}
+
+
+freq()
+{
+}
+
+
+never()
+{
+}
+
+unfreq2()
+{
+}
+
+never2()
+{
+}
+
+__attribute__ ((weak)) wnever()
+{
+}
+
+/* { dg-final { scan-obj { <_unfreq1> }  { section .text.unlikely: } { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_freq1> }	 { section .text.hot: }      { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_notfreq1> } { section .text.hot: }      { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_freq> }	 { section .text: }	     { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_never> }	 { section .text: }	     { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_unfreq2> }  { section .text: }	     { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_never2> }   { section .text: }	     { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_wnever> }   { section .text: }	     { Disassembly of section } { others.o } } } */
--- gcc/gcc/testsuite/gcc.dg/profiling/default/others3.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/profiling/default/others3.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,10 @@
+/* { dg-do assemble } */
+/* { dg-options { -O2 -fno-inline } } */
+
+never3()
+{
+}
+
+/* { dg-final { scan-obj { <_never3> }   { section .text: }	     { Disassembly of section } { others3.o } } } */
+/* { dg-final { cleanup-saved-temps ".gcda" } } */
+
--- gcc/gcc/testsuite/gcc.dg/profiling/profiling.exp	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/profiling/profiling.exp	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,34 @@
+#   Copyright (C) 2004, 2007 Free Software Foundation, Inc.
+
+# This program is free software; you can redistribute it and/or modify
+# it under the terms of the GNU General Public License as published by
+# the Free Software Foundation; either version 3 of the License, or
+# (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with GCC; see the file COPYING3.  If not see
+# <http://www.gnu.org/licenses/>.
+
+# Profiling option to add to the existing options set
+# The first argument is the line number of the dg-profile-options directive
+# within the test file. Remaining arguments are as for xfail lists:
+# option { target }
+
+# Load support procs.
+load_lib gcc-dg.exp
+load_lib scanobj.exp
+
+# Initialize `dg'.
+dg-init
+
+# Main loop.
+dg-runtest [lsort [glob -nocomplain $srcdir/$subdir/*/*.c ]] \
+        "" ""
+
+# All done.
+dg-finish
--- gcc/gcc/testsuite/gcc.dg/profiling/function-section/main.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/profiling/function-section/main.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,22 @@
+/* { dg-do link } */
+/* { dg-options { -O2 -fno-inline -fprofile-arcs } } */
+/* { dg-additional-sources "others.c others3.c" } */
+/* { dg-do run { target native } } */
+
+main()
+{
+  int i = 0;
+
+  puts ("foo");
+
+  for (i=0; i < 100000; i++)
+    {
+      freq();
+      freq1();
+    }
+
+  unfreq1();
+  unfreq2();
+
+  return 0;
+}
--- gcc/gcc/testsuite/gcc.dg/profiling/function-section/others.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/profiling/function-section/others.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,52 @@
+/* { dg-do assemble } */
+/* { dg-options { -O2 -fno-inline -ffunction-sections } } */
+
+void unfreq1() __attribute__ ((cold)) ;
+void freq1() __attribute__ ((hot)) ;
+void notfreq1() __attribute__ ((hot)) ;
+
+void
+unfreq1()
+{
+}
+
+void
+freq1()
+{
+}
+
+void
+notfreq1()
+{
+}
+
+
+freq()
+{
+}
+
+
+never()
+{
+}
+
+unfreq2()
+{
+}
+
+never2()
+{
+}
+
+__attribute__ ((weak)) wnever()
+{
+}
+
+/* { dg-final { scan-obj { <_unfreq1> }  { section .text.unlikely.unfreq1: } { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_freq1> }	 { section .text.hot.freq1: }	     { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_notfreq1> } { section .text.hot.notfreq1: }     { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_freq> }	 { section .text.freq: }	     { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_never> }	 { section .text.never: }	     { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_unfreq2> }  { section .text.unfreq2: }	     { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_never2> }   { section .text.never2: }	     { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_wnever> }   { section .text.wnever: }	     { Disassembly of section } { others.o } } } */
--- gcc/gcc/testsuite/gcc.dg/profiling/function-section/others3.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/profiling/function-section/others3.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,9 @@
+/* { dg-do assemble } */
+/* { dg-options { -O2 -fno-inline -ffunction-sections } } */
+
+never3()
+{
+}
+
+/* { dg-final { scan-obj { <_never3> }   { section .text.never3: }	     { Disassembly of section } { others3.o } } } */
+/* { dg-final { cleanup-saved-temps ".gcda" } } */
--- gcc/gcc/testsuite/gcc.dg/profiling/branch-probabilities-function-section/main.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/profiling/branch-probabilities-function-section/main.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,22 @@
+/* { dg-do link } */
+/* { dg-options { -O2 -fno-inline -fprofile-arcs } } */
+/* { dg-additional-sources "others.c others3.c" } */
+/* { dg-do run { target native } } */
+
+main()
+{
+  int i = 0;
+
+  puts ("foo");
+
+  for (i=0; i < 100000; i++)
+    {
+      freq();
+      freq1();
+    }
+
+  unfreq1();
+  unfreq2();
+
+  return 0;
+}
--- gcc/gcc/testsuite/gcc.dg/profiling/branch-probabilities-function-section/others.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/profiling/branch-probabilities-function-section/others.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,52 @@
+/* { dg-do assemble } */
+/* { dg-options { -O2 -fno-inline -fbranch-probabilities -ffunction-sections -fdata-sections -Wl,-gc-section } } */
+
+void unfreq1() __attribute__ ((cold)) ;
+void freq1() __attribute__ ((hot)) ;
+void notfreq1() __attribute__ ((hot)) ;
+
+void
+unfreq1()
+{
+}
+
+void
+freq1()
+{
+}
+
+void
+notfreq1()
+{
+}
+
+
+freq()
+{
+}
+
+
+never()
+{
+}
+
+unfreq2()
+{
+}
+
+never2()
+{
+}
+
+__attribute__ ((weak)) wnever()
+{
+}
+
+/* { dg-final { scan-obj { <_unfreq1> }   { section .text.unfreq1: }           { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_unfreq2> }   { section .text.unfreq2: }	       { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_freq1> }	  { section .text.hot.freq1: }         { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_freq> }	  { section .text.hot.freq: }	       { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_notfreq1> }  { section .text.unlikely.notfreq1: } { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_never> }	  { section .text.unlikely.never: }    { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_never2> }    { section .text.unlikely.never2: }   { Disassembly of section } { others.o } } } */
+/* { dg-final { scan-obj { <_wnever> }    { section .text.unlikely.wnever: }   { Disassembly of section } { others.o } } } */
--- gcc/gcc/testsuite/gcc.dg/profiling/branch-probabilities-function-section/others3.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/profiling/branch-probabilities-function-section/others3.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,9 @@
+/* { dg-do assemble } */
+/* { dg-options { -O2 -fno-inline -fbranch-probabilities -ffunction-sections -fdata-sections -Wl,-gc-section } } */
+
+never3()
+{
+}
+
+/* { dg-final { scan-obj { <_never3> }    { section .text.unlikely.never3: }	      { Disassembly of section } { others3.o } } } */
+/* { dg-final { cleanup-saved-temps ".gcda" } } */
--- gcc/gcc/testsuite/gcc.dg/always_inline2.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/gcc.dg/always_inline2.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,8 +1,8 @@
 /* { dg-do compile } */
-/* { dg-options "-O2" } */
-inline __attribute__ ((always_inline)) void t(void); /* { dg-error "body not available" } */
+/* { dg-options "-Winline -O2" } */
+inline __attribute__ ((always_inline)) void t(void); /* { dg-warning "body not available" } */
 void
 q(void)
 {
-  t(); 				/* { dg-error "called from here" } */
+  t(); 				/* { dg-warning "called from here" } */
 }
--- gcc/gcc/testsuite/gcc.dg/pr42427.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/gcc.dg/pr42427.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -2,6 +2,7 @@
 /* { dg-options "-O2 -fexceptions -fnon-call-exceptions -fpeel-loops" } */
 /* { dg-add-options c99_runtime } */
 /* { dg-require-effective-target ilp32 } */
+/* { dg-require-effective-target complex } */
 
 #include <complex.h>
 
--- gcc/gcc/testsuite/gcc.dg/case-const-2.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/gcc.dg/case-const-2.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,9 +1,11 @@
 /* Test for case labels not integer constant expressions but folding
-   to integer constants (used in Linux kernel, PR 39613).  */
+   to integer constants (used in Linux kernel, PR 39613, 52283).  */
 /* { dg-do compile } */
 /* { dg-options "-pedantic" } */
 
 extern int i;
+extern unsigned int u;
+
 void
 f (int c)
 {
@@ -13,3 +15,14 @@ f (int c)
       ;
     }
 }
+
+void
+b (int c)
+{
+  switch (c)
+    {
+    case (int) (2  | ((4 < 8) ? 8 : u)): /* { dg-warning "case label is not an integer constant expression" } */
+      ;
+    }
+}
+
--- gcc/gcc/testsuite/gcc.dg/always_inline3.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/gcc.dg/always_inline3.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,11 +1,11 @@
 /* { dg-do compile } */
-/* { dg-options "-O2" } */
+/* { dg-options "-Winline -O2" } */
 int do_something_evil (void);
 inline __attribute__ ((always_inline)) void
-q2(void) /* { dg-error "recursive inlining" } */
+q2(void) /* { dg-warning "recursive inlining" } */
 {
   if (do_something_evil ())
     return;
-  q2(); 			/* { dg-error "called from here" } */
+  q2(); 			/* { dg-warning "called from here" } */
   q2(); /* With -O2 we don't warn here, it is eliminated by tail recursion.  */
 }
--- gcc/gcc/testsuite/gcc.dg/fold-plusmult-2.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/gcc.dg/fold-plusmult-2.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,20 +1,20 @@
 /* { dg-do compile } */
-/* { dg-options "-fdump-tree-original" } */
+/* { dg-options "-O2 -fdump-tree-original" } */
 
 int foo (int i)
 {
   return 2 + i * 4;
 }
 
-/* We do _not_ want the above to be canonicalized to (i * 2 + 1) * 2.  */
-
-int bar (int i)
+int
+bar (int tab[], int index)
 {
-  return 4 + i * 2;
+  return tab [index+2];
 }
 
-/* But eventually this to be canonicalized to (i + 2) * 2.  */
 
+/* The rational is that it is best not to downsize the multiplier. */
+
 /* { dg-final { scan-tree-dump "i \\\* 4 \\\+ 2" "original" } } */
-/* { dg-final { scan-tree-dump "\\\(i \\\+ 2\\\) \\\* 2" "original" } } */
+/* { dg-final { scan-tree-dump "index \\\* 4\\\) \\\+ 8" "original" } } */
 /* { dg-final { cleanup-tree-dump "original" } } */
--- gcc/gcc/testsuite/gcc.dg/fold-plusmult.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/gcc.dg/fold-plusmult.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,5 +1,8 @@
 /* { dg-do compile } */
-/* { dg-options "-fdump-tree-original" } */
+/* moved from gimple to rtl. see
+	http://gcc.gnu.org/bugzilla/show_bug.cgi?id=39423
+*/
+/* { dg-options "-O2 -fdump-rtl-cse2" } */
 
 int test1 (int a)
 {
@@ -11,5 +14,6 @@ int test2 (int a)
   return (a + a)*2;
 }
 
-/* { dg-final { scan-tree-dump-times "<a> \\\* 4" 2 "original" } } */
-/* { dg-final { cleanup-tree-dump "original" } } */
+/* { dg-final { scan-rtl-dump-times "ashift" 2 "cse2" } } */
+/* { dg-final { scan-rtl-dump-times "const_int 2" 2 "cse2" } } */
+/* { dg-final { cleanup-tree-dump "cse2" } } */
--- gcc/gcc/testsuite/gcc.dg/case-const-3.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/gcc.dg/case-const-3.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,9 +1,11 @@
 /* Test for case labels not integer constant expressions but folding
-   to integer constants (used in Linux kernel, PR 39613).  */
+   to integer constants (used in Linux kernel, PR 39613, 52283, ).  */
 /* { dg-do compile } */
 /* { dg-options "-pedantic-errors" } */
 
 extern int i;
+extern unsigned int u;
+
 void
 f (int c)
 {
@@ -13,3 +15,16 @@ f (int c)
       ;
     }
 }
+
+void
+b (int c)
+{
+  switch (c)
+    {
+    case (int) (2  | ((4 < 8) ? 8 : u)): /* { dg-error "case label is not an integer constant expression" } */
+      ;
+    }
+}
+
+
+
--- gcc/gcc/testsuite/gcc.dg/foo.specs	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/gcc.dg/foo.specs	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,2 @@
+*cppruntime:
++ %{tfoo: -DFOO}
--- gcc/gcc/testsuite/ChangeLog.STM	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/ChangeLog.STM	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,197 @@
+2013-01-13  Christian Bruel  <christian.bruel@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=25892
+	* gcc.target/sh/sh-switch.c: New testcase.
+
+2012-12-06  Laurent Alfonsi  <laurent.alfonsi@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=24977
+	* gcc.dg/muladdsi3.c: New test.
+
+2012-04-25  Laurent Alfonsi  <laurent.alfonsi@st.com>
+
+        * lib/c-torture.exp (c-torture-execute): Appends 
+	additional linker flag if exists.
+
+2012-09-10  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.target/sh/muladd.c: New test.
+
+2012-08-23  Laurent Alfonsi  <laurent.alfonsi@st.com>
+
+	* lib/scanobj.exp: Added
+	* gcc.dg/profiling/profiling.exp: Added
+	* gcc.dg/profiling/default: New tests.
+	* gcc.dg/profiling/branch-probabilities: New tests.
+	* gcc.dg/profiling/function-section: New tests.
+	* gcc.dg/profiling/branch-probabilities-function-section: New tests.
+
+2012-05-30  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/spec-options.c: New test.
+	* gcc.dg/foo.specs: New file.
+
+2012-04-23  Christian Bruel  <christian.bruel@st.com>
+
+        https://bugzilla.stlinux.com/show_bug.cgi?id=18466
+        * gcc.target/sh/jump_compact_1.c: New test.
+        * gcc.target/sh/jump_compact_2.c: New test.
+
+2010-02-15  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/case-const-1.c: Add cond expr label and cast case.
+	* gcc.dg/case-const-2.c: Likewise.
+	* gcc.dg/case-const-3.c: Likewise.
+
+2012-02-14  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/pr49948.c: Disable if no pthread.
+
+2012-01-12  Christian Bruel  <christian.bruel@st.com>
+
+	Backport from mainline
+	2011-12-05  Jakub Jelinek  <jakub@redhat.com>
+		    Eric Botcazou  <ebotcazou@adacore.com>
+
+	PR middle-end/51323
+	PR middle-end/50074
+	* gcc.c-torture/execute/pr51323.c: New test.
+
+2011-09-02  Christian Bruel  <christian.bruel@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=14098
+	* gcc.c-torture/execute/st14098.c: New.
+
+	Backport from mainline
+	2011-05-30  Kaz Kojima  <kkojima@gcc.gnu.org>
+
+	PR target/49186
+	* gcc.c-torture/execute/pr49186.c: New.
+
+2010-06-30  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.target/sh/sh-trapa.c: Test trapa.
+
+2011-03-18 Christian Bruel  <christian.bruel@st.com>
+
+	* g++.dg/warn/Wstrict-aliasing-float-ref-int-obj.C: Test success.
+
+2011-02-11 Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.c-torture/gcc.dg/cpp/_Pragma3.c: xfail.
+
+2010-12-07  Christian Bruel  <christian.bruel@st.com>
+
+	* lib/lto.exp: Disable whopr.
+	* lib/gcc.exp: Likewise.
+	* lib/c-torture.exp: Likewise.
+
+2010-11-08  Christian Bruel  <christian.bruel@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=10391
+	* gcc.dg/ssp.c: New testcase. 
+
+2010-11-03  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.c-torture/execute/lroundf.c: New testcase. 
+	* gcc.c-torture/execute/lroundf.x: New.
+
+2010-10-14  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.c-torture/execute/lrintf.c: New testcase. 
+	* gcc.c-torture/execute/lrintf.x: New.
+
+2010-08-10  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.c-torture/execute/builtin-prefetch-6.c: xfail for sh-linux.
+
+2010-06-15  Christian Bruel  <christian.bruel@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=9320
+	* gcc.dg/shlrtst.c: New testcase. 
+
+2010-06-07  Christian Bruel  <christian.bruel@st.com>
+
+	* lib/target-supports.exp (check_effective_target_complex): Define.
+	* gcc.dg/pr42427.c: Check complex available.
+
+2010-04-20  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl30850:
+	* gcc.dg/builtins-nan.c: New test.
+
+2010-01-26  Christian Bruel  <christian.bruel@st.com>
+
+	http://gcc.gnu.org/bugzilla/show_bug.cgi?id=39423
+	* gcc.dg/fold-plusmult.c: Check rtl instead of gimple.
+
+2010-01-26  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.target/sh/20080410-1.c: Remove -fira option.
+
+2009-11-10  Christian Bruel  <christian.bruel@st.com>
+
+	* g++.dg/eh/postreload.C: New test.
+
+2009-10-06  Antony King  <antony.king@st.com>
+
+	INSbl30052:
+	* lib/target-supports.exp (check_effective_target_sync_int_long):
+	Enable atomic builtins for sh*-superh-elf.
+	(check_effective_target_sync_char_short): Likewise.
+
+2009-09-29  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/const-weak.c: Change dump.
+
+2009-09-03  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/memcpy-1.c: xfail for SH.
+
+2009-09-03  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/const-weak.c: Update dg-final rules.
+
+2008-06-01  Richard Sandiford  <rdsandiford@googlemail.com>
+ 
+	* gcc.c-torture/execute/ieee/ieee.exp: Load c-torture.exp.
+
+2009-07-03  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/torture/pr36998.c: Add -fno-omit-frame-pointer for SH.
+
+2009-06-18  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/torture/pr37868.c: xfail for SH.
+	
+2009-06-26  Christian Bruel  <christian.bruel@st.com>
+
+	* lib/target-supports.exp: Disable profiling for SH when on the simu.
+
+2009-03-12  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/array-index.c: New optimisation test.
+
+2009-05-27  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/cpp/trad/include.c: Don't force __STDC__.
+
+2008-05-06  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/28671
+	* gcc.dg/const-weak.c: New testcase. 
+
+2008-04-17  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/28594
+	* gcc.dg/long-long-compare-1.c: New testcase. 
+	
+2008-01-28  Christian Bruel  <christian.bruel@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=3313
+	* gcc.dg/packed-array.c: New testcase. 
+
+2007-01-31  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.target/sh/fpchg1.c: New test.
+	* gcc.target/sh/fpchg2.c: Idem.
--- gcc/gcc/testsuite/g++.dg/other/error27.C	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/g++.dg/other/error27.C	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,5 +1,4 @@
 // PR c++/35332
-// { dg-do compile }
 // { dg-options "-fno-finite-math-only" { target sh*-*-* } }
 
 void foo (double x, double y)
--- gcc/gcc/testsuite/g++.dg/warn/Wstrict-aliasing-float-ref-int-obj.C	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/g++.dg/warn/Wstrict-aliasing-float-ref-int-obj.C	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -5,8 +5,8 @@
 int foo() {
   int x;
   float& q = reinterpret_cast<float&> (x);  /* { dg-message "dereferencing type-punned" "" { target *-*-* } } */
-  q = 1.0; /* { dg-warning "does break strict-aliasing" "" { xfail *-*-* } } */
+  q = 1.0; /* { dg-warning "does break strict-aliasing" "" } */
   return x;
 }
 
-/* { dg-message "initialized" "" { xfail *-*-* } 7 } */
+
--- gcc/gcc/testsuite/g++.dg/eh/postreload.C	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/g++.dg/eh/postreload.C	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,35 @@
+// This testcase failed on sh, because stack offset to access i was shared
+// between handler and main.
+// { dg-do run }
+// { dg-options "-O2" }
+
+extern void abort (void);
+extern void exit (int);
+
+void
+bar (int *i, int *tab) __attribute__ ((weak,noinline)); 
+
+main()
+{
+  int i = 123;
+  int tab[47];
+
+  bar (&i, tab);
+
+  try
+    {
+      throw 1;
+    }
+  catch (...)
+    {
+      return 0;
+    }
+
+  abort ();
+}
+
+void
+bar (int *i, int *tab)
+{
+}
+
--- gcc/gcc/testsuite/lib/c-torture.exp	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/lib/c-torture.exp	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -126,7 +126,8 @@ proc c-torture-compile { src option } {
 #
 proc c-torture-execute { sources args } {
     global tmpdir tool srcdir output compiler_conditional_xfail_data
-
+    global additional_linker_flag
+    
     # Use the first source filename given as the filename under test.
     set src [lindex $sources 0]
 
@@ -135,6 +136,11 @@ proc c-torture-execute { sources args } {
     } else {
 	set additional_flags ""
     }
+
+    if { [info exists additional_linker_flag] } {
+       lappend additional_flags $additional_linker_flag
+    }
+
     # Check for alternate driver.
     if [file exists [file rootname $src].x] {
 	verbose "Using alternate driver [file rootname [file tail $src]].x" 2
--- gcc/gcc/testsuite/lib/scanobj.exp	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/testsuite/lib/scanobj.exp	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,138 @@
+# Reset data used to avoid unecessary recompilation of same file
+proc dg-scan-obj-reset {} {
+    global saved_output_filename
+    global saved_output_datafile
+
+    if [info exists saved_output_filename] {
+        set saved_output_filename 0
+    }
+    if [info exists saved_output_datafile] {
+        set saved_output_datafile 0
+    }
+}
+
+
+# Scan a symbol in an object file.  If it is present and POSITIVE
+# is non-zero, or it is not present and POSITIVE is zero, the test
+# passes.  The ORIG_ARGS is the list of arguments provided by dg-final
+# to scan-assembler.
+# The first element in ORIG_ARGS is the regular expression to look for
+# in the file.
+# The secund element in ORIG_ARGS is the begin pattern to look for in the file.
+# The third element in ORIG_ARGS is the end pattern to look for in the file.
+# The fourth element in ORIG_ARGS, if present, is the object file.
+# The last element in ORIG_ARGS, if present, is a DejaGNU target selector.
+proc dg-scan-obj { name positive testcase orig_args } {
+    global GCC_UNDER_TEST
+    global objdump
+    global base_dir
+    global saved_output_filename
+    global saved_output_datafile
+
+    if { [llength $orig_args] >= 5 } {
+	switch [dg-process-target [lindex $orig_args 4]] {
+	    "S" { }
+	    "N" { return }
+	    "F" { setup_xfail "*-*-*" }
+	    "P" { }
+	}
+    }
+
+    # Find objdump like we find g++ in g++.exp.
+    if ![info exists objdump]  {
+        set objdump "objdump"
+        regsub "(.*)gcc$" $GCC_UNDER_TEST {\1objdump} objdump
+	set objdump [findfile $objdump $objdump \
+                [findfile $base_dir/../../../binutils/objdump \
+                     $base_dir/../../../binutils/objdump \
+                     [findfile $base_dir/../../objdump $base_dir/../../objdump \
+                          [findfile $base_dir/objdump $base_dir/objdump \
+                               [transform objdump]]]]]
+	verbose -log "objdump is $objdump"
+    }
+
+    # what is the object filename?
+    if { [llength $orig_args] >= 4 } {
+        # object file is given by the user
+        set objfile [lindex $orig_args 3]
+    } else {
+        # object file is the one from the testcase
+        set objfile "[file rootname [file tail [lindex $testcase 0]]].o"
+    }
+    set objfile [string trim $objfile]
+
+    set output_file "[glob -nocomplain $objfile]"
+    if { $output_file == "" } {
+	fail "$name $orig_args: object file does not exist"
+	return
+    }
+
+    if {[info exists saved_output_filename] &&
+        [info exists saved_output_datafile] &&
+        $saved_output_filename == $output_file} {
+        # get data from previous step
+        set output $saved_output_datafile
+    } else {
+        # get new data
+        set fd [open "| $objdump -d $output_file" r]
+        set output [read $fd]
+        close $fd
+        set saved_output_filename $output_file
+        set saved_output_datafile $output
+    }
+
+    set pattern [string trim [lindex $orig_args 0]]
+    set begin   [string trim [lindex $orig_args 1]]
+    set end     [string trim [lindex $orig_args 2]]
+
+    set match 0
+    set inside 0
+    set data [split $output "\n"]
+    foreach line $data {
+        if { $inside == "1" } {
+            # Detect pattern
+            if { [regexp $pattern $line] } {
+                set match 1
+                break
+            }
+            # Detect end
+            if { [regexp $end $line] } {
+                break
+            }
+        }
+        # Detect begin
+        if { [regexp $begin $line] } {
+            set inside 1
+        }
+    }
+
+    if { $match == $positive } {
+	pass "$testcase $name $orig_args"
+    } else {
+	fail "$testcase $name $orig_args"
+    }
+}
+
+
+# Look for a symbol in an object file, invoked via dg-final.
+# See dg-scan-obj for details.
+
+proc scan-obj { args } {
+    # This assumes that we are two frames down from dg-test, and that
+    # it still stores the filename of the testcase in a local variable "name".
+    upvar 2 name testcase
+
+    dg-scan-obj "scan-obj" 1 $testcase $args
+}
+
+
+# Check that a symbol is not present in an object file, invoked via dg-final.
+# See dg-scan-obj for details.
+
+proc scan-obj-not { args } {
+    # This assumes that we are two frames down from dg-test, and that
+    # it still stores the filename of the testcase in a local variable "name".
+    upvar 2 name testcase
+
+    dg-scan-obj "scan-obj-not" 0 $testcase $args
+}
--- gcc/gcc/testsuite/lib/target-supports.exp	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/testsuite/lib/target-supports.exp	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -498,6 +498,13 @@ proc check_profiling_available { test_what } {
 	return 0
     }
 
+    # There is no profiling support on SH when running on the simu.
+    if { [istarget sh-superh-elf] } {
+	if [board_info target exists is_simulator] {
+	    return 0
+	}
+    }
+
     # uClibc does not have gcrt1.o.
     if { [check_effective_target_uclibc]
 	 && ($test_what == "-p" || $test_what == "-pg") } {
@@ -3865,6 +3872,7 @@ proc check_effective_target_sync_int_long { } {
 	     || [istarget hppa*-*linux*]
 	     || [istarget s390*-*-*] 
 	     || [istarget powerpc*-*-*]
+	     || [istarget sh*-superh-elf]
 	     || ([istarget sparc*-*-*] && [check_effective_target_sparc_v9])
 	     || [check_effective_target_mips_llsc] } {
            set et_sync_int_long_saved 1
@@ -3894,6 +3902,7 @@ proc check_effective_target_sync_char_short { } {
 	     || [istarget hppa*-*linux*]
 	     || [istarget s390*-*-*] 
 	     || [istarget powerpc*-*-*]
+	     || [istarget sh*-superh-elf]
 	     || ([istarget sparc*-*-*] && [check_effective_target_sparc_v9])
 	     || [check_effective_target_mips_llsc] } {
            set et_sync_char_short_saved 1
@@ -3945,6 +3954,15 @@ proc check_effective_target_newlib {} {
     }]
 }
 
+# Return true if this is a target supports complex.h
+
+proc check_effective_target_complex { } {
+    return [check_no_compiler_messages complex assembly {
+	#include <complex.h>
+	main() { complex double a; return 0; }
+    }]
+}
+
 # Return 1 if
 #   (a) an error of a few ULP is expected in string to floating-point
 #       conversion functions; and
--- gcc/gcc/cp/ChangeLog.STM	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/cp/ChangeLog.STM	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,9 @@
+2011-03-21  Christian Bruel  <christian.bruel@st.com>
+
+	Backport from trunk: PH_CPP 82Y23
+	* parser.c: (cp_parser_primary_expressio): Don't warn.
+
+2009-03-26  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl27506
+	* name-lookup.c: (do_nonmember_using_decl): Fixed error handling.
--- gcc/gcc/cp/name-lookup.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/cp/name-lookup.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -2560,7 +2560,11 @@ do_nonmember_using_decl (tree scope, tree name, tr
     {
       *newtype = decls.type;
       if (oldtype && *newtype && !decls_match (oldtype, *newtype))
+	{
 	error ("%qD is already declared in this scope", name);
+	  *newtype = NULL_TREE;
+	}
+
     }
 
     /* If *newval is empty, shift any class or enumeration name down.  */
--- gcc/gcc/builtin-types.def	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/builtin-types.def	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -125,11 +125,11 @@ DEF_PRIMITIVE_TYPE (BT_DFLOAT128_PTR, dfloat128_pt
 DEF_PRIMITIVE_TYPE (BT_VALIST_REF, va_list_ref_type_node)
 DEF_PRIMITIVE_TYPE (BT_VALIST_ARG, va_list_arg_type_node)
 
-DEF_PRIMITIVE_TYPE (BT_I1, builtin_type_for_size (BITS_PER_UNIT*1, 1))
-DEF_PRIMITIVE_TYPE (BT_I2, builtin_type_for_size (BITS_PER_UNIT*2, 1))
-DEF_PRIMITIVE_TYPE (BT_I4, builtin_type_for_size (BITS_PER_UNIT*4, 1))
-DEF_PRIMITIVE_TYPE (BT_I8, builtin_type_for_size (BITS_PER_UNIT*8, 1))
-DEF_PRIMITIVE_TYPE (BT_I16, builtin_type_for_size (BITS_PER_UNIT*16, 1))
+DEF_PRIMITIVE_TYPE (BT_I1, builtin_type_for_size (BITS_PER_UNIT*1, 0))
+DEF_PRIMITIVE_TYPE (BT_I2, builtin_type_for_size (BITS_PER_UNIT*2, 0))
+DEF_PRIMITIVE_TYPE (BT_I4, builtin_type_for_size (BITS_PER_UNIT*4, 0))
+DEF_PRIMITIVE_TYPE (BT_I8, builtin_type_for_size (BITS_PER_UNIT*8, 0))
+DEF_PRIMITIVE_TYPE (BT_I16, builtin_type_for_size (BITS_PER_UNIT*16, 0))
 
 DEF_POINTER_TYPE (BT_PTR_CONST_STRING, BT_CONST_STRING)
 DEF_POINTER_TYPE (BT_PTR_LONG, BT_LONG)
--- gcc/gcc/tree-ssa-ccp.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/tree-ssa-ccp.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -3,6 +3,7 @@
    2010, 2011, 2012 Free Software Foundation, Inc.
    Adapted from original RTL SSA-CCP by Daniel Berlin <dberlin@dberlin.org>
    Adapted to GIMPLE trees by Diego Novillo <dnovillo@redhat.com>
+   Copyright (c) 2010 STMicroelectronics.
 
 This file is part of GCC.
 
@@ -385,6 +386,9 @@ canonicalize_float_value (prop_value_t *val)
   if (!HONOR_NANS (mode)
       && REAL_VALUE_ISNAN (d))
     {
+      if (warn_non_finite_math)
+	warning (OPT_Wnon_finite_math,
+		 "non-finite operation %E not honored", val->value);
       val->lattice_val = UNDEFINED;
       val->value = NULL;
       return;
--- gcc/gcc/mode-switching.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/mode-switching.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,6 +1,7 @@
 /* CPU mode switching
    Copyright (C) 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2007, 2008,
    2009, 2010 Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -97,7 +98,182 @@ static void reg_dies (rtx, HARD_REG_SET *);
 static void reg_becomes_live (rtx, const_rtx, void *);
 static void make_preds_opaque (basic_block, int);
 
+/* Bitmap to compute mode flipping.  */
 
+static sbitmap *mode_in_flip;  /* flip in mode status for each basic blocks.  */
+static sbitmap *mode_out_flip; /* flip out mode status for each basic blocks.  */
+/* To support mode switching, the algorithm cannot set the modes after
+   the insert and delete bitmaps are computed by pre_edge_lcm, because
+   'avin' is computed iteratively for each possible modes for each entity.
+   The mode emission will be done after all mode are processed.
+   (see commit_mode_sets).  */
+
+static int **modes_needed;  /* modes needs to be inserted on this edge.  */
+
+/* Indicates that edge mode information is unknown. Cannot use 'no_mode'
+   because its value depends of its entity */
+#define NO_MODE -1
+
+
+/* Return true when one of the predecessor edges of BB is marked with
+   EDGE_COMPLEX. (similar to bb_has_eh_pred in basic_block.h).  */
+static bool
+bb_has_complex_pred (basic_block bb)
+{
+  edge e;
+  edge_iterator ei;
+
+  FOR_EACH_EDGE (e, ei, bb->preds)
+    {
+      if (e->flags & EDGE_COMPLEX)
+	return true;
+    }
+  return false;
+}
+
+/* Test avin modes.
+   if 'out' is 'true' we want to know if the mode out of the basic block
+   can be flipped. If 'in' is true we want to know if the mode entering the basic
+   block can be flipped.  */
+
+static int
+test_flip_status(int entity, basic_block bb, bool out)
+{
+  if (out)
+    return TEST_BIT (mode_out_flip[bb->index], entity);
+  else
+    return TEST_BIT (mode_in_flip[bb->index], entity);
+}
+
+/* Merges the avin modes.  */
+
+static void
+set_flip_status (sbitmap *avin, sbitmap *avout)
+{
+  basic_block bb;
+
+  FOR_EACH_BB (bb)
+    {
+      int i = bb->index;
+
+      /* Merge modes for each entity for each bb.
+	 If multiple avin modes are set for the same bb, they are not
+	 exclusive and a flip may not be emitted.
+	 If more that 2 modes can be defined, flip may not be emitted.  */
+      if (! bb_has_complex_pred (bb))
+	{
+	  sbitmap_a_xor_b (mode_in_flip[i], mode_in_flip[i], avin[i]);
+	  sbitmap_a_xor_b (mode_out_flip[i], mode_out_flip[i], avout[i]);
+	}
+    }
+}
+
+/* Allocates and initializes modes_infos.  */
+
+static void
+init_modes_infos (int n_entities)
+{
+  int j;
+  int num_edges = 0;
+  basic_block bb;
+
+  /* How many edges do we have ?  */
+
+  FOR_BB_BETWEEN (bb, ENTRY_BLOCK_PTR, EXIT_BLOCK_PTR, next_bb)
+      num_edges += EDGE_COUNT (bb->succs);
+
+  modes_needed = XNEWVEC (int *, n_entities);
+
+  for (j = 0; j < n_entities; j++)
+    {
+      modes_needed[j] = XNEWVEC (int, num_edges);
+
+      /* Initial NO_MODE value is -1, because 0 is a value mode.  */
+      memset (modes_needed[j], NO_MODE, num_edges * sizeof (int));
+    }
+
+  /* Allocates bitmaps for modes.  */
+  mode_in_flip = sbitmap_vector_alloc (last_basic_block, n_entities);
+  mode_out_flip = sbitmap_vector_alloc (last_basic_block, n_entities);
+  sbitmap_vector_zero (mode_in_flip, last_basic_block);
+  sbitmap_vector_zero (mode_out_flip, last_basic_block);
+}
+
+/* frees memory used to hold the modes information.  */
+
+static void
+free_modes_infos (int n_entities)
+{
+  int j;
+
+  for (j = 0; j < n_entities; j++)
+    free (modes_needed[j]);
+
+  free (modes_needed);
+  sbitmap_vector_free (mode_in_flip);
+  sbitmap_vector_free (mode_out_flip);
+}
+
+/* records the mode associated with edge e for entity j.  */
+
+static void
+add_mode_set (int j, int e, int mode)
+{
+  modes_needed[j][e] = mode;
+}
+
+/* returns the mode needed on edge e for entity j. -1 if none.  */
+
+static int
+get_mode (int j, int e)
+{
+  return modes_needed[j][e];
+}
+
+/* Finally, after all the modes after been inserted after lcm, we can
+   process with the mode emission.  */
+
+static int
+commit_mode_sets (struct edge_list *edge_list, int j)
+{
+  int need_commit = 0;
+  int e;
+
+  for (e = 0; e < NUM_EDGES (edge_list); e++)
+    {
+      HARD_REG_SET live_at_edge;
+      edge eg = INDEX_EDGE (edge_list, e);
+      basic_block src_bb = eg->src;
+      int mode, prev_mode;
+      rtx mode_set;
+
+      if ((mode = get_mode (j, e)) == NO_MODE)
+	continue;
+
+      prev_mode = test_flip_status (j, src_bb, true);
+
+      REG_SET_TO_HARD_REG_SET (live_at_edge, df_get_live_out (src_bb));
+
+      start_sequence ();
+      EMIT_MODE_SET (entity_map[j], mode, prev_mode, live_at_edge);
+
+      mode_set = get_insns ();
+      end_sequence ();
+
+      /* Do not bother to insert empty sequence.  */
+      if (mode_set == NULL_RTX)
+	continue;
+
+      /* We should not get an abnormal edge here.  */
+      gcc_assert (! (eg->flags & EDGE_ABNORMAL));
+	
+      need_commit = 1;
+      insert_insn_on_edge (mode_set, eg);
+    }
+
+  return need_commit;
+}
+
 /* This function will allocate a new BBINFO structure, initialized
    with the MODE, INSN, and basic block BB parameters.  */
 
@@ -437,7 +613,6 @@ optimize_mode_switching (void)
   basic_block bb;
   int need_commit = 0;
   sbitmap *kill;
-  struct edge_list *edge_list;
   static const int num_modes[] = NUM_MODES_FOR_MODE_SWITCHING;
 #define N_ENTITIES ARRAY_SIZE (num_modes)
   int entity_map[N_ENTITIES];
@@ -447,6 +622,8 @@ optimize_mode_switching (void)
   int max_num_modes = 0;
   bool emited ATTRIBUTE_UNUSED = false;
   basic_block post_entry ATTRIBUTE_UNUSED, pre_exit ATTRIBUTE_UNUSED;
+  sbitmap *avin, *avout;
+  struct edge_list *edge_list = 0;
 
   for (e = N_ENTITIES - 1, n_entities = 0; e >= 0; e--)
     if (OPTIMIZE_MODE_SWITCHING (e))
@@ -483,6 +660,8 @@ optimize_mode_switching (void)
   antic = sbitmap_vector_alloc (last_basic_block, n_entities);
   transp = sbitmap_vector_alloc (last_basic_block, n_entities);
   comp = sbitmap_vector_alloc (last_basic_block, n_entities);
+  avin = sbitmap_vector_alloc (last_basic_block, n_entities);
+  avout = sbitmap_vector_alloc (last_basic_block, n_entities);
 
   sbitmap_vector_ones (transp, last_basic_block);
 
@@ -586,6 +765,9 @@ optimize_mode_switching (void)
     }
 
   kill = sbitmap_vector_alloc (last_basic_block, n_entities);
+
+  init_modes_infos (n_entities);
+
   for (i = 0; i < max_num_modes; i++)
     {
       int current_mode[N_ENTITIES];
@@ -615,9 +797,12 @@ optimize_mode_switching (void)
 
       FOR_EACH_BB (bb)
 	sbitmap_not (kill[bb->index], transp[bb->index]);
-      edge_list = pre_edge_lcm (n_entities, transp, comp, antic,
-				kill, &insert, &del);
+      edge_list = pre_edge_lcm_avs (n_entities, transp, comp, antic,
+				    kill, avin, avout, &insert, &del);
 
+      /* Merge modes for all entities.  */
+      set_flip_status (avin, avout);
+
       for (j = n_entities - 1; j >= 0; j--)
 	{
 	  /* Insert all mode sets that have been inserted by lcm.  */
@@ -633,10 +818,6 @@ optimize_mode_switching (void)
 	  for (e = NUM_EDGES (edge_list) - 1; e >= 0; e--)
 	    {
 	      edge eg = INDEX_EDGE (edge_list, e);
-	      int mode;
-	      basic_block src_bb;
-	      HARD_REG_SET live_at_edge;
-	      rtx mode_set;
 
 	      eg->aux = 0;
 
@@ -645,25 +826,8 @@ optimize_mode_switching (void)
 
 	      eg->aux = (void *)1;
 
-	      mode = current_mode[j];
-	      src_bb = eg->src;
-
-	      REG_SET_TO_HARD_REG_SET (live_at_edge, df_get_live_out (src_bb));
-
-	      start_sequence ();
-	      EMIT_MODE_SET (entity_map[j], mode, live_at_edge);
-	      mode_set = get_insns ();
-	      end_sequence ();
-
-	      /* Do not bother to insert empty sequence.  */
-	      if (mode_set == NULL_RTX)
-		continue;
-
-	      /* We should not get an abnormal edge here.  */
-	      gcc_assert (! (eg->flags & EDGE_ABNORMAL));
-
-	      need_commit = 1;
-	      insert_insn_on_edge (mode_set, eg);
+	      /* Remember we need to emit it.  */
+	      add_mode_set(j, e, current_mode[j]);
 	    }
 
 	  FOR_EACH_BB_REVERSE (bb)
@@ -678,6 +842,9 @@ optimize_mode_switching (void)
       sbitmap_vector_free (del);
       sbitmap_vector_free (insert);
       clear_aux_for_edges ();
+
+      /* Keep an edge_list for later.  */
+      if (i != max_num_modes - 1)
       free_edge_list (edge_list);
     }
 
@@ -686,9 +853,16 @@ optimize_mode_switching (void)
     {
       int no_mode = num_modes[entity_map[j]];
 
+      /* In case there was no mode inserted. the mode information on the edge
+	 might not be complete.
+	 Update mode info on edges and commit pending mode sets.  */
+      need_commit |= commit_mode_sets (edge_list, j);
+
       FOR_EACH_BB_REVERSE (bb)
 	{
 	  struct seginfo *ptr, *next;
+	  int last_mode = test_flip_status (j, bb, false);
+
 	  for (ptr = bb_info[j][bb->index].seginfo; ptr; ptr = next)
 	    {
 	      next = ptr->next;
@@ -697,10 +871,14 @@ optimize_mode_switching (void)
 		  rtx mode_set;
 
 		  start_sequence ();
-		  EMIT_MODE_SET (entity_map[j], ptr->mode, ptr->regs_live);
+		  EMIT_MODE_SET (entity_map[j], ptr->mode, last_mode,
+				 ptr->regs_live);
 		  mode_set = get_insns ();
 		  end_sequence ();
 
+		  /* modes are are localy set.  */
+		  last_mode = 1;
+
 		  /* Insert MODE_SET only if it is nonempty.  */
 		  if (mode_set != NULL_RTX)
 		    {
@@ -719,6 +897,9 @@ optimize_mode_switching (void)
       free (bb_info[j]);
     }
 
+  free_edge_list (edge_list);
+  free_modes_infos (n_entities);
+
   /* Finished. Free up all the things we've allocated.  */
   sbitmap_vector_free (kill);
   sbitmap_vector_free (antic);
--- gcc/gcc/tree-ssa-math-opts.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/tree-ssa-math-opts.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1408,7 +1408,8 @@ execute_cse_sincos (void)
 		CASE_FLT_FN (BUILT_IN_SIN):
 		CASE_FLT_FN (BUILT_IN_CEXPI):
 		  /* Make sure we have either sincos or cexp.  */
-		  if (!TARGET_HAS_SINCOS && !TARGET_C99_FUNCTIONS)
+		  if (!TARGET_HAS_SINCOS && !TARGET_C99_FUNCTIONS
+		      && flag_cse_sincos)
 		    break;
 
 		  arg = gimple_call_arg (stmt, 0);
--- gcc/gcc/opts.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/opts.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -459,6 +459,7 @@ static const struct default_options default_option
     { OPT_LEVELS_2_PLUS, OPT_fcrossjumping, NULL, 1 },
     { OPT_LEVELS_2_PLUS, OPT_foptimize_sibling_calls, NULL, 1 },
     { OPT_LEVELS_2_PLUS, OPT_fcse_follow_jumps, NULL, 1 },
+    { OPT_LEVELS_2_PLUS, OPT_fcse_sincos, NULL, 1 },
     { OPT_LEVELS_2_PLUS, OPT_fgcse, NULL, 1 },
     { OPT_LEVELS_2_PLUS, OPT_fexpensive_optimizations, NULL, 1 },
     { OPT_LEVELS_2_PLUS, OPT_frerun_cse_after_loop, NULL, 1 },
--- gcc/gcc/common/config/sh/sh-common.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/common/config/sh/sh-common.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -190,18 +190,14 @@ sh_handle_option (struct gcc_options *opts,
 static void
 sh_option_init_struct (struct gcc_options *opts)
 {
-  /* We can't meaningfully test TARGET_SH2E / TARGET_IEEE
-     here, so leave it to TARGET_OPTION_OVERRIDE to set
-     flag_finite_math_only.  We set it to 2 here so we know if the user
-     explicitly requested this to be on or off.  */
-  opts->x_flag_finite_math_only = 2;
 }
 
 /* Implement TARGET_OPTION_DEFAULT_PARAMS.  */
 static void
 sh_option_default_params (void)
 {
-  set_default_param_value (PARAM_SIMULTANEOUS_PREFETCHES, 2);
+   set_default_param_value (PARAM_SIMULTANEOUS_PREFETCHES,
+ 			   (TARGET_SH4_300 ? 6 : 2));
 }
 
 #undef TARGET_OPTION_OPTIMIZATION_TABLE
--- gcc/gcc/lcm.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/lcm.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,6 +1,7 @@
 /* Generic partial redundancy elimination with lazy code motion support.
    Copyright (C) 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2007, 2008,
    2010, 2011 Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -370,17 +371,18 @@ compute_insert_delete (struct edge_list *edge_list
     }
 }
 
-/* Given local properties TRANSP, ANTLOC, AVOUT, KILL return the insert and
-   delete vectors for edge based LCM.  Returns an edgelist which is used to
-   map the insert vector to what edge an expression should be inserted on.  */
+/* Given local properties TRANSP, ANTLOC, AVLOC, KILL return the insert and
+   delete vectors for edge based LCM, and return the AVIN, AVOUT bitmap.
+   Returns an edgelist which is used to map the insert vector to
+   what edge an expression should be inserted on.  */
 
 struct edge_list *
-pre_edge_lcm (int n_exprs, sbitmap *transp,
+pre_edge_lcm_avs (int n_exprs, sbitmap *transp,
 	      sbitmap *avloc, sbitmap *antloc, sbitmap *kill,
+		  sbitmap *avin, sbitmap *avout,
 	      sbitmap **insert, sbitmap **del)
 {
   sbitmap *antin, *antout, *earliest;
-  sbitmap *avin, *avout;
   sbitmap *later, *laterin;
   struct edge_list *edge_list;
   int num_edges;
@@ -402,10 +404,7 @@ struct edge_list *
 #endif
 
   /* Compute global availability.  */
-  avin = sbitmap_vector_alloc (last_basic_block, n_exprs);
-  avout = sbitmap_vector_alloc (last_basic_block, n_exprs);
   compute_available (avloc, kill, avout, avin);
-  sbitmap_vector_free (avin);
 
   /* Compute global anticipatability.  */
   antin = sbitmap_vector_alloc (last_basic_block, n_exprs);
@@ -431,7 +430,6 @@ struct edge_list *
 
   sbitmap_vector_free (antout);
   sbitmap_vector_free (antin);
-  sbitmap_vector_free (avout);
 
   later = sbitmap_vector_alloc (num_edges, n_exprs);
 
@@ -470,6 +468,28 @@ struct edge_list *
   return edge_list;
 }
 
+/* Wrapper to allocate avin/avout and call pre_edge_lcm_avs.  */
+
+struct edge_list *
+pre_edge_lcm (int n_exprs, sbitmap *transp,
+	      sbitmap *avloc, sbitmap *antloc, sbitmap *kill,
+	      sbitmap **insert, sbitmap **del)
+{
+  struct edge_list *edge_list;
+  sbitmap *avin, *avout;
+
+  avin = sbitmap_vector_alloc (last_basic_block, n_exprs);
+  avout = sbitmap_vector_alloc (last_basic_block, n_exprs);
+
+  edge_list = pre_edge_lcm_avs (n_exprs, transp, avloc, antloc, kill,
+				 avin, avout, insert, del);
+
+  sbitmap_vector_free (avout);
+  sbitmap_vector_free (avin);
+
+  return edge_list;
+}
+
 /* Compute the AVIN and AVOUT vectors from the AVLOC and KILL vectors.
    Return the number of passes we performed to iterate to a solution.  */
 
--- gcc/gcc/c-decl.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/c-decl.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1844,7 +1844,10 @@ diagnose_mismatched_decls (tree newdecl, tree oldd
 	      || (DECL_INITIAL (newdecl)
 		  && !prototype_p (TREE_TYPE (newdecl)))))
 	{
-	  warning (OPT_Wshadow, "declaration of %q+D shadows "
+	  if (flag_generate_lto) 
+	    sorry ("non-ANSI declaration shadowing a built-in function");
+	  else
+	    warning (OPT_Wshadow, "declaration of %q+D shadows "
 		   "a built-in function", newdecl);
 	  /* Discard the old built-in function.  */
 	  return false;
--- gcc/gcc/configure.ac	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/configure.ac	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1424,7 +1424,7 @@ case ${enable_threads} in
     target_thread_file='single'
     ;;
   aix | dce | lynx | mipssde | posix | rtems | \
-  single | tpf | vxworks | win32)
+  single | tpf | vxworks | win32 | generic)
     target_thread_file=${enable_threads}
     ;;
   *)
@@ -1816,6 +1816,16 @@ then
 	CROSS="-DCROSS_DIRECTORY_STRUCTURE"
 	ALL=all.cross
 	SYSTEM_HEADER_DIR=$build_system_header_dir
+ 
+ 	# For builds with an in-tree newlib, then the headers are not
+ 	# copied to build_system_header_dir, so things like limits.h
+ 	# won't work unless we point at the real headers.
+ 	if test "$with_newlib" = yes \
+ 		&& (test -z "$with_headers" || test "$with_headers" = yes) \
+ 		&& test -d $srcdir/../newlib/libc/include; then
+ 	  SYSTEM_HEADER_DIR="\$(abs_srcdir)/../newlib/libc/include"
+ 	fi
+ 
 	case "$host","$target" in
 	# Darwin crosses can use the host system's libraries and headers,
 	# because of the fat library support.  Of course, it must be the
@@ -4619,6 +4629,8 @@ if test x$host != x$target || test "x$TARGET_SYSTE
   else
     target_header_dir="${with_sysroot}${native_system_header_dir}"
   fi
+elif test x$host != x$build && test "x$with_build_sysroot" != "x"; then
+  target_header_dir="${with_build_sysroot}${native_system_header_dir}"
 else
   target_header_dir=${native_system_header_dir}
 fi
--- gcc/gcc/cppdefault.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/cppdefault.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -67,4 +67,11 @@ extern const char *gcc_exec_prefix;
 /* Return true if the toolchain is relocated.  */
 bool cpp_relocated (void);
 
+extern const char cpp_STANDARD_EXEC_PREFIX[];
+extern const size_t cpp_STANDARD_EXEC_PREFIX_len;
+extern const char *gcc_exec_prefix;
+
+/* Return true if the toolchain is relocated.  */
+bool cpp_relocated (void);
+
 #endif /* ! GCC_CPPDEFAULT_H */
--- gcc/gcc/stor-layout.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/stor-layout.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1492,14 +1492,6 @@ finalize_record_size (record_layout_info rli)
   rli->offset_align = BITS_PER_UNIT;
   normalize_rli (rli);
 
-  /* Determine the desired alignment.  */
-#ifdef ROUND_TYPE_ALIGN
-  TYPE_ALIGN (rli->t) = ROUND_TYPE_ALIGN (rli->t, TYPE_ALIGN (rli->t),
-					  rli->record_align);
-#else
-  TYPE_ALIGN (rli->t) = MAX (TYPE_ALIGN (rli->t), rli->record_align);
-#endif
-
   /* Compute the size so far.  Be sure to allow for extra bits in the
      size in bytes.  We have guaranteed above that it will be no more
      than a single byte.  */
@@ -1509,6 +1501,17 @@ finalize_record_size (record_layout_info rli)
     unpadded_size_unit
       = size_binop (PLUS_EXPR, unpadded_size_unit, size_one_node);
 
+
+  /* Determine the desired alignment.  */
+#ifdef ROUND_TYPE_ALIGN
+  TYPE_SIZE (rli->t) = unpadded_size;
+  TYPE_ALIGN (rli->t) = ROUND_TYPE_ALIGN (rli->t, TYPE_ALIGN (rli->t),
+					  rli->record_align);
+
+#else
+  TYPE_ALIGN (rli->t) = MAX (TYPE_ALIGN (rli->t), rli->record_align);
+#endif
+
   /* Round the size up to be a multiple of the required alignment.  */
   TYPE_SIZE (rli->t) = round_up (unpadded_size, TYPE_ALIGN (rli->t));
   TYPE_SIZE_UNIT (rli->t)
--- gcc/gcc/profile.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/profile.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -698,8 +698,13 @@ compute_branch_probabilities (unsigned cfg_checksu
 
       if (bb->count < 0)
 	{
-	  error ("corrupted profile info: number of iterations for basic block %d thought to be %i",
-		 bb->index, (int)bb->count);
+	  if (warn_branch_probabilities_computation)
+	    warning (OPT_Wbranch_probabilities_computation, "corrupted profile "
+		     "info: number of iterations for basic block %d thought to "
+		     "be %i", bb->index, (int)bb->count);
+	  else
+	    error ("corrupted profile info: number of iterations for basic "
+		   "block %d thought to be %i", bb->index, (int)bb->count);
 	  bb->count = 0;
 	}
       FOR_EACH_EDGE (e, ei, bb->succs)
@@ -719,8 +724,14 @@ compute_branch_probabilities (unsigned cfg_checksu
 	    }
 	  if (e->count < 0 || e->count > bb->count)
 	    {
-	      error ("corrupted profile info: number of executions for edge %d-%d thought to be %i",
-		     e->src->index, e->dest->index,
+	      if (warn_branch_probabilities_computation)
+		warning (OPT_Wbranch_probabilities_computation, "corrupted "
+			 "profile info: number of executions for edge %d-%d "
+			 "thought to be %i", e->src->index, e->dest->index,
+			 (int)e->count);
+	      else 
+		error ("corrupted profile info: number of executions for edge "
+		       "%d-%d thought to be %i", e->src->index, e->dest->index,
 		     (int)e->count);
 	      e->count = bb->count / 2;
 	    }
--- gcc/gcc/c-typeck.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/c-typeck.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -2,6 +2,7 @@
    Copyright (C) 1987, 1988, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011
    Free Software Foundation, Inc.
+   Copyright (c) 2010 STMicroelectronics.
 
 This file is part of GCC.
 
@@ -1752,6 +1753,7 @@ decl_constant_value (tree decl)
       && !TREE_THIS_VOLATILE (decl)
       && TREE_READONLY (decl)
       && DECL_INITIAL (decl) != 0
+      && !DECL_WEAK (decl)
       && TREE_CODE (DECL_INITIAL (decl)) != ERROR_MARK
       /* This is invalid if initial value is not constant.
 	 If it has either a function call, a memory reference,
--- gcc/gcc/insn-addr.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/insn-addr.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -62,4 +62,6 @@ insn_addresses_new (rtx insn, int insn_addr)
 #define INSN_ADDRESSES_NEW(insn, addr)		\
   (insn_addresses_new (insn, addr))
 
+extern int print_address (int uid);
+
 #endif /* ! GCC_INSN_ADDR_H */
--- gcc/gcc/except.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/except.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -192,10 +192,10 @@ static int add_call_site (rtx, int, int);
 
 static void push_uleb128 (VEC (uchar, gc) **, unsigned int);
 static void push_sleb128 (VEC (uchar, gc) **, int);
-#ifndef HAVE_AS_LEB128
+
 static int dw2_size_of_call_site_table (int);
 static int sjlj_size_of_call_site_table (void);
-#endif
+
 static void dw2_output_call_site_table (int, int);
 static void sjlj_output_call_site_table (void);
 
@@ -1547,7 +1547,7 @@ remove_eh_handler (eh_region region)
    Only used by reload hackery; should not be used by new code.  */
 
 void
-for_each_eh_label (void (*callback) (rtx))
+for_each_eh_label (void (*callback) (rtx, void *), void *arg)
 {
   eh_landing_pad lp;
   int i;
@@ -1558,7 +1558,7 @@ void
 	{
 	  rtx lab = lp->landing_pad;
 	  if (lab && LABEL_P (lab))
-	    (*callback) (lab);
+	    (*callback) (lab, arg);
 	}
     }
 }
@@ -2614,7 +2614,6 @@ push_sleb128 (VEC (uchar, gc) **data_area, int val
 }
 
 
-#ifndef HAVE_AS_LEB128
 static int
 dw2_size_of_call_site_table (int section)
 {
@@ -2649,7 +2648,6 @@ sjlj_size_of_call_site_table (void)
 
   return size;
 }
-#endif
 
 static void
 dw2_output_call_site_table (int cs_format, int section)
@@ -2843,13 +2841,10 @@ static void
 output_one_function_exception_table (int section)
 {
   int tt_format, cs_format, lp_format, i;
-#ifdef HAVE_AS_LEB128
   char ttype_label[32];
   char cs_after_size_label[32];
   char cs_end_label[32];
-#else
   int call_site_len;
-#endif
   int have_tt_data;
   int tt_format_size = 0;
 
@@ -2864,11 +2859,11 @@ output_one_function_exception_table (int section)
   else
     {
       tt_format = ASM_PREFERRED_EH_DATA_FORMAT (/*code=*/0, /*global=*/1);
-#ifdef HAVE_AS_LEB128
-      ASM_GENERATE_INTERNAL_LABEL (ttype_label,
-				   section ? "LLSDATTC" : "LLSDATT",
-				   current_function_funcdef_no);
-#endif
+      if (TARGET_USES_LEB128)
+	ASM_GENERATE_INTERNAL_LABEL (ttype_label,
+				     section ? "LLSDATTC" : "LLSDATT",
+				     current_function_funcdef_no);
+
       tt_format_size = size_of_encoded_value (tt_format);
 
       assemble_align (tt_format_size * BITS_PER_UNIT);
@@ -2894,86 +2889,93 @@ output_one_function_exception_table (int section)
   dw2_asm_output_data (1, tt_format, "@TType format (%s)",
 		       eh_data_format_name (tt_format));
 
-#ifndef HAVE_AS_LEB128
-  if (targetm_common.except_unwind_info (&global_options) == UI_SJLJ)
-    call_site_len = sjlj_size_of_call_site_table ();
-  else
-    call_site_len = dw2_size_of_call_site_table (section);
-#endif
+  if (! TARGET_USES_LEB128)
+    {
+      if (targetm_common.except_unwind_info (&global_options) == UI_SJLJ)
+	call_site_len = sjlj_size_of_call_site_table ();
+      else
+	call_site_len = dw2_size_of_call_site_table (section);
+    }
 
   /* A pc-relative 4-byte displacement to the @TType data.  */
   if (have_tt_data)
     {
-#ifdef HAVE_AS_LEB128
-      char ttype_after_disp_label[32];
-      ASM_GENERATE_INTERNAL_LABEL (ttype_after_disp_label,
-				   section ? "LLSDATTDC" : "LLSDATTD",
-				   current_function_funcdef_no);
-      dw2_asm_output_delta_uleb128 (ttype_label, ttype_after_disp_label,
-				    "@TType base offset");
-      ASM_OUTPUT_LABEL (asm_out_file, ttype_after_disp_label);
-#else
-      /* Ug.  Alignment queers things.  */
-      unsigned int before_disp, after_disp, last_disp, disp;
+      if (TARGET_USES_LEB128)
+	{
+	  char ttype_after_disp_label[32];
+	  ASM_GENERATE_INTERNAL_LABEL (ttype_after_disp_label,
+				       section ? "LLSDATTDC" : "LLSDATTD",
+				       current_function_funcdef_no);
+	  dw2_asm_output_delta_uleb128 (ttype_label, ttype_after_disp_label,
+					"@TType base offset");
+	  ASM_OUTPUT_LABEL (asm_out_file, ttype_after_disp_label);
+	}
+      else
+	{
+	  /* Ug.  Alignment queers things.  */
+	  unsigned int before_disp, after_disp, last_disp, disp;
 
-      before_disp = 1 + 1;
-      after_disp = (1 + size_of_uleb128 (call_site_len)
-		    + call_site_len
-		    + VEC_length (uchar, crtl->eh.action_record_data)
-		    + (VEC_length (tree, cfun->eh->ttype_data)
-		       * tt_format_size));
+	  before_disp = 1 + 1;
+	  after_disp = (1 + size_of_uleb128 (call_site_len)
+			+ call_site_len
+			+ VEC_length (uchar, crtl->eh.action_record_data)
+			+ (VEC_length (tree, cfun->eh->ttype_data)
+			   * tt_format_size));
 
-      disp = after_disp;
-      do
-	{
-	  unsigned int disp_size, pad;
+	  disp = after_disp;
+	  do
+	    {
+	      unsigned int disp_size, pad;
 
-	  last_disp = disp;
-	  disp_size = size_of_uleb128 (disp);
-	  pad = before_disp + disp_size + after_disp;
-	  if (pad % tt_format_size)
-	    pad = tt_format_size - (pad % tt_format_size);
-	  else
-	    pad = 0;
-	  disp = after_disp + pad;
+	      last_disp = disp;
+	      disp_size = size_of_uleb128 (disp);
+	      pad = before_disp + disp_size + after_disp;
+	      if (pad % tt_format_size)
+		pad = tt_format_size - (pad % tt_format_size);
+	      else
+		pad = 0;
+	      disp = after_disp + pad;
+	    }
+	  while (disp != last_disp);
+
+	  dw2_asm_output_data_uleb128 (disp, "@TType base offset");
 	}
-      while (disp != last_disp);
-
-      dw2_asm_output_data_uleb128 (disp, "@TType base offset");
-#endif
     }
 
   /* Indicate the format of the call-site offsets.  */
-#ifdef HAVE_AS_LEB128
-  cs_format = DW_EH_PE_uleb128;
-#else
-  cs_format = DW_EH_PE_udata4;
-#endif
+ if (TARGET_USES_LEB128)
+   cs_format = DW_EH_PE_uleb128;
+ else
+   cs_format = DW_EH_PE_udata4;
+
   dw2_asm_output_data (1, cs_format, "call-site format (%s)",
 		       eh_data_format_name (cs_format));
 
-#ifdef HAVE_AS_LEB128
-  ASM_GENERATE_INTERNAL_LABEL (cs_after_size_label,
-			       section ? "LLSDACSBC" : "LLSDACSB",
-			       current_function_funcdef_no);
-  ASM_GENERATE_INTERNAL_LABEL (cs_end_label,
-			       section ? "LLSDACSEC" : "LLSDACSE",
-			       current_function_funcdef_no);
-  dw2_asm_output_delta_uleb128 (cs_end_label, cs_after_size_label,
-				"Call-site table length");
-  ASM_OUTPUT_LABEL (asm_out_file, cs_after_size_label);
-  if (targetm_common.except_unwind_info (&global_options) == UI_SJLJ)
-    sjlj_output_call_site_table ();
-  else
-    dw2_output_call_site_table (cs_format, section);
-  ASM_OUTPUT_LABEL (asm_out_file, cs_end_label);
-#else
-  dw2_asm_output_data_uleb128 (call_site_len, "Call-site table length");
-  if (targetm_common.except_unwind_info (&global_options) == UI_SJLJ)
-    sjlj_output_call_site_table ();
-  else
-    dw2_output_call_site_table (cs_format, section);
-#endif
+if (TARGET_USES_LEB128)
+    {
+      ASM_GENERATE_INTERNAL_LABEL (cs_after_size_label,
+				   section ? "LLSDACSBC" : "LLSDACSB",
+				   current_function_funcdef_no);
+      ASM_GENERATE_INTERNAL_LABEL (cs_end_label,
+				   section ? "LLSDACSEC" : "LLSDACSE",
+				   current_function_funcdef_no);
+      dw2_asm_output_delta_uleb128 (cs_end_label, cs_after_size_label,
+				    "Call-site table length");
+      ASM_OUTPUT_LABEL (asm_out_file, cs_after_size_label);
+      if (targetm_common.except_unwind_info (&global_options) == UI_SJLJ)
+	sjlj_output_call_site_table ();
+      else
+	dw2_output_call_site_table (cs_format, section);
+      ASM_OUTPUT_LABEL (asm_out_file, cs_end_label);
+    }
+ else
+   {
+     dw2_asm_output_data_uleb128 (call_site_len, "Call-site table length");
+     if (targetm_common.except_unwind_info (&global_options) == UI_SJLJ)
+       sjlj_output_call_site_table ();
+     else
+       dw2_output_call_site_table (cs_format, section);
+   }
 
   /* ??? Decode and interpret the data for flag_debug_asm.  */
   {
@@ -2992,10 +2994,8 @@ output_one_function_exception_table (int section)
       output_ttype (type, tt_format, tt_format_size);
     }
 
-#ifdef HAVE_AS_LEB128
-  if (have_tt_data)
+  if (TARGET_USES_LEB128 && have_tt_data)
       ASM_OUTPUT_LABEL (asm_out_file, ttype_label);
-#endif
 
   /* ??? Decode and interpret the data for flag_debug_asm.  */
   if (targetm.arm_eabi_unwinder)
--- gcc/gcc/except.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/except.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -231,7 +231,7 @@ struct GTY(()) eh_status
 
 /* Invokes CALLBACK for every exception handler label.  Only used by old
    loop hackery; should not be used by new code.  */
-extern void for_each_eh_label (void (*) (rtx));
+extern void for_each_eh_label (void (*) (rtx, void*), void*);
 
 extern void init_eh_for_function (void);
 
--- gcc/gcc/emit-rtl.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/emit-rtl.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -3,6 +3,7 @@
    1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,
    2010, 2011
    Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -4292,8 +4293,18 @@ emit_insn_after_1 (rtx first, rtx after, basic_blo
   PREV_INSN (first) = after;
   NEXT_INSN (last) = after_after;
   if (after_after)
+
+    {
     PREV_INSN (after_after) = last;
 
+      if (NONJUMP_INSN_P (after_after)
+	  && GET_CODE (PATTERN (after_after)) == SEQUENCE)
+	{
+	  rtx sequence = PATTERN (after_after);
+	  PREV_INSN (XVECEXP (sequence, 0, 0)) = last;
+	}
+    }
+
   if (after == get_last_insn())
     set_last_insn (last);
 
--- gcc/gcc/gimple-fold.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/gimple-fold.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -156,7 +156,7 @@ canonicalize_constructor_val (tree cval)
 tree
 get_symbol_constant_value (tree sym)
 {
-  if (const_value_known_p (sym))
+  if (const_value_known_p (sym) && !DECL_WEAK (sym))
     {
       tree val = DECL_INITIAL (sym);
       if (val)
--- gcc/gcc/ChangeLog.STM	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/ChangeLog.STM	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,1262 @@
+2013-02-13  Laurent Alfonsi  <laurent.alfonsi@st.com>
+
+	* tree.c (build_one_cst, build_zero_cst): Use boolean global nodes.
+
+2013-01-31  Christian Bruel  <christian.bruel@st.com>
+
+	* sh.c (sh_expand_prologue): Check args.pretend_args_size to push args.
+
+2013-01-31  Laurent Alfonsi  <laurent.alfonsi@st.com>
+
+	http://codex.cro.st.com/tracker/?func=detail&aid=197859
+	* config/sh/sh.c (sh_insn_length_adjustment): Fix adjustment size for
+	insn located inside the sequences.
+
+2013-01-21  Christian Bruel  <christian.bruel@st.com>
+
+	* sh.h (CASE_VECTOR_SHORTEN_MODE): Pessimize switch mode before
+	 constant pools.
+
+2012-11-23  Christian Bruel  <christian.bruel@st.com>
+
+	* toplev.c (compile_file): Emit __gnu_lto_v1 only if HAVE_LTO_PLUGIN.
+	* collect2.c (scan_prog_file): __gnu_lto_v1 not implemented check.
+
+2013-01-12  Christian Bruel  <christian.bruel@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=25892
+	* config/sh/sh.c (sh_expand_prologue): Postpone new_stack mem symbol.
+	(broken_move): Handle UNSPECV_SP_SWITCH_B.
+	* config/sh/sh.md (sp_switch_1): Use set (reg:SI SP_REG).
+
+2013-01-12  DJ Delorie  <dj@redhat.com>
+
+	* config/sh/sh.md (UNSPECV_SP_SWITCH_B): New.
+	(UNSPECV_SP_SWITCH_E): New.
+	(sp_switch_1): Change to an unspec.
+	(sp_switch_2): Change to an unspec.  Don't use post-inc when we
+	replace $r15.
+
+2013-01-10  Laurent Alfonsi  <laurent.alfonsi@st.com>
+
+	* c-family/c.opt (flag_short_double): Useful also for LTO phase.
+	* lto/lto-lang.c (lto_init): LTO initialized with flag_short_double.
+
+2012-12-21  Christian Bruel  <christian.bruel@st.com>
+
+       * configure.ac: Set target_header_dir with build-sysroot.
+       * configure: Regenerate.
+       * config.in: Regenerate.
+
+2012-11-18  Christian Bruel  <christian.bruel@st.com>
+
+        * lto-opts.c (append_to_collect_gcc_options): Check !*first_p.
+
+2012-12-05  Laurent Alfonsi  <laurent.alfonsi@st.com>
+
+	* config/sh/sh.md (*muladdsi3): add clobber register macl.
+
+2012-11-08  Laurent Alfonsi  <laurent.alfonsi@st.com>
+
+	http://codex.cro.st.com/tracker/?func=detail&aid=180343
+	* gcc.c (LINK_COMMAND_SPEC): Skip -lgcov in case of nostdlib.
+
+2012-11-05  Christian Bruel  <christian.bruel@st.com>
+
+	* tree-ssa-tail-merge.c (replace_block_by): Update bb->count.
+
+2012-10-29  Christian Bruel  <christian.bruel@st.com>
+
+	PR target/32163
+	* config/sh/sh.md (stack_protect_test_si): Check stack_protector_block.
+	(sym_label2reg): Revert use of gen_chk_guard_add.
+	(chk_guard_add): Delete.
+	(UNSPEC_CHKADD): Delete.
+	* config/sh/sh.c (stack_protector_block): New function.
+	* config/sh/sh-protos.h (stack_protector_block): Declare.
+
+2012-10-11  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh-mem.c (set_mem_size): Fix operand
+
+2012-08-29  Christian Bruel  <christian.bruel@st.com>
+
+	PR target/54546
+	* config/sh/sh-protos.h (sh_need_epilogue): Delete.
+	(sh_can_use_simple_return_p): Declare.
+	* config/sh/sh.c (sh_can_use_simple_return_p): Define.
+	(sh_need_epilogue, sh_need_epilogue_known): Delete.
+	(sh_output_function_epilogue): Remove sh_need_epilogue_known.
+	* config/sh/sh.md (simple_return, return): Define.
+	(epilogue): Use inline return rtl.
+	(sh_expand_epilogue): Cleanup parameters boolean type.
+	(any_return): New iterator.
+
+2012-08-29  Christian Bruel  <christian.bruel@st.com>
+
+	Backport from mainline
+	2012-06-11  Richard Henderson  <rth@redhat.com>
+	* dwarf2cfi.c (scan_trace): Handle annulled branch-taken delay slots.
+
+2012-08-29  Christian Bruel  <christian.bruel@st.com>
+
+	Backport from mainline
+	2012-04-10  Ulrich Weigand  <ulrich.weigand@linaro.org>
+ 	    Richard Sandiford  <rdsandiford@googlemail.com>
+	* fwprop.c (propagate_rtx): Also set PR_CAN_APPEAR for subregs.
+
+2012-08-09 Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/newlib.h: Add NO_IMPLICIT_EXTERN_C.
+
+2012-07-19  Christian Bruel  <christian.bruel@st.com>
+
+	PR target/54029
+	* config/sh/sh.c (gen_far_branch): Set JUMP_LABEL for return jumps.
+
+2012-06-22  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.md (*muladdsi3): New pattern.
+
+2012-06-22  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_reorg): Check for return rtx.
+
+2012-06-13  Christian Bruel  <christian.bruel@st.com>
+
+	PR target/53621
+	* config/sh/sh.c (sh_option_override): Don't force
+	 flag_omit_frame_pointer and maccumulate_outgoing_args.
+	* config/sh/sh.opt (maccumulate-outgoing-args): Init as Var.
+
+2012-06-11  Christian Bruel  <christian.bruel@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=19749
+	Backport from mainline
+	2012-03-27  Chung-Lin Tang  <cltang@codesourcery.com>
+
+	PR target/52667
+	* config/sh/sh.c (find_barrier): Add equality check of last_got
+	to avoid going above orig insn. Update comments.
+
+2012-06-06  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.md (arith_shiftsi): New pattern.
+
+2012-05-30  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.c (save_switch): Add user_p parameter.
+	(read_specs): Likewise.
+	(set_specs): Likewise.
+	(validate_switches): Likewise.
+	(validate_switches_from_spec): Likewise.
+	(validate_all_switches): Pass on user_p parameter.
+	(struct spec_list): Add user_p field.
+	(struct switchstr): Add known field.
+	(save_switch): Add known parameter.
+	(INIT_STATIC_SPEC): Initialize user_p;
+	(driver_unknown_option_callbac): Call save_switch if
+	 OPT_SPECIAL_unknown.
+	(driver_handle_option): Propagate OPT_specs.
+	(do_spec_1): Set validated only if known.
+	(check_live_switch): Likewise.
+	(validate_switches): Set validated if known or user_spec.
+	(main): Adjust error message.
+
+2012-05-21  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/superh.h (LIB_SPEC): Use board_link libs for LTO.
+
+2012-04-23  Christian Bruel  <christian.bruel@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=18466
+	* config/sh/sh.c (sh_asm_count): Handle .fill directive.
+	(sh_insn_length_adjustment): Fix far jump offset.
+	(output_far_jump): Likewise.
+
+2012-04-23  Christian Bruel  <christian.bruel@st.com>
+
+	* final.c (print_address): New debug function.
+	* insn-addr.h (print_address): Declare.
+
+2012-04-11 Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/gcc/opts.c (default_options_table): Add OPT_fcse_sincos.
+	* gcc/gcc/tree-ssa-math-opts.c (execute_cse_sincos): Test OPT_fcse_sincos.
+	* gcc/doc/invoke.tex (-fcse-sincos): Describe.
+
+2012-03-15  Laurent Alfonsi <laurent.alfonsi@st.com>
+	Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_reorg): Uses eh_label instead of eh_region.
+	(handler_uses_reg): Don't check defs after jump.
+	* except.c (for_each_eh_region): Deleted.
+	(for_each_eh_label): Add parameter.
+	* reload1.c (set_initial_label_offsets): Likewise.
+	 (set_initial_eh_label_offset): Likewise.
+
+2012-03-07  Christian Bruel  <christian.bruel@st.com>
+
+	http://gcc.gnu.org/bugzilla/show_bug.cgi?id=52283
+	* stmt.c (warn_if_unused_value): Skit NOP_EXPR
+	* convert.c (convert_to_integer): Don't force TREE_NO_WARNING.
+
+2012-02-10  Christian Bruel  <christian.bruel@st.com>
+
+	* reload1.c (calculate_elim_costs_all_insns): Allow PARALLEL patterns.
+
+2012-01-18  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.md (UNSPEC_STR): Fix.
+	(UNSPEC_ROT): Likewise.
+
+2012-01-17  Christian Bruel  <christian.bruel@st.com>
+
+ 	* config/sh/sh.md (UNSPEC_ATOMIC): Define.
+ 	* config/sh/sync.md (sync_old_nand_fetch<mode>): New function.
+	(sync_old_<fetchop_name>_fetch<mode>): Likewise.
+	(sync_old_fetch_nand<mode>): Likewise.
+	(sync_old_fetch_<fetchop_name><mode>): Likewise.
+	(sync_compare_and_swap<mode>): Likewise.
+	(atomic_fetch_<fetchop_name><mode>) Rename sync_fetch_<fetchop_name><mode>
+	(atomic_<fetchop_name>_fetch<mode>) Rename sync_<fetchop_name>_fetch<mode>
+	(atomic_nand_fetch<mode>) Rename sync_nand_fetch<mode>.
+	(atomic_fetch_nand<mode>) Rename sync_nand_fetch<mode>.
+	(atomic_compare_and_swap<mode>): Remove.
+
+2011-12-22  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (MOVE_BY_PIECES_P): Tuned.
+	(MOVE_BY_PIECES_P): Likewise.
+
+2011-11-22  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_hw_workaround): Handle .align in absolute offsets.
+
+2011-11-15  Christian Bruel  <christian.bruel@st.com>
+
+	Port from Stlinux Mon Feb 02 2009 Carl Shaw <carl.shaw@st.com>
+	* config/sh/lib1funcs.asm (_ic_invalidate_syscall): New function.
+	* config/sh/t-linux (LIB1ASMFUNCS_CACHE: Add _ic_invalidate_syscall.
+
+2011-11-15  Christian Bruel  <christian.bruel@st.com>
+
+	* final.c (shorten_branches): Shorten branches even in -O0.
+
+2011-11-15  Christian Bruel  <christian.bruel@st.com>
+
+	* varasm.c (default_unique_section): Fix order of partition strings.
+
+2011-11-14  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/linux.h (SUBTARGET_CPP_SPEC): Define __SH4_300__ if -m4-300.
+
+2011-10-26  Christian Bruel  <christian.bruel@st.com>
+
+        https://bugzilla.stlinux.com/show_bug.cgi?id=14812
+	* final.c (insn_current_reference_address): Use min length to pessimize
+	a forward branch address.
+
+2011-10-13  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_insn_length_adjustment): Use cur_length.
+
+2011-07-09  Christian Bruel  <christian.bruel@st.com>
+
+	* tree-inline.c (tree_inlinable_function_p): Use sorry instead of error.
+
+2011-08-05  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (LEGITIMATE_CONSTANT_P): replace by
+	 sh_legitimate_constant_p().
+	* config/sh/sh.c (sh_legitimate_constant_p): New function from
+	 LEGITIMATE_CONSTANT_P.
+	Don't allow fldi1/fldi0 in reload propagation.
+	(fldi_ok): Check TARGERT_FLDI.
+	* config/sh/sh-protos.h (sh_legitimate_constant_p): Add prototype.
+	* config/sh/sh.opt (-mfldi): New option.
+	* gcc/doc/invoke.tex (-mfldi): Describe.
+
+2011-07-19  Christian Bruel  <christian.bruel@st.com>
+
+	* cgraphunit.c (process_function_and_variable_attributes): don't warn
+	always_inline for static functions.
+
+2011-07-05  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl31163
+	* config/sh/sh.c (fldi_ok): New parameter secondary.
+	(sh_secondary_reload): Don't allow generation of fldi after reload.
+	* config/sh/sh.h (SECONDARY_INPUT_RELOAD_CLASS): Likewise.
+	* config/sh/constraints.md (fldi_ok): Allow fldi0 in constraints.
+	* config/sh/sh-protos.h (fldi_ok): Add secondary parameter.
+
+2011-05-25  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/embed-elf.h (TARGET_TRAPA): Define for __builtin_trap.
+	* config/sh/sh.md (trap): Define insn.
+
+2011-04-19  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/sh.h (TARGET_IEEE): Set to 1.
+
+2011-03-21  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/defaults.h (ASM_ALIGN_FUNCTION_LOG): Define.
+	* gcc/varasm.c (assemble_start_function): Use ASM_ALIGN_FUNCTION_LOG.
+	* gcc/config/sh/sh-protos.h (sh_align_function_log): Declare.
+	* gcc/config/sh/sh.c (sh_align_function_log): Define.
+	* gcc/config/sh/sh.h (sh_align_function_log): Define.
+	* gcc/defaults.h (ASM_ALIGN_FUNCTION_LOG): Define.
+
+2011-04-18  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/sh.md (divsi3): Emit barrier after jump.
+
+2011-03-18  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/sh.md (movsf_ie): Don't use R0 for float const reload.
+
+2011-03-07  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/sh.c (expand_block_move): Optimize block copies by chunks of 48 bytes.
+
+2011-03-07  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/sh.c (expand_sfunc_op): Fix REG_EQUAL note.
+	(sh_output_mi_thunk): Fix REG_DEAD note.
+
+2011-04-15  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/sh.c (sh_output_function_epilogue): Reset mdep_reorg_phase.
+	  (sh_output_mi_thunk): Add barrier.
+	* gcc/resource.c (incr_ticks_for_insn): Add assert.
+
+2011-04-15  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config.gcc (sh-*): Compile sh-mem.c for SH targets.
+	* gcc/config/sh/t-shc: Add sh-mem.o dependency.
+	* gcc/config/sh/sh-mem.c: New file.
+
+2011-03-30  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/reorg.c (insn_conflict_latency): New function.
+	(fill_simple_delay_slots): Use insn_conflict_latency.
+
+2011-03-29  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh4_expand_cmpstr): Define.
+	* config/sh/sh.md (cmpstrsi): Define.
+
+2011-02-14  Christian Bruel  <christian.bruel@st.com>
+
+        https://bugzilla.stlinux.com/show_bug.cgi?id=11193
+	* gcc/reload.c (find_reloads): Revert exclusion of alternatives
+	 containing small register_class.
+
+2011-01-24  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/read-rtl.c (print_rtx_ptr_loc): Don't emit #line for .md files
+
+2011-01-24  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/sh.md (movsf_ie): Adjust fp_mode for fldi instructions.
+
+2011-01-12  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/sh.c (handler_uses_reg): Fix landing_pad scan.
+
+2011-01-11  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/sh.c (movsf_ie): Fix fp_mode.
+
+2010-11-30  Christian Bruel  <christian.bruel@st.com>
+
+        https://bugzilla.stlinux.com/show_bug.cgi?id=10586
+	* gcc/config/sh/sh.c (find_barrier): Skip notes.
+	(sh_asm_count): Conservately count aligns before reorg.
+
+2010-11-08  Christian Bruel  <christian.bruel@st.com>
+
+        https://bugzilla.stlinux.com/show_bug.cgi?id=10391
+	* gcc/ira.c (update_equiv_regs): Don't propagate after blockage.
+
+2010-11-03  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh-protos.h (sh_expand_lround): Addto_nearest flag.
+	* gcc/config/sh/sh.c (sh_expand_lround): Use to_nearest flag.
+	* gcc/config/sh/sh.md (lroundsfsi2): Define.
+
+2010-10-13  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh-protos.h (sh_expand_lround): Declare.
+	* gcc/config/sh/sh.c (sh_expand_lround): Define.
+	* gcc/config/sh/sh.md (UNSPEC_BUILTIN_ROUND, lrintsfsi2): Define.
+
+2010-09-29  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (sh_compare_op0, sh_compare_op1): Delete.
+
+2010-09-23  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (TARGET_CPU_CPP_BUILTINS): Define __MOVD__.
+	(ROUND_TYPE_ALIGN, MOVE_BY_PIECES_P): Fix test.
+	* config/sh/sh.c (expand_block_move): Don't toggle sz bit.
+
+2010-09-20  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (dump_table): Fixed constants alignments for 
+	TARGET_ALIGN_DOUBLE.
+
+2010-09-02  Christian Bruel  <christian.bruel@st.com>
+
+	* final.c (remap_debug_filename): translate filename for CYGPATH.
+
+2010-08-12  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.opt (mdiv=call-pre1): New option.
+	* config/sh/sh.h (SH_DIV_CALL_PRE1): New sh_div_strategy.
+	* config/sh/sh.md (sdivsi3): Use TARGET_DIVIDE_CALL_PRE1.
+	 (ashlsi3_k): New pattern.
+	* doc/invoke.texi (mdiv=call-pre1): Document.
+
+2010-07-21  Christian Bruel  <christian.bruel@st.com>
+
+        https://bugzilla.stlinux.com/show_bug.cgi?id=9620
+        * config/sh/sh.c (find_barrier): Update alignement after barrier.
+
+2010-07-07  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.opt (mtas): New Target option.
+	* config/sh/sh.h (SUBTARGET_ASM_ISA_SPEC): Pass --tas to assembler
+	(TARGET_CPU_CPP_BUILTINS): Define __HAVE_TAS__.
+	* doc/invoke.texi (mtas,mno-tas): Documents.
+
+2010-07-05  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/sh.c (sh_dwarf_register_span): No span if TARGET_FMOVD.
+	(push_regs): Push FP_REGISTER first.
+	(pop_regs): Pop FP_REGISTER last.
+	(emit_fpu_flip): Switch size if TARGET_FMOVD.
+
+2010-07-02  Christian Bruel  <christian.bruel@st.com>
+
+	* doc/md.texi: Document SH constraints.
+
+2010-06-04  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/supervisor-atomic.asm: (sync_nand_and_fetch): Fix
+	* gcc/config/sh/linux-atomic.asm: (sync_nand_and_fetch): Fix
+	(__sync_bool_compare_and_swap_, __sync_val_compare_and_swap_).
+	 Reorganized.
+
+2010-06-04  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/gthr-generic.c: Change license to GPLv3.
+	* gcc/gthr-generic.h: Likewise
+	* gcc/config/sh/supervisor-atomic.asm: Likewise
+	* gcc/config/sh/ieee-754-df.S: Likewise
+	* gcc/config/sh/ieee-754-df.S: Likewise
+
+2010-06-03  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/crti.asm (_init, _fini): Remove underscore.
+	* config/sh/crt1.asm (_init, _fini): Likewise.
+
+2010-06-02  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/linux.h (TARGET_OS_CPP_BUILTINS): Make __GNUC_STM_RELEASE__.
+
+2010-04-29  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh-protos.h (sh_varying_insn_p): Declare.
+	* gcc/config/sh/sh.c (sh_varying_insn_p): Define.
+	(sh_insn_length_adjustment): Use.
+	* gcc/config/sh/sh.h (VARYING_INSN_P): Define.
+	* gcc/final.c (VARYING_INSN_P): Use.
+
+2010-04-20  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl30850:
+	* config/sh/ieee-754-df.S (nedf2f): Don't check Qbit for NaNs.
+	* config/sh/ieee-754-sf.S (nesf2f): Likewise.
+	* config/sh/sh.md (cmpunsf_i1, cmpundf_i1): Likewise. Clobber R2.
+
+2010-04-09  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/sched-deps.c (sched_analyze_1): Don't extend R0 lifetime.
+	* config/sh/sh.md (movsf_ie): fix clobber constraint.
+
+2010-03-27  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_insn_length_adjustment): Adjust instruction size
+	 for jump_compact.
+
+2010-03-26  Christian Bruel  <christian.bruel@st.com>
+
+        https://bugzilla.stlinux.com/show_bug.cgi?id=8634
+	Backport from trunk:
+	2009-04-22  Paolo Bonzini  <bonzini@gnu.org>
+
+	* config/sh/sh.c (shift_insns_rtx, shiftcosts, gen_shifty_op,
+	sh_dynamicalize_shift_p, shl_and_scr_length): Truncate
+	shift counts to avoid out-of-bounds array accesses.
+
+2010-02-10  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (find_barrier): Don't emit a CP inside the GP setting.
+
+2010-01-04  Christian Bruel  <christian.bruel@st.com>
+  
+        https://bugzilla.stlinux.com/show_bug.cgi?id=8178
+	* final.c (shorten_branches): Enable asm statements to vary.
+ 	* config/sh/sh.c (asm_size): Catch multiple .long asm statements.
+	(sh_insn_length_alignment): New function.
+	(sh_asm_count): force max addr if insn_current_address unknown.
+	* config/sh/sh-protos.h (sh_insn_length_alignment): Declare.
+	* config/sh/sh.h (INSN_LENGTH_ALIGNMENT): Call sh_insn_length_alignment.
+
+2010-01-26  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/t-sh: Remove duplicate gt-sh.h dependencies.
+
+2010-01-20  Christian Bruel  <christian.bruel@st.com>
+
+	Backport from trunk:
+        2009-05-19  Ben Elliston  <bje@au.ibm.com>
+
+	* unwind-dw2-fde.c (fde_unencoded_compare): Replace type punning
+	assignments with memcpy calls.
+	(add_fdes): Likewise.
+	(binary_search_unencoded_fdes): Likewise.
+	(linear_search_fdes): Eliminate type puns.
+
+2010-01-20  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (OVERRIDE_OPTIONS, OPTIMIZATION_OPTIONS): Don't force
+	flag_schedule_insns for pic.
+
+2010-01-20  Christian Bruel  <christian.bruel@st.com>
+
+	Backport from trunk:
+	2009-05-12  Kaz Kojima  <kkojima@gcc.gnu.org>
+
+	PR target/39561
+	* config/sh/sh.h (OPTIMIZATION_OPTIONS): Don't set
+	TARGET_EXPAND_CBRANCHDI4.
+	* config/sh/sh.md (cbranchdi4): Don't check TARGET_EXPAND_CBRANCHDI4.
+	* config/sh/sh.opt (mexpand-cbranchdi): Remove.
+	(cmpeqdi): Fix comment.
+
+2009-10-26  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (ACCUMULATE_OUTGOING_ARGS): Define.
+	(MASK_ADJUST_UNROLL): Remove.
+        (OVERRIDE_OPTION): Check and set.
+	* config/sh/sh.opt (maccumulate-outgoing-args): New Target option.
+	(madjust-unroll): Remove.
+	* doc/invoke.texi (maccumulate-outgoing-args, madjust-unroll): Likewise.
+	* gcc/config/sh/sh.c (rounded_frame_size): Alloc outgoing args.
+
+2009-11-03  Christian Bruel  <christian.bruel@st.com>
+  
+        https://bugzilla.stlinux.com/show_bug.cgi?id=7377
+ 	* config/sh/sh.c (sh_insn_length_adjustment): Adjust jumps labels with
+	alignment.
+
+2009-10-29  Yvan Roux  <yvan.roux@st.com>
+
+	* doc/invoke.texi (-Wbranch-probabilities-computation): Document.
+	* common.opt (-Wbranch-probabilities-computation): New warning option.
+	* profile.c (compute_branch_probabilities): Ignore inconsistent bb
+	and/or edge counts computation if -Wbranch-probabilities-computation is
+	given.
+
+2009-10-28  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (SUBTARGET_ASM_ISA_SPEC): --isa=sh4-up for generic SH4s.
+
+2009-10-26  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (SUBTARGET_ASM_ISA_SPEC): Fix specs for -m4-singles.
+	* config/sh/sh.opt (SH_ASM_SPEC): Add -mnotas option.
+	* doc/invoke.texi (-mnotas): Document.
+
+2009-10-22  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/lib1funcs.asm (udiv_qrnnd_16): Fix alignment.
+
+2009-10-21  Christian Bruel  <christian.bruel@st.com>
+	    Yvan Roux  <yvan.roux@st.com>
+
+	* config/sh/sh.c (sh_reorg): Fix relax after incoming edges.
+
+2009-10-16  Christian Bruel  <christian.bruel@st.com>
+
+	* cfg.c (dump_edge_info): Print locus.
+	* cfgexpand.c (expand_gimple_basic_block): Initialize goto_locus.
+
+2009-10-14  Antony King  <antony.king@st.com>
+
+	INSbl30528:
+	* gcc.c (do_spec_1): Add support for %M specs.
+	(getenv_spec_function): Allow multiple arguments.
+	* doc/invoke.texi: Likewise.
+
+2009-10-15  Christian Bruel  <christian.bruel@st.com>
+	    Yvan Roux  <yvan.roux@st.com>
+
+	* config/sh/sh.c (handler_uses_reg): New function.
+	(sh_reorg): Call for each region.
+	* except.c (struct eh_region): Move to except.h
+	(for_each_eh_region): Accepts parameter.
+	* except.h (struct eh_region): Move here.
+	* tree-cfg.c (for_each_eh_region): Accepts parameter.
+
+2009-10-12  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_output_mi_thunk): Mark temporary dead.
+
+2009-10-08  Christian Bruel  <christian.bruel@st.com>
+
+	http://gcc.gnu.org/bugzilla/show_bug.cgi?id=41639 
+	* builtin-types.def (BT_I[1,2,4,8,16): Set signed.
+	* config/sh/linux-atomic.asm (ATOMIC_TEST_AND_SET): Keep sign.
+	(ATOMIC_COMPARE_AND_SWAP): Likewise.
+	(ATOMIC_FETCH_AND_OP, ATOMIC_FETCH_AND_COMBOP): Likewise.
+
+2009-10-06  Antony King  <antony.king@st.com>
+
+	INSbl30052:
+	* config/sh/t-superh [LIB2FUNCS_EXTRA]: Add.
+	* config/sh/supervisor-atomic.asm: New file.
+
+2009-10-09  Christian Bruel  <christian.bruel@st.com>
+
+	* doc/invoke.texi (mdead-delay): Document.
+	* doc/gcc.info (mdead-delay): Document.
+
+2009-10-05  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_reorg): Call df_analyze for notes.
+	    
+2009-10-02  Christian Bruel  <christian.bruel@st.com>
+	    Yvan Roux  <yvan.roux@st.com>
+
+	* defaults.h (HOT_TEXT_SECTION_PREFIX,
+	 UNLIKELY_EXECUTED_TEXT_SECTION_PREFIX): New macros.
+	* dwarf2out.c (dwarf2out_init, dwarf2out_finish): unlikely_text_section
+	takes param.		  
+	* predict.c (choose_function_section): Remove.
+	* varasm.c (first_function_block_is_cold): Remove.
+	(initialize_cold_section_name): Handle named unlikely sections.
+	(unlikely_text_section): Takes tree parameter.
+	(unlikely_text_section_p): Remove.
+	(function_section): Handle cold sections.
+	* output.h (first_function_block_is_cold): Remove.
+	(unlikely_text_section_p): Likewise.
+	(unlikely_text_section): Takes tree parameter.
+	* config/i386/i386.c: first_function_block_is_cold renamed 
+	in_cold_section_p.
+
+2009-08-28  Jan Beulich  <jbeulich@novell.com>
+
+	* configure.ac: For in-tree ld, do a plain version check to
+	determine whether comdat groups are supported.
+	* configure: Regenerate.
+
+2009-09-24  Christian Bruel  <christian.bruel@st.com>
+
+	http://gcc.gnu.org/bugzilla/show_bug.cgi?id=41486
+	* config/sh/sh.h (flag_tree_cselim): Unset flag_tree_cselim.
+
+2009-09-24  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_set_return_address): Fix adjustment.
+	* config/sh/sh.h (flag_omit_frame_pointer): Set.
+	* config/sh/sh.md (RTX_FRAME_RELATED_P): Set.
+
+2009-09-24  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_optimization_options): Set flag_omit_frame_pointer.
+
+2009-09-03  Christian Bruel  <christian.bruel@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=7000
+	* config/sh/sh.md (movdf_i4): Fix length attribute.
+	* config/sh/sh.c (sh_jump_align): Rework.
+
+2008-05-06  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/28671
+	* tree-ssa-ccp.c (get_symbol_constant_value): Check DECL_WEAK.
+	* c-typeck.c (decl_constant_value): Likewise.
+
+2009-08-14  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/t-linux (LIB1ASMMFUNCS_DIVTABLE): Define.
+	* config/sh/t-sh (libgcc-4-200.a): Create.
+	(LIB1ASMMFUNCS_DIVTABLE): Use.
+	(LIB1ASMFUNCS): Remove _sdivsi3_i4 _udivsi3_i4 _div_table from built.
+	* config/sh/embed-elf.h (LIBGCC_SPEC): Fixed lgcc-X-4-200 specs.
+	* config/sh/lib1funcs.asm (ieee-754-X.S): Guard against L_div_table.
+
+2009-08-10  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh.c (OPTIMIZATION_OPTIONS): Unset fschedule_insns off.
+
+2009-08-07  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh.c (fixup_mova): Fix casesi_worker access.
+	(dump_table): Likewise.
+
+2009-07-30  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config.gcc (fix-proto): Set to no for SH.
+
+2009-07-27  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/t-linux (EXTRA_MULTILIB_PARTS): Set.
+	* config/sh/t-sh (unwind-dw2-Os-4-200.o): Remove.
+
+2009-07-27  Christian Bruel  <christian.bruel@st.com>
+
+        * config/sh/sh.h (OPTIMIZATION_OPTIONS): Set dead-delay if optimizing.
+	* config/sh/sh.opt (mdead-delay): don't force 0.
+
+2009-07-02  Christian Bruel  <christian.bruel@st.com>
+
+	* tree-sra.c (bitfield_overlaps_p): Fix array tree type check.
+
+2009-06-18  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (simultaneous-prefetches): Set for st40-300.
+
+2009-06-18  Christian Bruel  <christian.bruel@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=6459
+	* config/sh/sh.md (cbranchdi4_i): Don't define insn.
+
+2009-06-05  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh.c (expand_block_move): Improve 64 bit -mfmovd.
+
+2009-06-09  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (SUBTARGET_ASM_ISA_SPEC): Pass -isa=sh4-nofpu-up
+	 for m4-nofpu.
+
+2009-04-12  Christian Bruel  <christian.bruel@st.com>
+
+	* default.h (TARGET_USES_LEB128): New macro.
+	* config/sh/sh.h (TARGET_USES_LEB128): Redefine.
+	* dwarf2asm.c (TARGET_USES_LEB128): Use instead of HAVE_AS_LEB128.
+	* except.c: Likewise.
+	* doc/tm.texi (TARGET_USES_LEB128): Document.
+	* doc/tm.texi.in (TARGET_USES_LEB128): Document.
+	* doc/gccint.info (TARGET_USES_LEB128): Likewise.
+	
+2009-05-05  Antony King  <antony.king@st.com>
+	    Christian Bruel  <christian.bruel@st.com>
+
+        INSbl30131
+	* lib1funcs.asm: Change local label naming convention.
+	* lib1funcs-Os-4-200.asm: Idem.
+
+2009-04-09  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/t-sh (ic_invalidate_array_4a.o): Fix st40-300 isa build.
+       (ic_invalidate_4a): Idem.
+        * config/sh/embed-elf.h (LIBGCC_SPEC): Fix ic_invalidate*.
+	* config/sh/sh.h (TARGET_CPU_DEFAULT, SUBTARGET_ASM_ISA_SPEC): Idem.
+	(ASM_ISA_SPEC_DEFAULT): Idem.
+
+2009-04-09  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.md (load_gbr): Fix operand constraint.
+
+2009-03-31  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (broken_move): Fixed.
+
+2009-03-31  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (OVERRIDE_OPTIONS, OPTIMIZATION_OPTIONS): Set 
+	flag_emit_frame_pointer.
+ 
+2009-03-24  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/trap-handler.c (exit): Declare noreturn.
+	* config/sh/t-sh $(CFLAGS_FOR_TARGET): passed to trap-handler build.
+
+2009-03-12  Christian Bruel  <christian.bruel@st.com>
+
+	http://gcc.gnu.org/bugzilla/show_bug.cgi?id=39423
+	* fold-const.c (fold_plusminus_mult_expr): Move canonicalization of
+	 index+cst...
+	* expr.c (expand_expr_real_1): ... here.
+
+2009-03-10  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_expand_epilogue): Don't insert blockage.
+
+2009-03-09  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl21915
+	* config/sh/sh.h (SH_LINK_SPEC): Pass -shared on -pic.
+
+2009-03-09  Christian Bruel  <christian.bruel@st.com>
+
+	* configure.ac: Change BUGURL, PKGVERSION.
+	* configure: Regenerate.
+
+2009-03-03  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.md (cbranchsi4): Enable.
+
+2009-03-03  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (CASE_VECTOR_MODE): Fix offset size for hwbug.
+	* config/sh/sh.c (sh_insn_length_adjustment): Fix pools for hwbug.
+
+2009-02-16  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/superh.h (SUBTARGET_ASM_RELAX_SPEC): Remove.
+	* config/sh/sh.h (SUBTARGET_ASM_RELAX_SPEC): Likewise.
+	(subtarget_asm_relax_spec). Likewise.
+
+2009-02-05  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (asm_size): Handle alignments.
+	(sh_asm_count): Likewise.
+	(sh_hw_workaround): Redesigned.
+	* config/sh/sh.h (SH_LINK_SPEC): pass --db-page-bug to the linker.
+	(INSN_LENGTH_ALIGNMENT): Fix minimum alignment.
+	* config/sh/linux-atomic.h: DB_ST40300_BUG_WORKAROUND fixes.
+	* config/sh/lib1funcs.asm: Likewise.
+
+2009-01-24  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.md (ivsi_inv_hitable): Fix alternative.
+	(divsi_inv_qitable): Likewise.
+
+2009-01-24  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/constraints.md (R03): New constraint.
+
+2009-01-24  Christian Bruel  <christian.bruel@st.com>
+
+	* emit-rtl.c (emit_insn_after_1): Update SEQUENCE.
+
+2009-01-20  Christian Bruel  <christian.bruel@st.com>
+
+	* final.c (TARGET_ASM_COUNT): Use.
+	* config/sh/sh-protos.h (sh_asm_count): Declared.
+	* config/sh/sh.h (TARGET_ASM_COUNT): Declared.
+	* config/sh/sh.c (sh_asm_count): Defined.
+
+2009-01-19  Christian Bruel  <christian.bruel@st.com>
+
+	* final.c (asm_insn_count): Check for empty asm.
+
+2009-01-05  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl29600
+	* config/sh/sh.c (sh_dwarf_register_span): New function.
+	(TARGET_DWARF_REGISTER_SPAN): Defined.
+	* config/sh/sh-protos.h (sh_dwarf_register_span): Declared.
+
+2008-12-08  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_insn_length_adjustment): Optimize out delay slot.
+	* config/sh/sh.md (dup_db_insn): New unspec pattern.
+	* config/sh/sh.opt (mdead-delay): New option.
+	* final.c (realloc_insn_lengths): New function.
+	* output.h (realloc_insn_lengths): Declare.
+
+2008-12-08  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (OPVERRIDE_OPTIONS): Don't force function alignment.
+
+2008-11-28  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl29605
+	* config/sh/sh.opt (mfmovd): Document.
+
+2008-11-27  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (CAN_DEBUG_WITHOUT_FP): Defined.
+
+2008-11-14  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.opt (mspace): Removed.
+	* doc/invoke.texi (mspace): Removed.
+	* config/sh/sh.h: Use optimize_size for TARGET_SMALLCODE.
+	* config/sh/sh.c: Likewise.
+	* config/sh/t-sh (TARGET_LIBGCC2_CFLAGS): Defined.
+
+2008-10-30  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_reorg): Allow relaxation within simple loops.
+	
+2008-10-24  Christian Bruel  <christian.bruel@st.com>
+
+	https://bugzilla.stlinux.com/show_bug.cgi?id=4907
+	* config/sh/sh.md (casesi_worker_x): Add MEM indirect.
+	* config/sh/sh.c (sh_insn_length_adjustment): Handle casesi_worker.
+
+2008-10-12  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_forward_branch_p): Handle casesi_worker.
+
+2008-05-28  Antony King  <antony.king@st.com>
+
+	Fix INSbl27707:
+	* config/sh/superh.h (LIB_SPEC): Re-order libraries.
+
+2008-08-03  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/24528
+	* config/sh/sh.md (ashrsi2_16): make it a define_expand.
+	(ashrsi2_31): Likewise.
+
+2008-08-03  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (find_barrier): Update lengths for conditional branches.
+
+2008-07-09  Christian Bruel  <christian.bruel@st.com>
+
+	st40-300 hardware bug workaround	
+	* config/sh/linux.h: (SUBTARGET_LINK_SPEC): Options passed to the linker.
+	* config/sh/sh.h (TARGET_CPU_CPP_BUILTINS): Define DB_ST40300_BUG_WORKAROUND.
+	(OVERRIDE_OPTIONS): Set align_functions.
+	* config/sh/sh.c (sh_hw_workaround, sh_forward_branch_p): New function.
+	(sh_insn_length_adjustment): Add length parameter, 
+	adjust length for workaround.
+	* config/sh/sh_protos.h (sh_hw_workaround): Likewise.
+	(sh_insn_length_adjustment): Add length parameter.
+	* final.c (final_scan_insn): call FINAL_PRESCAN_INSN.
+	* config/sh/sh.md (db-page-bug): new option.
+	* config/sh/sh.opt (mdb-page-bug): New option.
+
+2008-07-02  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.md (consttable_end): set length.
+	* config/sh/sh.h (MD_CAN_REDIRECT_BRANCH): Disable.
+	* final.c (shorten_branches): Add assertion.
+	* config/sh/sh.c (sh_jump_align): Use get_attr_min_length.
+	(barrier_align): Likewise.
+	(find_barrier): Take into account alignments into size.
+	(sh_reorg): use init_insn_lengths instead of INSN_ADDRESSES_FREE.
+
+2008-07-02  Christian Bruel  <christian.bruel@st.com>
+
+	* final.c (get_attr_length_1): Call get_attr_length_1 with fallback_fn
+	 instead of get_attr_length.
+
+2008-06-22  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (sh_cfun_naked_p): New function.
+	(sh_handle_fndecl_attribute): Likewise.
+	(sh_attribute_table): Add "naked".
+	(sh_expand_prologue): Check sh_cfun_naked_p.
+	(sh_expand_prologue): Likewise.
+
+2008-06-22  Christian Bruel  <christian.bruel@st.com>
+
+	(expand_cbranchdi4): Shut up warning.
+
+2008-06-22  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/24993
+	* config/sh/elf.h (MAX_OFILE_ALIGNMENT): Define.
+
+2008-06-17  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.md (doloop_end): Disable when optimizing for size.
+
+2008-04-17  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/28594
+	* config/sh/sh.c (expand_cbranchdi4): Use original operands for
+	msw_skip comparison.
+
+2008-04-25  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/28502
+	* config/sh/sh.c (barrier_align): Skip notes.
+
+2007-12-12  Christian Bruel  <christian.bruel@st.com>
+
+	* store-layout.c (finalize_record_size): Fixed TYPE_ALIGN.
+	* sh.c (expand_block_move): Optimize 64 bits copies if -mfmovd.
+	* sh.h (MOVE_BY_PIECES_P): Handle -mfmov.
+	(ROUND_TYPE_ALIGN): Likewise.
+
+2007-10-23  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/t-sh (_addsub_sf, _mul_sf, _addsub_df,  _extendsfdf2,
+	 _truncdfsf2, _fixunssfsi, _fixsfsi, _floatunssisf, _floatsisf,
+	_fixdfsi _floatunssidf _floatsidf, _muldf3, _divsf3): Renamed.
+	* config/sh/ieee-754-df.S: Likewise.
+	* config/sh/ieee-754-sf.S: Likewise.
+
+2007-10-23  Yvan Roux  <yvan.roux@st.com>
+
+	* config/sh/lib1funcs-4-300.asm (le128_neg): Fixed.
+	
+2007-10-04  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.c (for_each_path): Check just_multi_suffix and multi_suffix.
+
+2007-10-04  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/t-linux (LIB1ASMFUNCS_CACHE): Cleaned up.
+
+2007-10-04  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (TARGET_HAVE_TLS): Removed.
+	* config/sh/linux.h (TARGET_HAVE_TLS): Defined.
+
+2007-10-03  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.md (cmpnedf_i1): Fix.
+
+2007-09-20  Yvan Roux  <yvan.roux@st.com>
+
+	* config/sh/t-sh: (LIB1ASMFUNCS) Add asm functions.
+	* config/sh/ieee-754-df.S: Fixed.
+	* config/sh/IEEE-754/m3/divsf3.S: Fixed.
+	* config/sh/IEEE-754/m3/divdf3.S: Fixed.
+	* config/sh/IEEE-754/m3/addsf3.S: Fixed.
+	* config/sh/IEEE-754/m3/adddf3.S: Fixed.
+	* config/sh/IEEE-754/m3/mulsf3.S: Fixed.
+	* config/sh/IEEE-754/m3muldf3.S: Fixed.
+
+2007-09-19  Christian Bruel  <christian.bruel@st.com>
+
+	INSbl/25896
+	* sh.md (movsf_y): New pattern.
+	(pop_fpul2_y): Likewise.
+
+2007-09-07  Christian Bruel  <christian.bruel@st.com>
+
+	* sh.h (SH_DBX_REGISTER_NUMBER): Added fpscr and fixed sr/gbr_regs.
+
+2007-08-16  Antony King  <antony.king@st.com>
+
+	* configure.ac: Relaxed check for .[su]leb128 support.
+	* configure: Regenerate.
+
+2007-08-14  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* configure.ac (SYSTEM_HEADER_DIR): Adjust for in-tree Newlib.
+	* configure: Regenerate.
+
+2007-07-16  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (MOVE_MAX_PIECES): Tuned for TARGET_SH1.
+
+2007-06-26  Christian Bruel  <christian.bruel@st.com>
+
+	* gthr-generic.h: Rename *p to *__p.
+
+2007-05-23  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.opt (align-small-blocks=): New Optimisation.
+	* doc/invoke.texi (align-small-blocks=): Likewise.
+	* config/sh/sh.c (sh_jump_align): Check sh_align_small_blocks.
+	(barrier_align): Check sh_align_small_blocks.
+
+2007-04-30  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh-protos.h (sh_jump_align): New Function.
+	* config/sh/sh.c (sh_jump_align): Likewise.
+	(barrier_align): compute alignment based on TARGET_CACHE32.
+	* config/sh/sh.h (JUMP_ALIGN): Define.
+
+2007-03-29  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/sh.h (OVERRIDE_OPTIONS): Set assembler_dialect for sh1.
+2007-03-28  Christian Bruel  <christian.bruel@st.com>
+	* doc/invoke.texi: Document -m4-300.
+
+2007-03-09  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (__nesf2): Renamed.
+	(__nedf2): Likewise.
+	* config/sh/ieee-754-df.S (__nesf2): Likewise.
+	* config/sh/ieee-754-sf.S (__nedf2): Likewise.
+	* config/sh/t-sh: Likewise.
+
+2007-01-31  Christian Bruel  <christian.bruel@st.com>
+
+	* basic-block.h (pre_edge_lcm_avs): Declare.
+	* config/i386/i386.h (EMIT_MODE_SET): Add FLIP parameter.
+	* doc/tm.texi (EMIT_MODE_SET): Idem.
+	* doc/tm.texi.in (EMIT_MODE_SET): Idem.
+	* config/sh/sh.h (EMIT_MODE_SET): Idem. Call emit_fpu_flip.
+        (CONDITIONAL_REGISTER_USAGE): Set global_regs[FPSCR_REG].
+	* config/sh/sh-protos.h	(emit_fpu_flip): Add proto.
+	* config/sh/sh.c (emit_fpu_flip): New function.
+	* config/sh/sh.md (toggle_pr): Defined for TARGET_SH4_300.
+	Defined if TARGET_FPU_SINGLE.
+	fpscr_toggle don't go in delay slot (temporary fix).
+	* lcm.c (pre_edge_lcm_avs): Renamed from pre_edge_lcm.
+	Call clear_aux_for_edges. Fix comments.
+	(pre_edge_lcm): New wrapper function to call pre_edge_lcm_avs.
+	(pre_edge_rev_lcm): Idem.
+	* mode-switching.c (init_modes_infos): New function.
+	(bb_has_complex_pred): New function.
+	(free_modes_infos): Idem.
+	(init_modes_infos): Idem
+	(add_mode_set): Idem.
+	(get_mode): Idem.
+	(commit_mode_sets): Idem.
+	(merge_modes): Idem.
+	(set_flip_status): Idem
+	(test_flip_status): Idem.
+	(optimize_mode_switching): Add support to maintain flip mode information.
+2007-01-29  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/IEEE-754/m3/adddf3.S: Fix inf mantissa.
+	* config/sh/IEEE-754/m3/addsf3.S: Likewise.
+	* config/sh/IEEE-754/m3/divsf3.S: Intialize xff000000 label.
+	* config/sh/sh.c (expand_sfunc_op): Use FIRST_FP_PARM_REG for
+	parameters.
+	* config/sh/sh.h (TARGET_OSFP): Disable.
+	* config/sh/sh.md (addsf3, subsf3, mulsf3): Use expand_sfunc_binopt
+	only when TARGET_OSFP.
+	(adddf3, subdf3, muldf3): Likewise.
+	(trunkdfsf2): Likewise.
+
+2007-01-22  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/t-sh (LIB1ASMFUNCS): Remove _add_sub_sf3, _mulsf3,
+	_hypotf, _muldf3, _add_sub_df3, _divsf3, _divdf3, _fixunssfsi,
+	_fixsfsi, _fixunsdfsi, _fixdfsi, _floatunssisf, _floatsisf,
+	_floatunssidf and _floatsidf.
+	(FPBIT, DPBIT, dp-bit.c, fp-bit.c): Re-instated.
+
+2007-01-12  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* config/sh/trap-handler.c: Call exit like old one used to.
+
+2006-09-02  J"orn Rennecke  <joern.rennecke@st.com>
+
+	config/sh/t-sh: ($(T)ic_invalidate_array_4-100.o): Add -I. .
+	($(T)ic_invalidate_array_4-200.o): Likewise.
+	($(T)ic_invalidate_array_4a.o): Likewise.
+
+2006-09-02  J"orn Rennecke  <joern.rennecke@st.com>
+
+	* sh.md (*movsicc_t_false, *movsicc_t_true): Add mode.
+
+2006-11-10  J"orn Rennecke  <joern.rennecke@st.com> 
+	    Aanchal Khanna   <aanchalk@noida.hcltech.com>
+	    Rakesh Kumar  <rakesh.kumar@noida.hcltech.com>
+
+	PR target/29845
+	* config/sh/sh-protos.h (sh_function_kind): New enumerator
+	SFUNC_FREQUENT.
+	(expand_sfunc_unop, expand_sfunc_binop): Declare.
+	* config/sh/lib1funcs.asm (ieee-754-sf.S, ieee-754-df.S): #include.
+	* config/sh/t-sh (LIB1ASMFUNCS): Add nesf2, _nedf2, _gtsf2t, _gtdf2t,
+	_gesf2f, _gedf2f, _extendsfdf2, , _truncdfsf2, _add_sub_sf3, _mulsf3,
+	_hypotf, _muldf3, _add_sub_df3, _divsf3, _divdf3, _fixunssfsi,
+	_fixsfsi, _fixunsdfsi, _fixdfsi, _floatunssisf, _floatsisf,
+	_floatunssidf and _floatsidf.
+	(FPBIT, DPBIT, dp-bit.c, fp-bit.c): Removed.
+	* config/sh/ieee-754-df.S, config/sh/ieee-754-sf.S: New files.
+	* config/sh/predicates.md (soft_fp_comparison_operand): New predicate.
+	(soft_fp_comparison_operator): Likewise.
+	* config/sh/sh.c (sh_soft_fp_cmp, expand_sfunc_op): New functions.
+	(expand_sfunc_unop, expand_sfunc_binop): Likewise.
+	(from_compare): Add support for software floating point.
+	(function_symbol): Always look up name.  Add SFUNC_FREQUENT case.
+	* config/sh/sh.h (TARGET_SH1_SOFTFP): New macro.
+	(TARGET_SH1_SOFTFP_MODE): Likewise.
+	* config/sh/sh-modes.def (CC_FP_NE, CC_FP_GT, CC_FP_UNLT): New modes.
+	* config/sh/lib1funcs.h (SLC, SLI, SLCMP, DMULU_SAVE): New macros.
+	(DMULUL, DMULUH, DMULU_RESTORE, SHLL4, SHLR4, SHLL6, SHLR6): Likewise.
+	(SHLL12, SHLR12, SHLR19, SHLL23, SHLR24, SHLR21, SHLL21): Likewise.
+	(SHLR11, SHLR22, SHLR23, SHLR20, SHLL20, SHLD_COUNT, SHLRN): Likewise.
+	(SHLLN, DYN_SHIFT): Likewise.
+	(SUPPORT_SH3_OSFP, SUPPORT_SH3E_OSFP): Likewise.
+	(SUPPORT_SH4_NOFPU_OSFP, SUPPORT_SH4_SINGLE_ONLY_OSFP): Likewise.
+	(TARGET_OSFP): Likewise.
+	(OPTIMIZATION_OPTIONS): Always enable TARGET_CBRANCHDI4 and
+	TARGET_EXPAND_CBRANCHDI4.
+	If flag_trapping_math is set, make it 2.
+	(OVERRIDE_OPTIONS): If flag_trapping_math is 2 and non-trapping
+	software floating point is used, clear flag_trapping_math.
+	For SH1, set TARGET_EXPAND_CBRANCHDI4
+	* config/sh/ieee-754-df.S, config/sh/ieee-754-sf.S: New files.
+	* config/sh/IEEE-754/m3/divsf3.S: Likewise.
+	* config/sh/IEEE-754/m3/divdf3.S: Likewise.
+	* config/sh/IEEE-754/m3/floatunssisf.S: Likewise.
+	* config/sh/IEEE-754/m3/floatunssidf.S: Likewise.
+	* config/sh/IEEE-754/m3/fixunsdfsi.S: Likewise.
+	* config/sh/IEEE-754/m3/divdf3-rt.S: Likewise.
+	* config/sh/IEEE-754/m3/addsf3.S: Likewise.
+	* config/sh/IEEE-754/m3/adddf3.S: Likewise.
+	* config/sh/IEEE-754/m3/mulsf3.S: Likewise.
+	* config/sh/IEEE-754/m3/muldf3.S: Likewise.
+	* config/sh/IEEE-754/m3/floatsisf.S: Likewise.
+	* config/sh/IEEE-754/m3/floatsidf.S: Likewise.
+	* config/sh/IEEE-754/m3/fixdfsi.S: Likewise.
+	* config/sh/IEEE-754/divdf3.S: Likewise.
+	* config/sh/IEEE-754/floatunssisf.S: Likewise.
+	* config/sh/IEEE-754/fixunsdfsi.S: Likewise.
+	* config/sh/IEEE-754/adddf3.S: Likewise.
+	* config/sh/IEEE-754/floatsisf.S: Likewise.
+	* config/sh/IEEE-754/muldf3.S: Likewise.
+	* config/sh/IEEE-754/fixdfsi.S: Likewise.
+	* config/sh/IEEE-754/divsf3.S: Likewise.
+	* config/sh/IEEE-754/fixunssfsi.S: Likewise.
+	* config/sh/IEEE-754/floatunssidf.S: Likewise.
+	* config/sh/IEEE-754/addsf3.S: Likewise.
+	* config/sh/IEEE-754/mulsf3.S: Likewise.
+	* config/sh/IEEE-754/floatsidf.S: Likewise.
+	* config/sh/IEEE-754/fixsfsi.S: Likewise.
+	* config/sh/sh.md (SF_NAN_MASK, DF_NAN_MASK, FR4_REG): New constants.
+	(fpcmp_i1, addsf3_i3, subsf3_i3): New patterns.
+	(mulsf3_i3, cmpnesf_i1, cmpgtsf_i1, cmpunltsf_i1): Likewise.
+	(cmpeqsf_i1_finite, cmplesf_i1_finite, cmpunsf_i1): Likewise.
+	(cmpuneqsf_i1, movcc_fp_ne, movcc_fp_gtmovcc_fp_unlt): Likewise.
+	(cmpltgtsf_t, cmporderedsf_t, cmpltgtsf_t_4): Likewise.
+	(cmporderedsf_t_4, abssc2, adddf3_i3_wrap, adddf3_i3): Likewise.
+	(muldf3_i3_wrap, muldf3_i3, cmpnedf_i1, cmpgtdf_i1): Likewise.
+	(cmpunltdf_i1, cmpeqdf_i1_finite, cmpundf_i1, cmpuneqdf_i1): Likewise.
+	(cmpltgtdf_t, cmpordereddf_t_4, extendsfdf2_i1): Likewise.
+	(extendsfdf2_i2e, extendsfdf2_i2e_r0, truncdfsf2_i2e): Likewise.
+	(extendsfdf2_i1_r0, truncdfsf2_i1): Likewise.
+	(cmpun_sdf, sunle, cmpuneq_sdf, bunle, bunlt): Likewise.
+
+2006-11-03  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* config/sh/crt1.asm (_superh_trap_handler): Remove function.
+	* config/sh/trap-handler.c: New file.
+	* config/sh/t-elf (EXTRA_MULTILIB_PARTS): Add trap-handler.o.
+	* config/sh/t-superh (EXTRA_MULTILIB_PARTS): Likewise.
+	* config/sh/t-sh: Add rule for trap-handler.o.
+	* config/sh/elf.h (STARTFILE_SPEC): Add trap-handler.o.
+	* config/sh/superh.h (STARTFILE_SPEC): Likewise.
+
+2006-04-11  J"orn Rennecke <joern.rennecke@st.com>
+
+	* gthr-generic.h: Update to match
+	http://gcc.gnu.org/ml/gcc-patches/2006-04/msg00237.html .
+	* gthr-generic.c, gthr-objc-generic.c: Likewise.
+	* Makefile.in configure.ac: Likewise.
+	* configure: Regenerate.
+
+2006-01-17  Antony King <anthony.king@st.com>
+            J"orn Rennecke <joern.rennecke@st.com>
+
+	* configure.ac: Recognize 'generic' value for threads.
+	Check for existance of a *.c and gthr-objc-*.c file for thread support.
+	* configure: Regenerate.
+	* gthr-generic.h: New file.
+	* gthr-generic.c: New file.
+	* gthr-objc-generic.c: New file.
--- gcc/gcc/lto/lto-lang.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/lto/lto-lang.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1133,7 +1133,7 @@ lto_init (void)
   linemap_add (line_table, LC_ENTER, 0, NULL, 0);
 
   /* Create the basic integer types.  */
-  build_common_tree_nodes (flag_signed_char, /*short_double=*/false);
+  build_common_tree_nodes (flag_signed_char, flag_short_double);
 
   /* The global tree for the main identifier is filled in by
      language-specific front-end initialization that is not run in the
--- gcc/gcc/common.opt	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/common.opt	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -539,6 +539,10 @@ Wfatal-errors
 Common Var(flag_fatal_errors)
 Exit on the first error occurred
 
+Wnon-finite-math
+Common Var(warn_non_finite_math) Warning
+Warn if explicit NaNs or infinities are used with -ffinite-math-only
+
 Wframe-larger-than=
 Common RejectNegative Joined UInteger
 -Wframe-larger-than=<number>	Warn if a function's stack frame requires more than <number> bytes
@@ -710,6 +714,10 @@ Driver Separate
 Z
 Driver
 
+Wbranch-probabilities-computation
+Common Var(warn_branch_probabilities_computation) Warning
+Warn instead of error in probabilities computation in -fbranch-probabilities
+
 aux-info
 Common Separate Var(aux_info_file_name)
 -aux-info <file>	Emit declaration information into <file>
@@ -1163,6 +1171,10 @@ Common Report Var(flag_gcse_las) Init(0) Optimizat
 Perform redundant load after store elimination in global common subexpression
 elimination
 
+fcse-sincos
+Common Report Var(flag_cse_sincos) Optimization
+Enable sincos merging
+
 fgcse-after-reload
 Common Report Var(flag_gcse_after_reload) Optimization
 Perform global common subexpression elimination after register allocation
--- gcc/gcc/varasm.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/varasm.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1645,14 +1645,14 @@ assemble_start_function (tree decl, const char *fn
      Note that we still need to align to DECL_ALIGN, as above,
      because ASM_OUTPUT_MAX_SKIP_ALIGN might not do any alignment at all.  */
   if (! DECL_USER_ALIGN (decl)
-      && align_functions_log > align
+      && ASM_ALIGN_FUNCTION_LOG (decl) > align
       && optimize_function_for_speed_p (cfun))
     {
 #ifdef ASM_OUTPUT_MAX_SKIP_ALIGN
       ASM_OUTPUT_MAX_SKIP_ALIGN (asm_out_file,
 				 align_functions_log, align_functions - 1);
 #else
-      ASM_OUTPUT_ALIGN (asm_out_file, align_functions_log);
+      ASM_OUTPUT_ALIGN (asm_out_file, ASM_ALIGN_FUNCTION_LOG (decl));
 #endif
     }
 
--- gcc/gcc/ira.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/ira.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -2763,6 +2763,17 @@ update_equiv_regs (void)
 	     only mark all destinations as having no known equivalence.  */
 	  if (set == 0)
 	    {
+	      if (GET_CODE (PATTERN (insn)) == UNSPEC_VOLATILE)
+		{
+		  int i;
+		  /* UNSPEC_VOLATILE is considered to use and clobber all hard 
+		     registers and all of memory.  This blocks insns from being
+		     combined across this point.  */
+		  for (i = FIRST_PSEUDO_REGISTER; i < VEC_length (reg_equivs_t, reg_equivs); i++)
+		    reg_equiv[i].replace = 0;
+		}
+
+
 	      note_stores (PATTERN (insn), no_equiv, NULL);
 	      continue;
 	    }
--- gcc/gcc/sched-deps.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/sched-deps.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -2422,6 +2422,20 @@ sched_analyze_1 (struct deps_desc *deps, rtx x, rt
       int regno = REGNO (dest);
       enum machine_mode mode = GET_MODE (dest);
 
+      /* Don't extend the lifetime of CLASS_LIKELY_SPILLED registers before RA
+	 since the clobbers due to reload are not yet computed.  */
+      if (!reload_completed && regno < FIRST_PSEUDO_REGISTER)
+	{
+	  int i = hard_regno_nregs[regno][mode];
+	  
+	  while (--i >= 0)
+	    if (targetm.class_likely_spilled_p (REGNO_REG_CLASS (regno + i)))
+	      {
+		flush_pending_lists (deps, insn, false, true);
+		break;
+	      }
+	}
+
       sched_analyze_reg (deps, regno, mode, code, insn);
 
 #ifdef STACK_REGS
--- gcc/gcc/dwarf2cfi.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/dwarf2cfi.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -2441,10 +2441,15 @@ scan_trace (dw_trace_info *trace)
 		  HOST_WIDE_INT restore_args_size;
 		  cfi_vec save_row_reg_save;
 
+                 /* If ELT is an instruction from target of an annulled
+                    branch, the effects are for the target only and so
+                    the args_size and CFA along the current path
+                    shouldn't change.  */
 		  add_cfi_insn = NULL;
 		  restore_args_size = cur_trace->end_true_args_size;
 		  cur_cfa = &cur_row->cfa;
-		  save_row_reg_save = VEC_copy (dw_cfi_ref, gc, cur_row->reg_save);
+		  save_row_reg_save
+		    = VEC_copy (dw_cfi_ref, gc, cur_row->reg_save);
 
 		  scan_insn_after (elt);
 
@@ -2457,8 +2462,20 @@ scan_trace (dw_trace_info *trace)
 		  cur_row->cfa = this_cfa;
 		  cur_row->reg_save = save_row_reg_save;
 		  cur_cfa = &this_cfa;
-		  continue;
 		}
+             else
+               {
+                 /* If ELT is a annulled branch-taken instruction (i.e.
+                    executed only when branch is not taken), the args_size
+                    and CFA should not change through the jump.  */
+                 create_trace_edges (control);
+
+                 /* Update and continue with the trace.  */
+                 add_cfi_insn = insn;
+                 scan_insn_after (elt);
+                 def_cfa_1 (&this_cfa);
+               }
+	      continue;
 	    }
 
 	  /* The insns in the delay slot should all be considered to happen
--- gcc/gcc/tree-inline.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/tree-inline.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -3841,9 +3841,9 @@ expand_call_inline (basic_block bb, gimple stmt, c
 	  /* PR 20090218-1_0.c. Body can be provided by another module. */
 	  && (reason != CIF_BODY_NOT_AVAILABLE || !flag_generate_lto))
 	{
-	  error ("inlining failed in call to always_inline %q+F: %s", fn,
-		 cgraph_inline_failed_string (reason));
-	  error ("called from here");
+	  warning (OPT_Winline, "inlining failed in call to %q+F: %s",
+		   fn, cgraph_inline_failed_string (reason));
+	  warning (OPT_Winline, "called from here");
 	}
       else if (warn_inline
 	       && DECL_DECLARED_INLINE_P (fn)
--- gcc/gcc/output.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/output.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -2,6 +2,7 @@
    final.c, and varasm.c.
    Copyright (C) 1987, 1991, 1994, 1997, 1998, 1999, 2000, 2001, 2002,
    2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011
+   Copyright (c) 2009  STMicroelectronics.
    Free Software Foundation, Inc.
 
 This file is part of GCC.
@@ -54,6 +55,9 @@ extern int get_attr_min_length (rtx);
    any branches of variable length if possible.  */
 extern void shorten_branches (rtx);
 
+/* Returns the actual insn length without adjustment.  */
+int get_insn_current_length (rtx insn);
+
 /* Output assembler code for the start of a function,
    and initialize some of the variables in this file
    for the new function.  The label for the function and associated
--- gcc/gcc/resource.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/resource.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1283,6 +1283,8 @@ incr_ticks_for_insn (rtx insn)
 {
   int b = find_basic_block (insn, MAX_DELAY_SLOT_LIVE_SEARCH);
 
+  gcc_assert (b < last_basic_block);
+
   if (b != -1)
     bb_ticks[b]++;
 }
--- gcc/gcc/config.gcc	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config.gcc	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1238,7 +1238,7 @@ i[34567]86-*-openbsd*)
 	gnu_ld=yes
 	;;
 i[34567]86-*-linux* | i[34567]86-*-kfreebsd*-gnu | i[34567]86-*-knetbsd*-gnu | i[34567]86-*-gnu* | i[34567]86-*-kopensolaris*-gnu)
-			# Intel 80386's running GNU/*
+			# Intel 80386\'s running GNU/*
 			# with ELF format using glibc 2
 	tm_file="${tm_file} i386/unix.h i386/att.h dbxelf.h elfos.h gnu-user.h glibc-stdint.h"
 	case ${target} in
@@ -2208,6 +2208,7 @@ sh-*-elf* | sh[12346l]*-*-elf* | \
   sh-*-linux* | sh[2346lbe]*-*-linux* | \
   sh-*-netbsdelf* | shl*-*-netbsdelf* | sh5-*-netbsd* | sh5l*-*-netbsd* | \
   sh64-*-netbsd* | sh64l*-*-netbsd*)
+	extra_objs="sh-mem.o"
 	tmake_file="${tmake_file} sh/t-sh sh/t-elf"
 	if test x${with_endian} = x; then
 		case ${target} in
@@ -3574,7 +3575,7 @@ case ${target} in
 
 	powerpc*-*-* | rs6000-*-*)
 		# FIXME: The PowerPC port uses the value set at compile time,
-		# although it's only cosmetic.
+		# although it\'s only cosmetic.
 		if test "x$with_cpu" != x
 		then
 			target_cpu_default2="\\\"$with_cpu\\\""
--- gcc/gcc/Makefile.in	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/Makefile.in	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -818,7 +818,7 @@ version     := $(BASEVER_c)
 # comma in the $(if ...) constructs is significant - do not remove it.
 BASEVER_s   := "\"$(BASEVER_c)\""
 DEVPHASE_s  := "\"$(if $(DEVPHASE_c), ($(DEVPHASE_c)))\""
-DATESTAMP_s := "\"$(if $(DEVPHASE_c), $(DATESTAMP_c))\""
+DATESTAMP_s := "\"$(if $(DATESTAMP_c), $(DATESTAMP_c))\""
 PKGVERSION_s:= "\"@PKGVERSION@\""
 BUGURL_s    := "\"@REPORT_BUGS_TO@\""
 
--- gcc/gcc/basic-block.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/basic-block.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,6 +1,7 @@
 /* Define control flow data structures for the CFG.
    Copyright (C) 1987, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004,
    2005, 2006, 2007, 2008, 2009, 2010 Free Software Foundation, Inc.
+   Copyright (c) 2009 STMicroelectronics.
 
 This file is part of GCC.
 
@@ -751,6 +752,9 @@ edge find_edge (basic_block, basic_block);
 extern struct edge_list *pre_edge_lcm (int, sbitmap *, sbitmap *,
 				       sbitmap *, sbitmap *, sbitmap **,
 				       sbitmap **);
+extern struct edge_list *pre_edge_lcm_avs (int, sbitmap *, sbitmap *,
+					   sbitmap *, sbitmap *, sbitmap *,
+					   sbitmap *, sbitmap **, sbitmap **);
 extern struct edge_list *pre_edge_rev_lcm (int, sbitmap *,
 					   sbitmap *, sbitmap *,
 					   sbitmap *, sbitmap **,
--- gcc/gcc/lto-opts.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/lto-opts.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -44,7 +44,7 @@ append_to_collect_gcc_options (struct obstack *ob,
 			       bool *first_p, const char *opt)
 {
   const char *p, *q = opt;
-  if (!first_p)
+  if (!*first_p)
     obstack_grow (ob, " ", 1);
   obstack_grow (ob, "'", 1);
   while ((p = strchr (q, '\'')))
--- gcc/gcc/config/i386/i386.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/i386/i386.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -2,6 +2,7 @@
    Copyright (C) 1988, 1992, 1994, 1995, 1996, 1997, 1998, 1999, 2000,
    2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011
    Free Software Foundation, Inc.
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -2157,7 +2158,7 @@ enum ix86_stack_slot
    is the set of hard registers live at the point where the insn(s)
    are to be inserted.  */
 
-#define EMIT_MODE_SET(ENTITY, MODE, HARD_REGS_LIVE) 			\
+#define EMIT_MODE_SET(ENTITY, MODE, FLIP, HARD_REGS_LIVE)		\
   ((MODE) != I387_CW_ANY && (MODE) != I387_CW_UNINITIALIZED		\
    ? emit_i387_cw_initialization (MODE), 0				\
    : 0)
--- gcc/gcc/config/sh/t-mlib-sh4-300	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/config/sh/t-mlib-sh4-300	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1 @@
+ML_sh4_300=m4-300/
--- gcc/gcc/config/sh/predicates.md	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/predicates.md	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -743,6 +743,33 @@
 (define_predicate "symbol_ref_operand"
   (match_code "symbol_ref"))
 
+(define_special_predicate "soft_fp_comparison_operand"
+  (match_code "subreg,reg")
+{
+  switch (GET_MODE (op))
+    {
+    default:
+      return 0;
+    case CC_FP_NEmode: case CC_FP_GTmode: case CC_FP_UNLTmode:
+      break;
+    }
+  return register_operand (op, mode);
+})
+
+(define_predicate "soft_fp_comparison_operator"
+  (match_code "eq, unle, ge")
+{
+  switch (GET_CODE (op))
+    {
+    default:
+      return 0;
+    case EQ:  mode = CC_FP_NEmode;    break;
+    case UNLE:        mode = CC_FP_GTmode;    break;
+    case GE:  mode = CC_FP_UNLTmode;  break;
+    }
+  return register_operand (XEXP (op, 0), mode);
+})
+
 ;; Same as target_reg_operand, except that label_refs and symbol_refs
 ;; are accepted before reload.
 
--- gcc/gcc/config/sh/sh-protos.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/sh-protos.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -4,6 +4,7 @@
    Free Software Foundation, Inc.
    Contributed by Steve Chamberlain (sac@cygnus.com).
    Improved by Jim Wilson (wilson@cygnus.com).
+   Copyright (c) 2011  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -25,8 +26,13 @@ along with GCC; see the file COPYING3.  If not see
 #define GCC_SH_PROTOS_H
 
 enum sh_function_kind {
-  /* A function with normal C ABI  */
+  /* A function with normal C ABI, or an SH1..SH4 sfunc that may resolved via
+     a PLT.  */
   FUNCTION_ORDINARY,
+  /* A function that is a bit large to put it in every calling dso, but that's
+     typically used often enough so that calling via GOT makes sense for
+     speed.  */
+  SFUNC_FREQUENT,
   /* A special function that guarantees that some otherwise call-clobbered
      registers are not clobbered.  These can't go through the SH5 resolver,
      because it only saves argument passing registers.  */
@@ -52,6 +58,7 @@ extern const char *output_movepcrel (rtx, rtx[], e
 extern const char *output_far_jump (rtx, rtx);
 
 extern rtx sfunc_uses_reg (rtx);
+extern int sh_jump_align (rtx);
 extern int barrier_align (rtx);
 extern int sh_loop_align (rtx);
 extern int fp_zero_operand (rtx);
@@ -66,6 +73,7 @@ extern void emit_sf_insn (rtx);
 extern void emit_df_insn (rtx);
 extern void output_pic_addr_const (FILE *, rtx);
 extern int expand_block_move (rtx *);
+extern int sh4_expand_cmpstr (rtx *);
 extern int prepare_move_operands (rtx[], enum machine_mode mode);
 extern enum rtx_code prepare_cbranch_operands (rtx *, enum machine_mode mode,
 					       enum rtx_code comparison);
@@ -114,13 +122,20 @@ extern void expand_sf_binop (rtx (*)(rtx, rtx, rtx
 extern void expand_df_unop (rtx (*)(rtx, rtx, rtx), rtx *);
 extern void expand_df_binop (rtx (*)(rtx, rtx, rtx, rtx), rtx *);
 extern void expand_fp_branch (rtx (*)(void), rtx (*)(void));
-extern int sh_insn_length_adjustment (rtx);
+extern void expand_sfunc_unop (enum machine_mode, rtx (*) (rtx, rtx),
+			       const char *, enum rtx_code code, rtx *);
+extern void expand_sfunc_binop (enum machine_mode, rtx (*) (rtx, rtx),
+				const char *, enum rtx_code code, rtx *);
+extern int sh_insn_length_adjustment (rtx, const int);
+extern int sh_insn_length_alignment (rtx);
 extern int sh_can_redirect_branch (rtx, rtx);
 extern void sh_expand_unop_v2sf (enum rtx_code, rtx, rtx);
 extern void sh_expand_binop_v2sf (enum rtx_code, rtx, rtx, rtx);
+extern void sh_expand_lround (rtx, rtx, bool);
 extern int sh_expand_t_scc (rtx *);
 extern rtx sh_gen_truncate (enum machine_mode, rtx, int);
 extern bool sh_vector_mode_supported_p (enum machine_mode);
+extern bool stack_protector_block (rtx, rtx);
 #endif /* RTX_CODE */
 
 extern const char *output_jump_label_table (void);
@@ -129,10 +144,9 @@ extern rtx get_fpscr_rtx (void);
 extern int sh_media_register_for_return (void);
 extern void sh_expand_prologue (void);
 extern void sh_expand_epilogue (bool);
-extern int sh_need_epilogue (void);
 extern void sh_set_return_address (rtx, rtx);
 extern int initial_elimination_offset (int, int);
-extern int fldi_ok (void);
+extern int fldi_ok (bool);
 extern int sh_hard_regno_rename_ok (unsigned int, unsigned int);
 extern int sh_cfun_interrupt_handler_p (void);
 extern int sh_cfun_resbank_handler_p (void);
@@ -147,6 +161,7 @@ extern int check_use_sfunc_addr (rtx, rtx);
 #ifdef HARD_CONST
 extern void fpscr_set_from_mem (int, HARD_REG_SET);
 #endif
+extern void emit_fpu_flip (void);
 
 extern void sh_pr_interrupt (struct cpp_reader *);
 extern void sh_pr_trapa (struct cpp_reader *);
@@ -157,6 +172,7 @@ extern rtx sh_get_pr_initial_val (void);
 extern int sh_pass_in_reg_p (CUMULATIVE_ARGS *, enum machine_mode, tree);
 extern void sh_init_cumulative_args (CUMULATIVE_ARGS *, tree, rtx, tree, signed int, enum machine_mode);
 extern rtx sh_dwarf_register_span (rtx);
+extern bool sh_varying_insn_p (rtx);
 
 extern rtx replace_n_hard_rtx (rtx, rtx *, int , int);
 extern int shmedia_cleanup_truncate (rtx *, void *);
@@ -168,4 +184,8 @@ extern int sh2a_get_function_vector_number (rtx);
 extern int sh2a_is_function_vector_call (rtx);
 extern void sh_fix_range (const char *);
 extern bool sh_hard_regno_mode_ok (unsigned int, enum machine_mode);
+
+extern int sh_asm_count (const char *, int *);
+extern int sh_align_function_log (tree);
+extern bool sh_can_use_simple_return_p (void);
 #endif /* ! GCC_SH_PROTOS_H */
--- gcc/gcc/config/sh/linux.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/linux.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -29,11 +29,13 @@ along with GCC; see the file COPYING3.  If not see
 #define SUBTARGET_CPP_SPEC "\
    %{posix:-D_POSIX_SOURCE} \
    %{pthread:-D_REENTRANT -D_PTHREADS} \
-"
+   %{m4-300*:-D__SH4_300__} "
 
 #define TARGET_OS_CPP_BUILTINS() \
   do						\
     {						\
+      extern const char version_string[];       \
+      builtin_define_with_value ("__GNUC_STM_RELEASE__", version_string, 1); \
       GNU_USER_TARGET_OS_CPP_BUILTINS();	\
     }						\
   while (0)
--- gcc/gcc/config/sh/sh.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/sh.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -4,6 +4,7 @@
    Free Software Foundation, Inc.
    Contributed by Steve Chamberlain (sac@cygnus.com).
    Improved by Jim Wilson (wilson@cygnus.com).
+   Copyright (c) 2011 STMicroelectronics.
 
 This file is part of GCC.
 
@@ -57,8 +58,8 @@ along with GCC; see the file COPYING3.  If not see
 #include "alloc-pool.h"
 #include "tm-constrs.h"
 #include "opts.h"
+#include "except.h"
 
-
 int code_for_indirect_jump_scratch = CODE_FOR_indirect_jump_scratch;
 
 #define MSW (TARGET_LITTLE_ENDIAN ? 1 : 0)
@@ -170,7 +171,6 @@ static bool shmedia_space_reserved_for_target_regi
 
 static void split_branches (rtx);
 static int branch_dest (rtx);
-static void force_into (rtx, rtx);
 static void print_slot (rtx);
 static rtx add_constant (rtx, enum machine_mode, rtx);
 static void dump_table (rtx, rtx);
@@ -191,11 +191,13 @@ static int calc_live_regs (HARD_REG_SET *);
 static HOST_WIDE_INT rounded_frame_size (int);
 static bool sh_frame_pointer_required (void);
 static rtx mark_constant_pool_use (rtx);
+static int sh_cfun_naked_p (void);
 static tree sh_handle_interrupt_handler_attribute (tree *, tree, tree, int, bool *);
 static tree sh_handle_resbank_handler_attribute (tree *, tree,
 						 tree, int, bool *);
 static tree sh2a_handle_function_vector_handler_attribute (tree *, tree,
 							   tree, int, bool *);
+static tree  sh_handle_fndecl_attribute (tree *, tree, tree, int, bool *);
 static tree sh_handle_sp_switch_attribute (tree *, tree, tree, int, bool *);
 static tree sh_handle_trap_exit_attribute (tree *, tree, tree, int, bool *);
 static tree sh_handle_renesas_attribute (tree *, tree, tree, int, bool *);
@@ -325,7 +327,11 @@ static const struct attribute_spec sh_attribute_ta
     sh_handle_resbank_handler_attribute, false },
   { "function_vector",   1, 1, true,  false, false,
     sh2a_handle_function_vector_handler_attribute, false },
+  /* don't generate function prologue/epilogue and `ret' command.  */
+  { "naked",             0, 0, true, false, false,
+    sh_handle_fndecl_attribute, false },
   { NULL,                0, 0, false, false, false, NULL, false }
+
 };
 
 /* Initialize the GCC target structure.  */
@@ -688,6 +694,8 @@ sh_option_override (void)
 	sh_div_strategy = SH_DIV_CALL_FP;
       else if (! strcmp (sh_div_str, "call-table") && TARGET_SH2)
 	sh_div_strategy = SH_DIV_CALL_TABLE;
+      else if (! strcmp (sh_div_str, "call-pre1"))	
+ 	sh_div_strategy = SH_DIV_CALL_PRE1;
       else
 	/* Pick one that makes most sense for the target in general.
 	   It is not much good to use different functions depending
@@ -748,31 +756,18 @@ sh_option_override (void)
 
   if (targetm.small_register_classes_for_mode_p (VOIDmode))		\
     {
-      /* Never run scheduling before reload, since that can
-	 break global alloc, and generates slower code anyway due
-	 to the pressure on R0.  */
-      /* Enable sched1 for SH4 if the user explicitly requests.
-	 When sched1 is enabled, the ready queue will be reordered by
-	 the target hooks if pressure is high.  We can not do this for
-	 PIC, SH3 and lower as they give spill failures for R0.  */
-      if (!TARGET_HARD_SH4 || flag_pic)
-        flag_schedule_insns = 0;
       /* ??? Current exception handling places basic block boundaries
 	 after call_insns.  It causes the high pressure on R0 and gives
 	 spill failures for R0 in reload.  See PR 22553 and the thread
 	 on gcc-patches
          <http://gcc.gnu.org/ml/gcc-patches/2005-10/msg00816.html>.  */
-      else if (flag_exceptions)
-	{
-	  if (flag_schedule_insns && global_options_set.x_flag_schedule_insns)
-	    warning (0, "ignoring -fschedule-insns because of exception handling bug");
-	  flag_schedule_insns = 0;
-	}
-      else if (flag_schedule_insns
-	       && !global_options_set.x_flag_schedule_insns)
+      if (flag_exceptions)
 	flag_schedule_insns = 0;
     }
 
+  if (TARGET_DBHWBUG)
+       align_functions = 32;
+
   /* Unwind info is not correct around the CFG unless either a frame
      pointer is present or M_A_O_A is set.  Fixing this requires rewriting
      unwind info generation to be aware of the CFG and propagating states
@@ -816,6 +811,8 @@ sh_option_override (void)
   else if (align_jumps < (TARGET_SHMEDIA ? 4 : 2))
     align_jumps = TARGET_SHMEDIA ? 4 : 2;
 
+  flag_tree_cselim = 0;
+
   /* Allocation boundary (in *bytes*) for the code of a function.
      SH1: 32 bit alignment is faster, because instructions are always
      fetched as a pair from a longword boundary.
@@ -823,21 +820,7 @@ sh_option_override (void)
   if (align_functions == 0)
     align_functions
       = optimize_size ? FUNCTION_BOUNDARY/8 : (1 << CACHE_LOG);
-  /* The linker relaxation code breaks when a function contains
-     alignments that are larger than that at the start of a
-     compilation unit.  */
-  if (TARGET_RELAX)
-    {
-      int min_align
-	= align_loops > align_jumps ? align_loops : align_jumps;
 
-      /* Also take possible .long constants / mova tables int account.	*/
-      if (min_align < 4)
-	min_align = 4;
-      if (align_functions < min_align)
-	align_functions = min_align;
-    }
-
   /* If the -mieee option was not explicitly set by the user, turn it on
      unless -ffinite-math-only was specified.  See also PR 33135.  */
   if (! global_options_set.x_TARGET_IEEE)
@@ -907,6 +890,12 @@ sh_print_operand_address (FILE *stream, rtx x)
     }
 }
 
+static int deleted_delay_slot_p (rtx insn)
+{
+  return (GET_CODE (PATTERN (insn)) == UNSPEC_VOLATILE
+	  && XINT (PATTERN (insn), 1) == UNSPECV_DB_INSN);
+}
+
 /* Print operand x (an rtx) in assembler syntax to file stream
    according to modifier code.
 
@@ -945,7 +934,8 @@ sh_print_operand (FILE *stream, rtx x, int code)
     case '.':
       if (final_sequence
 	  && ! INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0))
-	  && get_attr_length (XVECEXP (final_sequence, 0, 1)))
+	  && (get_attr_length (XVECEXP (final_sequence, 0, 1))
+	      || deleted_delay_slot_p (XVECEXP (final_sequence, 0, 1))))
 	fprintf (stream, ASSEMBLER_DIALECT ? "/s" : ".s");
       break;
     case ',':
@@ -1415,158 +1405,6 @@ sh_encode_section_info (tree decl, rtx rtl, int fi
     SYMBOL_REF_FLAGS (XEXP (rtl, 0)) |= SYMBOL_FLAG_FUNCVEC_FUNCTION;
 }
 
-/* Like force_operand, but guarantees that VALUE ends up in TARGET.  */
-static void
-force_into (rtx value, rtx target)
-{
-  value = force_operand (value, target);
-  if (! rtx_equal_p (value, target))
-    emit_insn (gen_move_insn (target, value));
-}
-
-/* Emit code to perform a block move.  Choose the best method.
-
-   OPERANDS[0] is the destination.
-   OPERANDS[1] is the source.
-   OPERANDS[2] is the size.
-   OPERANDS[3] is the alignment safe to use.  */
-
-int
-expand_block_move (rtx *operands)
-{
-  int align = INTVAL (operands[3]);
-  int constp = (CONST_INT_P (operands[2]));
-  int bytes = (constp ? INTVAL (operands[2]) : 0);
-
-  if (! constp)
-    return 0;
-
-  /* If we could use mov.l to move words and dest is word-aligned, we
-     can use movua.l for loads and still generate a relatively short
-     and efficient sequence.  */
-  if (TARGET_SH4A_ARCH && align < 4
-      && MEM_ALIGN (operands[0]) >= 32
-      && can_move_by_pieces (bytes, 32))
-    {
-      rtx dest = copy_rtx (operands[0]);
-      rtx src = copy_rtx (operands[1]);
-      /* We could use different pseudos for each copied word, but
-	 since movua can only load into r0, it's kind of
-	 pointless.  */
-      rtx temp = gen_reg_rtx (SImode);
-      rtx src_addr = copy_addr_to_reg (XEXP (src, 0));
-      int copied = 0;
-
-      while (copied + 4 <= bytes)
-	{
-	  rtx to = adjust_address (dest, SImode, copied);
-	  rtx from = adjust_automodify_address (src, BLKmode,
-						src_addr, copied);
-
-	  set_mem_size (from, 4);
-	  emit_insn (gen_movua (temp, from));
-	  emit_move_insn (src_addr, plus_constant (src_addr, 4));
-	  emit_move_insn (to, temp);
-	  copied += 4;
-	}
-
-      if (copied < bytes)
-	move_by_pieces (adjust_address (dest, BLKmode, copied),
-			adjust_automodify_address (src, BLKmode,
-						   src_addr, copied),
-			bytes - copied, align, 0);
-
-      return 1;
-    }
-
-  /* If it isn't a constant number of bytes, or if it doesn't have 4 byte
-     alignment, or if it isn't a multiple of 4 bytes, then fail.  */
-  if (align < 4 || (bytes % 4 != 0))
-    return 0;
-
-  if (TARGET_HARD_SH4)
-    {
-      if (bytes < 12)
-	return 0;
-      else if (bytes == 12)
-	{
-	  rtx func_addr_rtx = gen_reg_rtx (Pmode);
-	  rtx r4 = gen_rtx_REG (SImode, 4);
-	  rtx r5 = gen_rtx_REG (SImode, 5);
-
-	  function_symbol (func_addr_rtx, "__movmemSI12_i4", SFUNC_STATIC);
-	  force_into (XEXP (operands[0], 0), r4);
-	  force_into (XEXP (operands[1], 0), r5);
-	  emit_insn (gen_block_move_real_i4 (func_addr_rtx));
-	  return 1;
-	}
-      else if (! optimize_size)
-	{
-	  const char *entry_name;
-	  rtx func_addr_rtx = gen_reg_rtx (Pmode);
-	  int dwords;
-	  rtx r4 = gen_rtx_REG (SImode, 4);
-	  rtx r5 = gen_rtx_REG (SImode, 5);
-	  rtx r6 = gen_rtx_REG (SImode, 6);
-
-	  entry_name = (bytes & 4 ? "__movmem_i4_odd" : "__movmem_i4_even");
-	  function_symbol (func_addr_rtx, entry_name, SFUNC_STATIC);
-	  force_into (XEXP (operands[0], 0), r4);
-	  force_into (XEXP (operands[1], 0), r5);
-
-	  dwords = bytes >> 3;
-	  emit_insn (gen_move_insn (r6, GEN_INT (dwords - 1)));
-	  emit_insn (gen_block_lump_real_i4 (func_addr_rtx));
-	  return 1;
-	}
-      else
-	return 0;
-    }
-  if (bytes < 64)
-    {
-      char entry[30];
-      rtx func_addr_rtx = gen_reg_rtx (Pmode);
-      rtx r4 = gen_rtx_REG (SImode, 4);
-      rtx r5 = gen_rtx_REG (SImode, 5);
-
-      sprintf (entry, "__movmemSI%d", bytes);
-      function_symbol (func_addr_rtx, entry, SFUNC_STATIC);
-      force_into (XEXP (operands[0], 0), r4);
-      force_into (XEXP (operands[1], 0), r5);
-      emit_insn (gen_block_move_real (func_addr_rtx));
-      return 1;
-    }
-
-  /* This is the same number of bytes as a memcpy call, but to a different
-     less common function name, so this will occasionally use more space.  */
-  if (! optimize_size)
-    {
-      rtx func_addr_rtx = gen_reg_rtx (Pmode);
-      int final_switch, while_loop;
-      rtx r4 = gen_rtx_REG (SImode, 4);
-      rtx r5 = gen_rtx_REG (SImode, 5);
-      rtx r6 = gen_rtx_REG (SImode, 6);
-
-      function_symbol (func_addr_rtx, "__movmem", SFUNC_STATIC);
-      force_into (XEXP (operands[0], 0), r4);
-      force_into (XEXP (operands[1], 0), r5);
-
-      /* r6 controls the size of the move.  16 is decremented from it
-	 for each 64 bytes moved.  Then the negative bit left over is used
-	 as an index into a list of move instructions.  e.g., a 72 byte move
-	 would be set up with size(r6) = 14, for one iteration through the
-	 big while loop, and a switch of -2 for the last part.  */
-
-      final_switch = 16 - ((bytes / 4) % 16);
-      while_loop = ((bytes / 4) / 16 - 1) * 16;
-      emit_insn (gen_move_insn (r6, GEN_INT (while_loop + final_switch)));
-      emit_insn (gen_block_lump_real (func_addr_rtx));
-      return 1;
-    }
-
-  return 0;
-}
-
 /* Prepare operands for a move define_expand; specifically, one of the
    operands must be in a register.  */
 
@@ -2204,7 +2042,6 @@ sh_emit_compare_and_branch (rtx *operands, enum ma
 	    op1 = force_reg (mode, op1);
         }
     }
-
   if (GET_MODE_CLASS (mode) == MODE_FLOAT)
     {
       if (code == LT
@@ -2416,6 +2253,10 @@ print_slot (rtx insn)
   INSN_DELETED_P (XVECEXP (insn, 0, 1)) = 1;
 }
 
+/* min/max for a 16-bits relative jump */
+#define FAR_JUMP_MIN -32764
+#define FAR_JUMP_MAX 32776
+
 const char *
 output_far_jump (rtx insn, rtx op)
 {
@@ -2429,11 +2270,11 @@ output_far_jump (rtx insn, rtx op)
   this_jmp.lab = gen_label_rtx ();
 
   if (TARGET_SH2
-      && offset >= -32764
-      && offset - get_attr_length (insn) <= 32766)
+      && offset >= FAR_JUMP_MIN
+      && offset <= FAR_JUMP_MAX)
     {
       far = 0;
-      jump = "mov.w	%O0,%1; braf	%1";
+      jump = "mov.w	%O0,%1\n\tbraf	%1";
     }
   else
     {
@@ -2441,12 +2282,12 @@ output_far_jump (rtx insn, rtx op)
       if (flag_pic)
 	{
 	  if (TARGET_SH2)
-	    jump = "mov.l	%O0,%1; braf	%1";
+	    jump = "mov.l	%O0,%1\n\tbraf	%1";
 	  else
-	    jump = "mov.l	r0,@-r15; mova	%O0,r0; mov.l	@r0,%1; add	r0,%1; mov.l	@r15+,r0; jmp	@%1";
+	    jump = "mov.l	r0,@-r15\n\tmova	%O0,r0\n\t mov.l	@r0,%1\n\tadd	r0,%1\n\t mov.l	@r15+,r0\n\tjmp	@%1";
 	}
       else
-	jump = "mov.l	%O0,%1; jmp	@%1";
+	jump = "mov.l	%O0,%1\n\tjmp	@%1";
     }
   /* If we have a scratch register available, use it.  */
   if (NONJUMP_INSN_P ((prev = prev_nonnote_insn (insn)))
@@ -2454,7 +2295,7 @@ output_far_jump (rtx insn, rtx op)
     {
       this_jmp.reg = SET_DEST (XVECEXP (PATTERN (prev), 0, 0));
       if (REGNO (this_jmp.reg) == R0_REG && flag_pic && ! TARGET_SH2)
-	jump = "mov.l	r1,@-r15; mova	%O0,r0; mov.l	@r0,r1; add	r1,r0; mov.l	@r15+,r1; jmp	@%1";
+	jump = "mov.l	r1,@-r15\n\t mova	%O0,r0\n\t mov.l	@r0,r1\n\t add	r1,r0\n\t mov.l	@r15+,r1\n\tjmp	@%1";
       output_asm_insn (jump, &this_jmp.lab);
       if (dbr_sequence_length ())
 	print_slot (final_sequence);
@@ -2538,8 +2379,12 @@ output_branch (int logic, rtx insn, rtx *operands)
 	      && ! INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0))
 	      && get_attr_length (XVECEXP (final_sequence, 0, 1)))
 	    {
-	      asm_fprintf (asm_out_file, "\tb%s%ss\t%LLF%d\n", logic ? "f" : "t",
+	      asm_fprintf (asm_out_file, "\tb%s%ss\t%LLF%d", logic ? "f" : "t",
 	                   ASSEMBLER_DIALECT ? "/" : ".", label);
+
+	      asm_fprintf (asm_out_file, "\t%s %d\t[length = %d]\n",
+			   ASM_COMMENT_START, INSN_UID (insn),
+			   get_attr_length (insn));
 	      print_slot (final_sequence);
 	    }
 	  else
@@ -2643,6 +2488,12 @@ output_branchy_insn (enum rtx_code code, const cha
   return templ;
 }
 
+/* Output a code sequence for INSN using TEMPLATE with OPERANDS; but before,
+   fill in operands 9 as a label to the successor insn.
+   We try to use jump threading where possible.
+   IF CODE matches the comparison in the IF_THEN_ELSE of a following jump,
+   we assume the jump is taken.  I.e. EQ means follow jmp and bf, NE means
+   follow jmp and bt, if the address is in range.  */
 const char *
 output_ieee_ccmpeq (rtx insn, rtx *operands)
 {
@@ -2994,7 +2845,7 @@ sh_rtx_costs (rtx x, int code, int outer_code, int
 	       && CONST_OK_FOR_K08 (INTVAL (x)))
         *total = 1;
       /* prepare_cmp_insn will force costly constants int registers before
-	 the cbranch[sd]i4 patterns can see them, so preserve potentially
+	 the cbrach[sd]i4 pattterns can see them, so preserve potentially
 	 interesting ones not covered by I08 above.  */
       else if (outer_code == COMPARE
 	       && ((unsigned HOST_WIDE_INT) INTVAL (x)
@@ -4083,9 +3934,9 @@ dump_table (rtx start, rtx barrier)
   rtx scan = barrier;
   int i;
   int need_align = 1;
+  int need_align_d = 1;
   rtx lab;
   label_ref_list_t ref;
-  int have_df = 0;
 
   /* Do two passes, first time dump out the HI sized constants.  */
 
@@ -4110,11 +3961,10 @@ dump_table (rtx start, rtx barrier)
 	      scan = emit_insn_after (gen_consttable_window_end (lab), scan);
 	    }
 	}
-      else if (p->mode == DFmode)
-	have_df = 1;
     }
 
   need_align = 1;
+  need_align_d = 1;
 
   if (start)
     {
@@ -4125,85 +3975,15 @@ dump_table (rtx start, rtx barrier)
 	    && recog_memoized (start) == CODE_FOR_casesi_worker_2)
 	  {
 	    rtx src = SET_SRC (XVECEXP (PATTERN (start), 0, 0));
-	    rtx lab = XEXP (XVECEXP (src, 0, 3), 0);
+	    rtx lab;
 
-	    scan = emit_label_after (lab, scan);
-	  }
-    }
-  if (TARGET_FMOVD && TARGET_ALIGN_DOUBLE && have_df)
-    {
-      rtx align_insn = NULL_RTX;
-
-      scan = emit_label_after (gen_label_rtx (), scan);
-      scan = emit_insn_after (gen_align_log (GEN_INT (3)), scan);
-      need_align = 0;
-
-      for (i = 0; i < pool_size; i++)
-	{
-	  pool_node *p = &pool_vector[i];
-
-	  switch (p->mode)
-	    {
-	    case HImode:
-	      break;
-	    case SImode:
-	    case SFmode:
-	      if (align_insn && !p->part_of_sequence_p)
-		{
-		  for (lab = p->label; lab; lab = LABEL_REFS (lab))
-		    emit_label_before (lab, align_insn);
-		  emit_insn_before (gen_consttable_4 (p->value, const0_rtx),
-				    align_insn);
-		  for (ref = p->wend; ref; ref = ref->next)
-		    {
-		      lab = ref->label;
-		      emit_insn_before (gen_consttable_window_end (lab),
-					align_insn);
-		    }
-		  delete_insn (align_insn);
-		  align_insn = NULL_RTX;
-		  continue;
-		}
-	      else
-		{
-		  for (lab = p->label; lab; lab = LABEL_REFS (lab))
-		    scan = emit_label_after (lab, scan);
-		  scan = emit_insn_after (gen_consttable_4 (p->value,
-							    const0_rtx), scan);
-		  need_align = ! need_align;
-		}
-	      break;
-	    case DFmode:
-	      if (need_align)
-		{
-		  scan = emit_insn_after (gen_align_log (GEN_INT (3)), scan);
-		  align_insn = scan;
-		  need_align = 0;
-		}
-	    case DImode:
-	      for (lab = p->label; lab; lab = LABEL_REFS (lab))
+	    gcc_assert (MEM_P (src));
+	    src = XEXP (src, 0);
+	    lab = XEXP (XVECEXP (src, 0, 3), 0);
 		scan = emit_label_after (lab, scan);
-	      scan = emit_insn_after (gen_consttable_8 (p->value, const0_rtx),
-				      scan);
-	      break;
-	    default:
-	      gcc_unreachable ();
 	    }
-
-	  if (p->mode != HImode)
-	    {
-	      for (ref = p->wend; ref; ref = ref->next)
-		{
-		  lab = ref->label;
-		  scan = emit_insn_after (gen_consttable_window_end (lab),
-					  scan);
-		}
-	    }
 	}
 
-      pool_size = 0;
-    }
-
   for (i = 0; i < pool_size; i++)
     {
       pool_node *p = &pool_vector[i];
@@ -4224,8 +4004,17 @@ dump_table (rtx start, rtx barrier)
 	    scan = emit_label_after (lab, scan);
 	  scan = emit_insn_after (gen_consttable_4 (p->value, const0_rtx),
 				  scan);
+
+	  need_align_d = 1;
 	  break;
+
 	case DFmode:
+	  if (TARGET_ALIGN_DOUBLE && need_align_d)
+	    {
+	      need_align = 0;
+	      need_align_d = 0;
+	      scan = emit_insn_after (gen_align_log (GEN_INT (3)), scan);
+	    }
 	case DImode:
 	  if (need_align)
 	    {
@@ -4291,6 +4080,8 @@ broken_move (rtx insn)
 	     order bits end up as.  */
 	  && GET_MODE (SET_DEST (pat)) != QImode
 	  && (CONSTANT_P (SET_SRC (pat))
+	      || (GET_CODE (SET_SRC (pat)) == UNSPEC_VOLATILE
+		  && XINT (SET_SRC (pat), 1) == UNSPECV_SP_SWITCH_B)
 	      /* Match mova_const.  */
 	      || (GET_CODE (SET_SRC (pat)) == UNSPEC
 		  && XINT (SET_SRC (pat), 1) == UNSPEC_MOVA
@@ -4360,6 +4151,10 @@ fixup_mova (rtx mova)
       wpat0 = XVECEXP (wpat, 0, 0);
       wpat1 = XVECEXP (wpat, 0, 1);
       wsrc = SET_SRC (wpat0);
+
+      gcc_assert (MEM_P (wsrc));
+      wsrc = XEXP (wsrc, 0);
+
       PATTERN (worker) = (gen_casesi_worker_2
 			  (SET_DEST (wpat0), XVECEXP (wsrc, 0, 1),
 			   XEXP (XVECEXP (wsrc, 0, 2), 0), lab,
@@ -4465,11 +4260,23 @@ find_barrier (int num_mova, rtx mova, rtx from)
   si_limit = 1018;
   hi_limit = 510;
 
+  if (TARGET_DBHWBUG)
+    {
+      hi_limit -= 2;
+      si_limit -= 2;
+    }
+
   while (from && count_si < si_limit && count_hi < hi_limit)
     {
       int inc = get_attr_length (from);
       int new_align = 1;
 
+      if (NOTE_P (from))
+	{
+	  from = NEXT_INSN (from);
+	  continue;
+	}
+
       /* If this is a label that existed at the time of the compute_alignments
 	 call, determine the alignment.  N.B.  When find_barrier recurses for
 	 an out-of-reach mova, we might see labels at the start of previously
@@ -4505,14 +4312,17 @@ find_barrier (int num_mova, rtx mova, rtx from)
       if (BARRIER_P (from))
 	{
 	  rtx next;
-
+	  int bar_align = barrier_align (from);
 	  found_barrier = from;
 
 	  /* If we are at the end of the function, or in front of an alignment
 	     instruction, we need not insert an extra alignment.  We prefer
 	     this kind of barrier.  */
-	  if (barrier_align (from) > 2)
-	    good_barrier = from;
+	  if (bar_align > 2)
+	    {
+	      new_align = 1 << bar_align;
+	      good_barrier = from;
+	    }
 
 	  /* If we are at the end of a hot/cold block, dump the constants
 	     here.  */
@@ -4640,6 +4450,24 @@ find_barrier (int num_mova, rtx mova, rtx from)
 	       && ! optimize_size)
 	new_align = 4;
 
+      /* long jumps will change the alignment for the .long label.  */
+      else if (GET_CODE (from) == JUMP_INSN
+ 	       && GET_CODE (PATTERN (from)) == SET
+ 	       && recog_memoized (from) == CODE_FOR_jump_compact
+ 	       && inc == 10)
+ 	new_align = 4;
+ 
+      if (TARGET_DBHWBUG
+ 	  && ((GET_CODE (from) == INSN
+ 	       && GET_CODE (PATTERN (from)) != USE
+ 	       && GET_CODE (PATTERN (from)) != CLOBBER)
+ 	      || GET_CODE (from) == CALL_INSN
+ 	      || (GET_CODE (from) == JUMP_INSN
+ 		  && GET_CODE (PATTERN (from)) != ADDR_DIFF_VEC
+ 		  && GET_CODE (PATTERN (from)) != ADDR_VEC))
+ 	  && get_attr_needs_delay_slot (from) == NEEDS_DELAY_SLOT_YES)
+ 	inc += 2;
+ 
       /* There is a possibility that a bf is transformed into a bf/s by the
 	 delay slot scheduler.  */
       if (JUMP_P (from) && !JUMP_TABLE_DATA_P (from) 
@@ -4656,6 +4484,9 @@ find_barrier (int num_mova, rtx mova, rtx from)
 	      si_align = new_align;
 	    }
 	  count_si = (count_si + new_align - 1) & -new_align;
+
+	  if (new_align < si_align)
+	    si_align = new_align;	
 	}
       if (found_hi)
 	{
@@ -4666,6 +4497,9 @@ find_barrier (int num_mova, rtx mova, rtx from)
 	      hi_align = new_align;
 	    }
 	  count_hi = (count_hi + new_align - 1) & -new_align;
+
+	  if (new_align < hi_align)
+	    hi_align = new_align;	
 	}
       from = NEXT_INSN (from);
     }
@@ -4715,7 +4549,7 @@ find_barrier (int num_mova, rtx mova, rtx from)
 	from = PREV_INSN (from);
 
       /* Don't emit a constant table int the middle of global pointer setting,
-	 since that that would move the addressing base GOT into another table. 
+	 since that that would move the addressing base GOT into another table.
 	 We need the first mov instruction before the _GLOBAL_OFFSET_TABLE_
 	 in the pool anyway, so just move up the whole constant pool.
 	 However, avoid doing so when the last single GOT mov is the starting
@@ -4724,13 +4558,6 @@ find_barrier (int num_mova, rtx mova, rtx from)
       if (last_got && last_got != orig)
 	from = PREV_INSN (last_got);
 
-      /* Don't insert the constant pool table at the position which
-	 may be the landing pad.  */
-      if (flag_exceptions
-	  && CALL_P (from)
-	  && find_reg_note (from, REG_EH_REGION, NULL_RTX))
-	from = PREV_INSN (from);
-
       /* Walk back to be just before any jump or label.
 	 Putting it before a label reduces the number of times the branch
 	 around the constant pool table will be hit.  Putting it before
@@ -5170,7 +4997,7 @@ gen_far_branch (struct far_branch *bp)
 
   ok = invert_jump (insn, label, 1);
   gcc_assert (ok);
-  
+
   /* If we are branching around a jump (rather than a return), prevent
      reorg from using an insn from the jump target as the delay slot insn -
      when reorg did this, it pessimized code (we rather hide the delay slot)
@@ -5232,6 +5059,29 @@ fixup_addr_diff_vecs (rtx first)
     }
 }
 
+int
+sh_jump_align (rtx label)
+{
+  rtx insn;
+  int size = 0;
+
+  gcc_assert (label && GET_CODE (label) == CODE_LABEL);
+
+  for (insn = NEXT_INSN (label);
+       insn && GET_CODE (insn) != BARRIER &&
+	 GET_CODE (insn) != CODE_LABEL;
+       insn = NEXT_INSN (insn))
+    {
+      if (INSN_P (insn))
+	size += get_attr_min_length (insn);
+
+      if (size > sh_align_small_blocks)
+	return align_jumps_log;
+    }
+
+  return 0;
+}
+
 /* BARRIER_OR_LABEL is either a BARRIER or a CODE_LABEL immediately following
    a barrier.  Return the base 2 logarithm of the desired alignment.  */
 int
@@ -5343,9 +5193,23 @@ barrier_align (rtx barrier_or_label)
 	}
     }
 
-  return align_jumps_log;
+  while (BARRIER_P (barrier_or_label))
+    barrier_or_label = next_nonnote_insn (barrier_or_label);
+
+  return sh_jump_align (barrier_or_label);
 }
 
+static bool
+in_between(rtx start, rtx end, rtx r)
+{
+  rtx scan;
+  for (scan = NEXT_INSN (start); scan && scan != end; scan = NEXT_INSN (scan))
+    if (scan == r)
+      return 1;
+
+  return 0;
+}
+
 /* If we are inside a phony loop, almost any kind of label can turn up as the
    first one in the loop.  Aligning a braf label causes incorrect switch
    destination addresses; we can detect braf labels because they are
@@ -5370,6 +5234,49 @@ sh_loop_align (rtx label)
   return align_loops_log;
 }
 
+static int fixup_addr;
+
+/* Check if a register function load is not alive in an exception handler
+   and can be safely removed when relaxing.  */
+
+typedef struct
+{
+  bool ret;
+  rtx reg;
+} rdata;
+
+/* Conservative approach. Will return true even if reg is set in the handler
+so its value could be dead. */
+static void
+handler_uses_reg (rtx scan, void *arg)
+{
+  rdata *rarg = (rdata*)arg;
+  bool seen_jump = false;;
+
+  if (rarg->ret)
+    return;
+
+  for (; scan; scan = NEXT_INSN (scan))
+    {
+      if (INSN_P (scan) || CALL_P (scan))
+        seen_jump = true;
+
+      if (! reg_mentioned_p (rarg->reg, scan))
+        continue;
+
+      if (!seen_jump && reg_set_p (rarg->reg, scan))
+	{
+	  rarg->ret = false;
+	  return;
+	}
+      if (reg_referenced_p (rarg->reg, PATTERN (scan)))
+	{
+	  rarg->ret = true;
+	  return;
+	}
+    }
+}
+
 /* Do a final pass over the function, just before delayed branch
    scheduling.  */
 
@@ -5420,10 +5327,17 @@ sh_reorg (void)
 	    }
 	}
 
+      if (df && optimize)
+	{
+	  df_note_add_problem ();
+	  df_analyze ();
+	}
+
       for (insn = first; insn; insn = NEXT_INSN (insn))
 	{
 	  rtx pattern, reg, link, set, scan, dies, label;
-	  int rescan = 0, foundinsn = 0;
+	  bool rescan = false, foundinsn = false;
+	  rdata region_arg;
 
 	  if (CALL_P (insn))
 	    {
@@ -5505,7 +5419,7 @@ sh_reorg (void)
 		 the call, and can result in situations where a single call
 		 insn may have two targets depending on where we came from.  */
 
-	      if (LABEL_P (scan) && ! foundinsn)
+	      if (LABEL_P (scan) && (LABEL_NUSES (scan) > 0))
 		break;
 
 	      if (! INSN_P (scan))
@@ -5516,8 +5430,22 @@ sh_reorg (void)
                  instructions at the jump destination did not use REG.  */
 
 	      if (JUMP_P (scan))
-		break;
+		{
+		  if (ANY_RETURN_P (PATTERN (scan)))
+		    break;
 
+		  if (simplejump_p (scan))
+		    {
+		      rtx lab = JUMP_LABEL(scan);
+		      rtx next = next_active_insn (lab);
+
+		      if (next && in_between (link, scan, next))
+			continue;
+		    }
+
+		  break;
+		}
+
 	      if (! reg_mentioned_p (reg, scan))
 		continue;
 
@@ -5533,7 +5461,7 @@ sh_reorg (void)
 		  /* There is a function call to this register other
                      than the one we are checking.  If we optimize
                      this call, we need to rescan again below.  */
-		  rescan = 1;
+		  rescan = true;
 		}
 
 	      /* ??? We shouldn't have to worry about SCANSET here.
@@ -5566,6 +5494,15 @@ sh_reorg (void)
 	      continue;
 	    }
 
+	  /* If the register is in use in one of the exception handler,
+	     can't relax it.  */
+
+	  region_arg.reg = reg;
+	  region_arg.ret = false;
+	  for_each_eh_label (handler_uses_reg, (void *)&region_arg);
+	  if (region_arg.ret)
+	    continue;
+
 	  /* Create a code label, and put it in a REG_LABEL_OPERAND note
              on the insn which sets the register, and on each call insn
              which uses the register.  In final_prescan_insn we look for
@@ -5575,6 +5512,7 @@ sh_reorg (void)
 	  label = gen_label_rtx ();
 	  add_reg_note (link, REG_LABEL_OPERAND, label);
 	  add_reg_note (insn, REG_LABEL_OPERAND, label);
+
 	  if (rescan)
 	    {
 	      scan = link;
@@ -5777,6 +5715,14 @@ sh_reorg (void)
 					       gen_rtvec (1, newsrc),
 					       UNSPEC_MOVA);
 		    }
+		  else if (GET_CODE (src) == UNSPEC_VOLATILE
+			   && XINT (src, 1) == UNSPECV_SP_SWITCH_B)
+		    {
+		      newsrc = XVECEXP (src, 0, 0);
+		      XVECEXP (src, 0, 0) = gen_const_mem (mode, newsrc);
+		      INSN_CODE (scan) = -1;
+		      continue;
+		    }
 		  else
 		    {
 		      lab = add_constant (src, mode, 0);
@@ -5796,7 +5742,7 @@ sh_reorg (void)
     PUT_MODE (insn, VOIDmode);
 
   mdep_reorg_phase = SH_SHORTEN_BRANCHES1;
-  INSN_ADDRESSES_FREE ();
+  init_insn_lengths ();
   split_branches (first);
 
   /* The INSN_REFERENCES_ARE_DELAYED in sh.h is problematic because it
@@ -5823,6 +5769,8 @@ sh_reorg (void)
     REG_USERVAR_P (get_fpscr_rtx ()) = 0;
 #endif
   mdep_reorg_phase = SH_AFTER_MDEP_REORG;
+
+   fixup_addr = 0;
 }
 
 int
@@ -6079,10 +6027,15 @@ split_branches (rtx first)
    variable length.  This is because the second pass of shorten_branches
    does not bother to update them.  */
 
+static void sh_hw_workaround (rtx insn);
+
 void
 final_prescan_insn (rtx insn, rtx *opvec ATTRIBUTE_UNUSED,
 		    int noperands ATTRIBUTE_UNUSED)
 {
+  if (TARGET_DBHWBUG)
+    sh_hw_workaround (insn);
+
   if (TARGET_DUMPISIZE)
     fprintf (asm_out_file, "\n! at %04x\n", INSN_ADDRESSES (INSN_UID (insn)));
 
@@ -6191,6 +6144,7 @@ output_stack_adjust (int size, rtx reg, int epilog
 
       if (CONST_OK_FOR_ADD (size))
 	emit_fn (GEN_ADD3 (reg, reg, GEN_INT (size)));
+
       /* Try to do it with two partial adjustments; however, we must make
 	 sure that the stack is properly aligned at all times, in case
 	 an interrupt occurs between the two partial adjustments.  */
@@ -6403,14 +6357,24 @@ pop (int rn)
 static void
 push_regs (HARD_REG_SET *mask, int interrupt_handler)
 {
-  int i = interrupt_handler ? LAST_BANKED_REG + 1 : 0;
+  int i;
   int skip_fpscr = 0;
 
+  /* Push double first. Keep the strictness alignment in case of -mdalign.  */
+  for (i = FIRST_FP_REG; i <= LAST_FP_REG; i++)
+    if (TEST_HARD_REG_BIT (*mask, i))
+      push (i);
+
+  i = interrupt_handler ? LAST_BANKED_REG + 1 : 0;
+
   /* Push PR last; this gives better latencies after the prologue, and
      candidates for the return delay slot when there are no general
      registers pushed.  */
   for (; i < FIRST_PSEUDO_REGISTER; i++)
     {
+      if (FP_REGISTER_P(i))
+	continue;
+
       /* If this is an interrupt handler, and the SZ bit varies,
 	 and we have to push any floating point register, we need
 	 to switch to the correct precision first.  */
@@ -6876,6 +6840,9 @@ sh_expand_prologue (void)
   tree sp_switch_attr
     = lookup_attribute ("sp_switch", DECL_ATTRIBUTES (current_function_decl));
 
+  if (sh_cfun_naked_p ())
+    return;
+
   current_function_interrupt = sh_cfun_interrupt_handler_p ();
 
   /* We have pretend args if we had an object sent partially in registers
@@ -6936,7 +6903,7 @@ sh_expand_prologue (void)
   /* Emit the code for SETUP_VARARGS.  */
   if (cfun->stdarg)
     {
-      if (TARGET_VARARGS_PRETEND_ARGS (current_function_decl))
+      if (crtl->args.pretend_args_size)
 	{
 	  /* Push arg regs as if they'd been provided by caller in stack.  */
 	  for (i = 0; i < NPARM_REGS(SImode); i++)
@@ -6966,7 +6933,6 @@ sh_expand_prologue (void)
 
       lab = add_constant (sp_switch, SImode, 0);
       newsrc = gen_rtx_LABEL_REF (VOIDmode, lab);
-      newsrc = gen_const_mem (SImode, newsrc);
 
       emit_insn (gen_sp_switch_1 (newsrc));
     }
@@ -7238,6 +7204,9 @@ sh_expand_epilogue (bool sibcall_p)
   int fpscr_deferred = 0;
   int e = sibcall_p ? -1 : 1;
 
+  if (sh_cfun_naked_p ())
+    return;
+
   d = calc_live_regs (&live_regs_mask);
 
   save_size = d;
@@ -7430,11 +7399,7 @@ sh_expand_epilogue (bool sibcall_p)
 	   register.  */
       if (TEST_HARD_REG_BIT (live_regs_mask, PR_REG)
 	  && !sh_cfun_resbank_handler_p ())	
-	{
-	  if (!frame_pointer_needed)
-	    emit_insn (gen_blockage ());
 	  pop (PR_REG);
-	}
 
       /* Banked registers are popped first to avoid being scheduled in the
 	 delay slot. RTE switches banks before the ds instruction.  */
@@ -7481,6 +7446,9 @@ sh_expand_epilogue (bool sibcall_p)
 	{
 	  int j = (FIRST_PSEUDO_REGISTER - 1) - i;
 
+	  if (FP_REGISTER_P(j))
+	    continue;
+
 	  if (j == FPSCR_REG && current_function_interrupt && TARGET_FMOVD
 	      && hard_reg_set_intersect_p (live_regs_mask,
 					  reg_class_contents[DF_REGS]))
@@ -7500,6 +7468,12 @@ sh_expand_epilogue (bool sibcall_p)
 	    pop (FPSCR_REG);
 	}
     }
+
+  /* Pop double last. */
+  for (i = LAST_FP_REG; i >= FIRST_FP_REG; i--)
+    if (TEST_HARD_REG_BIT (live_regs_mask, i))
+      pop (i);
+
   if (target_flags != save_flags && ! current_function_interrupt)
     emit_insn (gen_toggle_sz ());
   target_flags = save_flags;
@@ -7525,24 +7499,6 @@ sh_expand_epilogue (bool sibcall_p)
     emit_use (gen_rtx_REG (SImode, PR_REG));
 }
 
-static int sh_need_epilogue_known = 0;
-
-int
-sh_need_epilogue (void)
-{
-  if (! sh_need_epilogue_known)
-    {
-      rtx epilogue;
-
-      start_sequence ();
-      sh_expand_epilogue (0);
-      epilogue = get_insns ();
-      end_sequence ();
-      sh_need_epilogue_known = (epilogue == NULL ? -1 : 1);
-    }
-  return sh_need_epilogue_known > 0;
-}
-
 /* Emit code to change the current function's return address to RA.
    TEMP is available as a scratch register, if needed.  */
 
@@ -7622,7 +7578,7 @@ static void
 sh_output_function_epilogue (FILE *file ATTRIBUTE_UNUSED,
 			     HOST_WIDE_INT size ATTRIBUTE_UNUSED)
 {
-  sh_need_epilogue_known = 0;
+  mdep_reorg_phase = SH_BEFORE_MDEP_REORG;
 }
 
 static rtx
@@ -8149,7 +8105,7 @@ sh_dwarf_register_span (rtx reg)
 {
   unsigned regno = REGNO (reg);
 
-  if (WORDS_BIG_ENDIAN || GET_MODE (reg) != DFmode)
+  if (WORDS_BIG_ENDIAN || GET_MODE (reg) != DFmode || TARGET_FMOVD)
     return NULL_RTX;
 
   return
@@ -8910,6 +8866,22 @@ sh_handle_resbank_handler_attribute (tree * node,
   return NULL_TREE;
 }
 
+/* Handle an attribute requiring a FUNCTION_DECL;
+   arguments as in struct attribute_spec.handler.  */
+static tree
+sh_handle_fndecl_attribute (tree *node, tree name, tree args ATTRIBUTE_UNUSED,
+			     int flags ATTRIBUTE_UNUSED, bool *no_add_attrs)
+{
+  if (TREE_CODE (*node) != FUNCTION_DECL)
+    {
+      warning (OPT_Wattributes, "%qs attribute only applies to functions",
+	       IDENTIFIER_POINTER (name));
+      *no_add_attrs = true;
+    }
+
+  return NULL_TREE;
+}
+
 /* Handle an "interrupt_handler" attribute; arguments as in
    struct attribute_spec.handler.  */
 static tree
@@ -9146,6 +9118,15 @@ sh_cfun_resbank_handler_p (void)
               != NULL_TREE) && TARGET_SH2A);
 }
 
+/* Return nonzero if the current function has attribute naked .  */
+static int
+sh_cfun_naked_p (void)
+{
+  return (lookup_attribute ("naked",
+			    DECL_ATTRIBUTES (current_function_decl))
+	  != NULL_TREE);
+}
+
 /* Implement TARGET_CHECK_PCH_TARGET_FLAGS.  */
 
 static const char *
@@ -9216,9 +9197,12 @@ fp_one_operand (rtx op)
    choosing an fldi alternative during reload and thus failing to
    allocate a scratch register for the constant loading.  */
 int
-fldi_ok (void)
+fldi_ok (bool secondary)
 {
-  return 1;
+  if (! TARGET_FLDI)
+    return 0;
+
+  return !secondary || (!reload_in_progress && !reload_completed);
 }
 
 /* Return the TLS type for TLS symbols, 0 for otherwise.  */
@@ -9230,6 +9214,65 @@ tls_symbolic_operand (rtx op, enum machine_mode mo
   return SYMBOL_REF_TLS_MODEL (op);
 }
 
+/* Expand an sfunc operation taking NARGS MODE arguments, using generator
+   function FUN, which needs symbol NAME loaded int a register first.
+   Add a REG_EQUAL note using EQUIV.  */
+static void
+expand_sfunc_op (int nargs, enum machine_mode mode, rtx (*fun) (rtx, rtx),
+		 const char *name, rtx equiv, rtx *operands)
+{
+  int i;
+  rtx addr, first = NULL_RTX, last, insn;
+  /* For now keep all variants ABI compatibilities.
+     Check for TARGET_OSFP: ARG_TO_R4 (see ieee-754-df.S) and _SH_FPU_ANY_.  */
+  int next_reg = FIRST_PARM_REG;
+
+  addr = gen_reg_rtx (Pmode);
+  function_symbol (addr, name, SFUNC_FREQUENT);
+
+  for (i = 1; i <= nargs; i++)
+    {
+      insn = emit_move_insn (gen_rtx_REG (mode, next_reg), operands[i]);
+      if (!first)
+	first = insn;
+      next_reg += GET_MODE_SIZE (mode) / UNITS_PER_WORD;
+    }
+  last = emit_insn ((*fun) (operands[0], addr));
+  add_reg_note (last, REG_EQUAL, equiv);
+
+  /* If flag_non_call_exceptions is in effect, it will stipulate BB boundaries
+     where we don't want them; we must not have a LIBCALL block spanning
+     multiple basic blocks.  */
+  if (flag_non_call_exceptions)
+    {
+      for (insn = first; insn != last; insn = NEXT_INSN (insn))
+	if (may_trap_p (insn))
+	  return;
+    }
+}
+
+/* Expand an sfunc unary operation taking an MODE argument, using generator
+   function FUN, which needs symbol NAME loaded int a register first.
+   Add a REG_EQUAL note using CODE.  */
+void
+expand_sfunc_unop (enum machine_mode mode, rtx (*fun) (rtx, rtx),
+		   const char *name, enum rtx_code code, rtx *operands)
+{
+  rtx equiv = gen_rtx_fmt_e (code, GET_MODE (operands[0]), operands[1]);
+  expand_sfunc_op (1, mode, fun, name, equiv, operands);
+}
+
+/* Expand an sfunc binary operation in MODE, using generator function FUN,
+   which needs symbol NAME loaded int a register first.
+   Add a REG_EQUAL note using CODE.  */
+void
+expand_sfunc_binop (enum machine_mode mode, rtx (*fun) (rtx, rtx),
+		    const char *name, enum rtx_code code, rtx *operands)
+{
+  rtx equiv = gen_rtx_fmt_ee (code, mode, operands[1], operands[2]);
+  expand_sfunc_op (2, mode, fun, name, equiv, operands);
+}
+
 /* Return the destination address of a branch.  */
 
 static int
@@ -9462,6 +9505,15 @@ get_free_reg (HARD_REG_SET regs_live)
   return gen_rtx_REG (Pmode, 7);
 }
 
+/* This function switches the fpscr.  */
+void
+emit_fpu_flip (void)
+{
+  emit_insn (gen_toggle_pr ());
+  if (TARGET_FMOVD)
+    emit_insn (gen_toggle_sz ());
+}
+
 /* This function will set the fpscr from memory.
    MODE is the mode we are setting it to.  */
 void
@@ -9480,9 +9532,290 @@ fpscr_set_from_mem (int mode, HARD_REG_SET regs_li
 #define IS_ASM_LOGICAL_LINE_SEPARATOR(C, STR) ((C) == ';')
 #endif
 
+static int
+sh_forward_branch_p(rtx first, int n)
+{
+  int new_align;
+  rtx insn;
+  rtx lab = JUMP_LABEL (first);
+  int njumps = 0;
+
+  for (insn = NEXT_INSN (first); insn; insn = NEXT_INSN (insn))
+    {
+      if (((GET_CODE (insn) == CALL_INSN
+	    || (GET_CODE (insn) == JUMP_INSN
+		&& GET_CODE (PATTERN (insn)) != ADDR_DIFF_VEC
+		&& GET_CODE (PATTERN (insn)) != ADDR_VEC))
+	   && num_delay_slots (insn))
+	  || (GET_CODE (insn) == INSN && GET_CODE (PATTERN (insn)) == SEQUENCE))
+	njumps += get_attr_length (insn) + 2;
+
+      else if (GET_CODE (insn) == INSN
+	       && GET_CODE (PATTERN (insn)) == UNSPEC_VOLATILE
+	       && XINT (PATTERN (insn), 1) == UNSPECV_ALIGN)
+	{
+	  new_align = 1 << INTVAL (XVECEXP (PATTERN (insn), 0, 0));
+	  njumps += new_align;
+	}
+
+      else if (GET_CODE (insn) == CODE_LABEL)
+	{
+	  new_align = label_to_alignment (insn);
+	  if (new_align)
+	    new_align = 1 << new_align;
+	  njumps += new_align;
+	}
+
+      else
+	njumps += get_attr_length (insn);
+
+      if (insn == lab)
+	  return njumps > (n * 2);
+    }
+
+  njumps = 0;
+
+  for (insn = PREV_INSN (first); insn; insn = PREV_INSN (insn))
+    {
+      if (((GET_CODE (insn) == CALL_INSN
+	    || ((GET_CODE (insn) == JUMP_INSN)
+		&& GET_CODE (PATTERN (insn)) != ADDR_DIFF_VEC
+		&& GET_CODE (PATTERN (insn)) != ADDR_VEC))
+	   && num_delay_slots (insn))
+	  || (GET_CODE (insn) == INSN && GET_CODE (PATTERN (insn)) == SEQUENCE))
+	njumps += get_attr_length (insn) + 2;
+
+      else if (GET_CODE (insn) == INSN
+	       && GET_CODE (PATTERN (insn)) == UNSPEC_VOLATILE
+	       && XINT (PATTERN (insn), 1) == UNSPECV_ALIGN)
+	{
+	  new_align = 1 << INTVAL (XVECEXP (PATTERN (insn), 0, 0));
+	  njumps += new_align;
+	}
+
+      else if (GET_CODE (insn) == CODE_LABEL)
+	{
+	  new_align = label_to_alignment (insn);
+	  if (new_align)
+	    new_align = 1 << new_align;
+	  njumps += new_align;
+	}
+
+      else
+	njumps += get_attr_length (insn);
+
+      if (insn == lab)
+	return njumps > (n * 2);
+    }
+
+  return 0;
+}
+
 int
-sh_insn_length_adjustment (rtx insn)
+sh_insn_length_alignment (rtx insn)
 {
+  int align = 1;
+
+  if (INSN_P (insn))
+    {
+      rtx body = PATTERN (insn);
+
+      if (GET_CODE (body) == ASM_INPUT || asm_noperands (body) >= 0)
+	{
+	  const char *templ;
+
+	  if (GET_CODE (body) == ASM_INPUT)
+	    templ = XSTR (body, 0);
+	  else
+	    templ = decode_asm_operands (body, NULL, NULL, NULL, NULL, NULL);
+
+	  (void) sh_asm_count (templ, &align);
+	  return align;
+	}
+
+      return 1 << TARGET_SHMEDIA;
+    }
+
+#if 0
+  (NONJUMP_INSN_P (A_INSN)						\
+   ? 1 << TARGET_SHMEDIA						\
+   : JUMP_P (A_INSN) || CALL_P (A_INSN)					\
+   ? 1 << TARGET_SHMEDIA						\
+   : CACHE_LOG)
+#endif
+
+  align = GET_CODE (insn) == JUMP_INSN
+    && GET_CODE (PATTERN (insn)) == ADDR_DIFF_VEC
+    && GET_MODE (PATTERN (insn)) == HImode
+    ? 0
+    : GET_CODE (insn) == BARRIER
+    ? 0
+    : GET_CODE (insn) == JUMP_INSN || GET_CODE (insn) == CALL_INSN
+    ? 1 << TARGET_SHMEDIA
+    : 1;
+
+  return align;
+}
+
+bool
+sh_varying_insn_p (rtx insn)
+{
+  return (mdep_reorg_phase == SH_AFTER_MDEP_REORG
+	  && GET_CODE (insn) == INSN
+	  && recog_memoized (insn) == CODE_FOR_casesi_worker_1);
+}
+
+int
+sh_insn_length_adjustment (rtx insn, const int cur_length)
+{
+  int addlen = 0;
+
+  if (sh_varying_insn_p (insn))
+    {
+      if (recog_memoized (insn) == CODE_FOR_casesi_worker_1)
+	{
+	  rtx src = SET_SRC (XVECEXP (PATTERN (insn), 0, 0));
+	  rtx lab, diff_vec;
+	  int u;
+
+	  if (MEM_P (src))
+	    src = XEXP (src, 0);
+
+	  lab = XEXP (XVECEXP (src, 0, 2), 0);
+	  diff_vec = PATTERN (next_real_insn (lab));
+
+	  u = ADDR_DIFF_VEC_FLAGS (diff_vec).offset_unsigned;
+
+	  if (GET_MODE (diff_vec) == QImode)
+	    {
+	      if (!u && cur_length == 4)
+		return -2;
+	      else if (u && cur_length == 2)
+		return 2;
+	    }
+	}
+
+      return 0;
+    }
+
+  /* optimize DC introduced by reorg.  */
+  if (TARGET_DEAD_DELAY && mdep_reorg_phase == SH_AFTER_MDEP_REORG && INSN_ADDRESSES_SET_P ())
+    {
+      if (deleted_delay_slot_p (insn))
+	return 0;
+
+      if (NONJUMP_INSN_P (insn) && GET_CODE (PATTERN (insn)) == SEQUENCE && cur_length == 4)
+	{
+	  rtx body = PATTERN (insn);
+	  rtx delay_insn = XVECEXP (body, 0, 1);
+	  rtx dpat = PATTERN (delay_insn);
+	  rtx nexti = next_real_insn (delay_insn);
+	  rtx label = NEXT_INSN (delay_insn);
+	  int log = 0;
+
+	  if (label && BARRIER_P (label))
+	    {
+	      for (; label && ! INSN_P (label);
+		   label = NEXT_INSN (label))
+		if (LABEL_P (label))
+		  {
+		    log = label_to_alignment (label);
+		    if (log) break;
+		  }
+	    }
+
+	  if (nexti && GET_CODE (dpat) == SET)
+	    {
+	      rtx jump_insn = XVECEXP (body, 0, 0);
+	      enum attr_type jump_type = get_attr_type (jump_insn);
+
+	      if ((jump_type != TYPE_SFUNC && jump_type != TYPE_CALL)
+		  && GET_CODE (jump_insn) == JUMP_INSN
+		  && rtx_equal_p (PATTERN (nexti), dpat)
+		  && ! reg_overlap_mentioned_p (SET_DEST (dpat), dpat))
+		{
+		  rtx lab = JUMP_LABEL (jump_insn);
+		  rtx prev = prev_real_insn (lab);
+
+		  /* bug 50754 optimize 2 instructions :
+		     br .l
+		     mov r1,r3
+		     .l: mov r1,r3  */
+		  if (nexti == prev)
+		    {
+		      delete_insn (insn);
+		      return -4;
+		    }
+		  /* bug 58105: optimize 1 instruction :
+		     bf/s .l
+		     mov r1,r3
+		     mov r1,r3  */
+		  else if (!log)
+		    {
+		      rtx old_delay = delay_insn;
+		      delay_insn = make_insn_raw (gen_dup_db_insn ());
+
+		      NEXT_INSN (delay_insn) = NEXT_INSN (old_delay);
+		      PREV_INSN (delay_insn) = jump_insn;
+		      NEXT_INSN (jump_insn) = delay_insn;
+		      XVECEXP (body, 0, 1) = delay_insn;		
+
+		      INSN_ADDRESSES_NEW (delay_insn, -1);
+		      return -2;
+		    }
+		}
+	    }
+	}
+    }
+
+  if (TARGET_DBHWBUG
+      && mdep_reorg_phase == SH_AFTER_MDEP_REORG && INSN_ADDRESSES_SET_P ())
+    {
+      if (recog_memoized (insn) == CODE_FOR_tls_global_dynamic
+	  || recog_memoized (insn) == CODE_FOR_tls_local_dynamic
+	  || recog_memoized (insn) == CODE_FOR_tls_initial_exec)
+	return -2;
+
+      if (recog_memoized (insn) == CODE_FOR_jump_compact)
+	{
+	  if (cur_length == 2)
+	    {
+	      int long_b = sh_forward_branch_p (insn, 0x7ff);
+
+	      if (long_b)
+		{
+		  rtx prev;
+		  int xlen = 0;
+		
+		  /* will need a scratch register.  */
+		  if (GET_CODE ((prev = prev_nonnote_insn (insn))) != INSN
+		      || INSN_CODE (prev) != CODE_FOR_indirect_jump_scratch)
+		    xlen = 4;
+
+		  if (!xlen)
+		    {
+		      /* a nop will be inserted. count it.  */
+		      /* mov r0 lab; braf r0; nop.  */
+		      if (GET_CODE (PATTERN (NEXT_INSN (PREV_INSN (insn))))
+			  != SEQUENCE)
+			xlen += 2;
+		    }
+		  addlen = 4 + xlen;
+		  return addlen;
+		}
+	    }
+	}
+
+      if ((recog_memoized (insn) == CODE_FOR_branch_true
+	   || recog_memoized (insn) == CODE_FOR_branch_false)
+	  && cur_length == 2)
+	{
+	  int long_b = sh_forward_branch_p (insn, 0x7f);
+	  if (long_b)
+	    return 4;
+	}
+    }
+
   /* Instructions with unfilled delay slots take up an extra two bytes for
      the nop in the delay slot.  */
   if (((NONJUMP_INSN_P (insn)
@@ -9490,10 +9823,176 @@ int
 	&& GET_CODE (PATTERN (insn)) != CLOBBER)
        || CALL_P (insn)
        || (JUMP_P (insn) && !JUMP_TABLE_DATA_P (insn)))
-      && GET_CODE (PATTERN (NEXT_INSN (PREV_INSN (insn)))) != SEQUENCE
+         /* we are not treating a sequence insn */
+      && GET_CODE (PATTERN ((insn)))!=SEQUENCE
       && get_attr_needs_delay_slot (insn) == NEEDS_DELAY_SLOT_YES)
-    return 2;
+    {
+      int xnop = 0;
+      int coupled_insn_size = 0;
 
+      if (TARGET_DBHWBUG
+	  && mdep_reorg_phase < SH_AFTER_MDEP_REORG && INSN_ADDRESSES_SET_P ())
+	xnop = 2;
+
+      /* if current insn is the first of the sequence : mark coupled insn size */
+      if (GET_CODE (PATTERN (NEXT_INSN ( PREV_INSN(insn)))) == SEQUENCE && 
+	  GET_CODE (PATTERN ((insn)))!=SEQUENCE)
+        coupled_insn_size = 2;
+      
+      if (cur_length == 10)
+	{
+	  /* Ideally that should be in final.c but we don't have computed the
+	     insns lengths at that time.
+	     Make sure here that .align padding is taken into account when
+	     computing distances.  */
+	  /* branch is 2 bytes bellow the start of the sequence.  */
+	  int uid_address = INSN_ADDRESSES (INSN_UID (insn));
+	  int offset = branch_dest (insn) - uid_address;
+	  int far = ! (offset >= FAR_JUMP_MIN && offset <= FAR_JUMP_MAX);
+	  rtx prev;
+
+	  /*
+	    (2) mov.l	.L,r4		[length = 10]
+	    (2) jmp	@r4
+	    (2) nop
+	       .align 2
+	    (4) .L: .long	.L5
+	  */
+
+	  if (far)
+	    {
+	      int long_address = uid_address + 6;
+	      int aligned_long_address = (long_address + 3) & -4;
+	      return aligned_long_address - long_address - coupled_insn_size;
+	    }
+
+	  /*  If we have a scratch register available, well use it. 
+	    (2) mov.w	.L,r4		[length = 8]
+	    (2) braf	@r4
+	    (2) nop
+	    (2) .L: .word	.L5
+	  */
+
+	  prev = prev_nonnote_insn (insn);
+	  if (prev && NONJUMP_INSN_P (prev)
+	      && INSN_CODE (prev) == CODE_FOR_indirect_jump_scratch)
+	    {
+	      return -2 - coupled_insn_size;
+	    }
+
+	  /*
+	    (2) mov.l	r13,@-r15       [length = 10]
+	    (2) mov.w	.L7794,r13
+	    (2) braf	r13
+	    (2) mov.l	@r15+,r13
+	    (2) .L7794:	.word .L5211-.L7794
+	  */
+	  if (GET_CODE (PATTERN (NEXT_INSN (PREV_INSN (insn)))) != SEQUENCE)
+	  	return 0;
+
+          /* fall down if we are analysing an insn of a sequence */
+	}
+
+      else if (cur_length == 14)
+	{
+	  int offset = branch_dest (insn) - INSN_ADDRESSES (INSN_UID (insn));
+	  int far = ! (offset >= FAR_JUMP_MIN && offset <= FAR_JUMP_MAX);
+
+	  /*
+	    (2) mov.l	r13,@-r15	[length = 12]
+	    (2) mov.l	.L,r13
+	    (2) jmp	@r13
+	    (2) mov.l	@r15+,r13
+	    (4) .L:	.long	.L5
+	  */
+	  if (far)
+	    {
+	      int uid_address = INSN_ADDRESSES (INSN_UID (insn));
+	      int long_address = uid_address + 8 - coupled_insn_size;
+	      int aligned_long_address = (long_address + 3) & -4;
+	      int adj;
+
+	      adj = aligned_long_address - long_address;
+
+	      return -2 + adj + xnop;
+	    }
+	  /*
+	    (2) mov.l	r13,@-r15	[length = 10]
+	    (2) mov.w	.L4460,r13
+	    (2) braf	r13
+	    (2) mov.l	@r15+,r13
+	    (2) .word .L24-.L4460
+	  */
+	  else
+	    return -4 + xnop;
+	}
+
+      if (GET_CODE (PATTERN (NEXT_INSN (PREV_INSN (insn)))) != SEQUENCE)
+         return 2 + xnop;
+
+      /* fall down if we are analysing an insn of a sequence */
+    }
+
+  /* Filled delay slot. */
+  /* xxx couldn't we do even if not DBWWBUG ?  */
+  if (TARGET_DBHWBUG  && INSN_ADDRESSES_SET_P ())
+    {
+      if (mdep_reorg_phase == SH_AFTER_MDEP_REORG)
+	{
+	  if (recog_memoized (insn) == CODE_FOR_jump_compact)
+	    {
+	      rtx prev;
+	      int offset = branch_dest (insn) - INSN_ADDRESSES (INSN_UID (insn));
+	      int far = ! (offset >= FAR_JUMP_MIN && offset <= FAR_JUMP_MAX);
+
+	      if (cur_length == 10)
+		{
+		  /*
+		    (0) delay slot	        [length = 2]
+		    (2) mov.l	r13,@-r15	[length = 10]
+		    (2) mov.w	.L,r13
+		    (2) braf	r13
+		    (2) mov.l	@r15+,r13
+		    (2) .L: .word
+		  */
+		  if (GET_CODE ((prev = prev_nonnote_insn (insn))) != INSN
+		      || INSN_CODE (prev) != CODE_FOR_indirect_jump_scratch)
+		    return 0;
+
+		  /*
+		    (2) mov.l	.L,r4	 [length = 10]
+		    (2) jmp	@r4
+		    (0) delay slot       [length = 2]
+		    (4) .L: .long	
+		    return 8 or 10 depending on padding.  
+		  */
+		  if (far)
+		    {
+		      int uid_address = INSN_ADDRESSES (INSN_UID (insn));
+		      int long_address = uid_address + 6;
+		      int aligned_long_address = (long_address + 3) & -4;
+		      int adj;
+
+		      adj = aligned_long_address - long_address;
+
+		      return -2 + adj;
+		    }
+		}
+	    }
+	  return 0;
+	}
+      else if (mdep_reorg_phase < SH_AFTER_MDEP_REORG)
+	if (((GET_CODE (insn) == INSN
+	      && GET_CODE (PATTERN (insn)) != USE
+	      && GET_CODE (PATTERN (insn)) != CLOBBER)
+	     || GET_CODE (insn) == CALL_INSN
+	     || (GET_CODE (insn) == JUMP_INSN
+		 && GET_CODE (PATTERN (insn)) != ADDR_DIFF_VEC
+		 && GET_CODE (PATTERN (insn)) != ADDR_VEC))
+	    && get_attr_needs_delay_slot (insn) == NEEDS_DELAY_SLOT_YES)
+	  return 2;
+    }
+
   /* SH2e has a bug that prevents the use of annulled branches, so if
      the delay slot is not filled, we'll have to put a NOP in it.  */
   if (sh_cpu_attr == CPU_SH2E
@@ -9555,9 +10054,11 @@ int
       while (c);
       return sum;
     }
+
   return 0;
 }
 
+
 /* Return TRUE for a valid displacement for the REG+disp addressing
    with MODE.  */
 
@@ -10004,6 +10505,7 @@ mark_constant_pool_use (rtx x)
   return lab;
 }
 
+
 /* Return true if it's possible to redirect BRANCH1 to the destination
    of an unconditional jump BRANCH2.  We only want to do this if the
    resulting branch will have a short displacement.  */
@@ -10035,6 +10537,7 @@ sh_can_redirect_branch (rtx branch1, rtx branch2)
 	    distance += get_attr_length (insn);
 	}
     }
+
   return 0;
 }
 
@@ -10705,9 +11208,6 @@ static bool
 sh_optimize_target_register_callee_saved (bool after_prologue_epilogue_gen)
 {
   HARD_REG_SET dummy;
-#if 0
-  rtx insn;
-#endif
 
   if (! shmedia_space_reserved_for_target_registers)
     return 0;
@@ -11144,10 +11644,10 @@ sh_media_init_builtins (void)
 
 static tree
 sh_media_builtin_decl (unsigned code, bool initialize_p ATTRIBUTE_UNUSED)
-{        
+{
   if (code >= ARRAY_SIZE (bdesc))
     return error_mark_node;
-          
+
   return bdesc[code].fndecl;
 }
 
@@ -11207,10 +11707,10 @@ sh_init_builtins (void)
 
 static tree
 sh_builtin_decl (unsigned code, bool initialize_p ATTRIBUTE_UNUSED)
-{        
+{
   if (TARGET_SHMEDIA)
     return sh_media_builtin_decl (code, initialize_p);
-          
+
   return error_mark_node;
 }
 
@@ -11314,6 +11814,61 @@ sh_expand_unop_v2sf (enum rtx_code code, rtx op0,
 }
 
 void
+sh_expand_lround (rtx op0, rtx op1, bool to_nearest)
+{
+  rtx tmp3;
+  rtx tmp2 = gen_reg_rtx (SImode);
+  rtx tmp1 = gen_reg_rtx (SImode);
+  rtx ftmp1 = gen_reg_rtx (SFmode);
+  rtx ftmp0 = gen_reg_rtx (SFmode);
+  rtx fpul = gen_rtx_REG (SImode, FPUL_REG);
+
+  emit_insn (gen_movsi_i (tmp2, GEN_INT (1 << 31)));
+
+  emit_move_insn (ftmp0, op1);
+
+  emit_move_insn (fpul, gen_lowpart (SImode, ftmp0));
+  emit_move_insn (tmp1, fpul);
+  emit_insn (gen_andsi3 (tmp2, tmp2, tmp1));
+
+  tmp3 = gen_lowpart (SImode,
+		      force_reg (SFmode,
+				 CONST_DOUBLE_ATOF ("0.5", SFmode)));
+
+  emit_insn (gen_iorsi3 (tmp3, tmp3, tmp2));
+
+  emit_insn (gen_addsf3 (ftmp0, ftmp0, gen_lowpart (SFmode, tmp3)));
+
+  emit_sf_insn (gen_fix_truncsfsi2_i4 (fpul, ftmp0, get_fpscr_rtx ()));
+
+  emit_move_insn (op0, fpul);
+
+  if (to_nearest)
+    {
+      rtx lab = gen_label_rtx ();
+
+      emit_sf_insn (gen_floatsisf2_i4 (ftmp1, fpul, get_fpscr_rtx ()));
+
+      emit_sf_insn (gen_cmpeqsf_t_i4 (ftmp1, ftmp0, get_fpscr_rtx ()));
+
+      emit_jump_insn (gen_branch_false (lab));
+
+      emit_move_insn (tmp3, const1_rtx);
+      emit_insn (gen_andsi3 (tmp3, tmp3, op0));
+      emit_insn (gen_cmpeqsi_t (tmp3, const0_rtx));
+      emit_jump_insn (gen_branch_true (lab));
+
+      emit_insn (gen_cmpeqsi_t (tmp2, const0_rtx));
+      emit_insn (gen_movt (tmp2));
+      emit_insn (gen_ashlsi_c (tmp2, tmp2));
+      emit_insn (gen_addsi3 (tmp2, tmp2, GEN_INT (-1)));
+      emit_insn (gen_subsi3 (op0, op0, tmp2));
+
+      emit_label (lab);
+    }
+}
+
+void
 sh_expand_binop_v2sf (enum rtx_code code, rtx op0, rtx op1, rtx op2)
 {
   rtx op = gen_rtx_fmt_ee (code, SFmode, op1, op2);
@@ -11593,6 +12148,8 @@ sh_output_mi_thunk (FILE *file, tree thunk_fndecl
 
   emit_note (NOTE_INSN_PROLOGUE_END);
 
+  emit_barrier ();
+
   /* Find the "this" pointer.  We have such a wide range of ABIs for the
      SH that it's best to do this completely machine independently.
      "this" is passed as first argument, unless a structure return pointer
@@ -11737,10 +12294,12 @@ sh_output_mi_thunk (FILE *file, tree thunk_fndecl
       emit_move_insn (scratch2, funexp);
       funexp = gen_rtx_MEM (FUNCTION_MODE, scratch2);
       sibcall = gen_sibcall (funexp, const0_rtx, NULL_RTX);
+      add_reg_note (sibcall, REG_DEAD, scratch2);
     }
   sibcall = emit_call_insn (sibcall);
   SIBLING_CALL_P (sibcall) = 1;
   use_reg (&CALL_INSN_FUNCTION_USAGE (sibcall), this_rtx);
+
   emit_barrier ();
 
   /* Run just enough of rest_of_compilation to do scheduling and get
@@ -11772,11 +12331,10 @@ function_symbol (rtx target, const char *name, enu
 {
   rtx sym;
 
-  /* If this is not an ordinary function, the name usually comes from a
-     string literal or an sprintf buffer.  Make sure we use the same
+  /* The name usually comes from a string literal or an sprintf buffer.
+     Make sure we use the same
      string consistently, so that cse will be able to unify address loads.  */
-  if (kind != FUNCTION_ORDINARY)
-    name = IDENTIFIER_POINTER (get_identifier (name));
+  name = IDENTIFIER_POINTER (get_identifier (name));
   sym = gen_rtx_SYMBOL_REF (Pmode, name);
   SYMBOL_REF_FLAGS (sym) = SYMBOL_FLAG_FUNCTION;
   if (flag_pic)
@@ -11784,6 +12342,10 @@ function_symbol (rtx target, const char *name, enu
       {
       case FUNCTION_ORDINARY:
 	break;
+      case SFUNC_FREQUENT:
+	if (!optimize || optimize_size)
+	  break;
+	/* Fall through.  */
       case SFUNC_GOT:
 	{
 	  rtx reg = target ? target : gen_reg_rtx (Pmode);
@@ -12290,7 +12852,7 @@ sh_loads_bankedreg_p (rtx insn)
 	return 1;
     }
 
-  return 0;  
+  return 0;
 }
 
 /* FNADDR is the MEM expression from a call expander.  Return an address
@@ -12370,7 +12932,7 @@ sh_secondary_reload (bool in_p, rtx x, reg_class_t
 	  && ! TARGET_SHMEDIA
 	  && immediate_operand ((x), mode)
 	  && ! ((fp_zero_operand (x) || fp_one_operand (x))
-		&& mode == SFmode && fldi_ok ()))
+		&& mode == SFmode && fldi_ok (true)))
 	switch (mode)
 	  {
 	  case SFmode:
@@ -12477,6 +13039,8 @@ sh_conditional_register_usage (void)
   for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno ++)
     if (! VALID_REGISTER_P (regno))
       fixed_regs[regno] = call_used_regs[regno] = 1;
+  if (TARGET_SH4A_FP || TARGET_SH4_300)
+    global_regs[FPSCR_REG] = 1;
   /* R8 and R9 are call-clobbered on SH5, but not on earlier SH ABIs.  */
   if (TARGET_SH5)
     {
@@ -12491,6 +13055,10 @@ sh_conditional_register_usage (void)
       CLEAR_HARD_REG_SET (reg_class_contents[FP0_REGS]);
       regno_reg_class[FIRST_FP_REG] = FP_REGS;
     }
+  if (TARGET_R0R3_TO_REG_MUL < 2)
+    regno_reg_class[R1_REG] = regno_reg_class[R2_REG]
+      = regno_reg_class[R3_REG] = GENERAL_REGS;
+    /* The peephole2s needs reg_class_contents[R0R3_REGS].  */ 
   if (flag_pic)
     {
       fixed_regs[PIC_OFFSET_TABLE_REGNUM] = 1;
@@ -12530,7 +13098,11 @@ sh_legitimate_constant_p (enum machine_mode mode,
 	     || !TARGET_SHMEDIA_FPU
 	     || TARGET_SHMEDIA64)
 	  : (GET_CODE (x) != CONST_DOUBLE
-	     || mode == DFmode || mode == SFmode
+	     || mode == DFmode
+	     || (mode == SFmode
+		 && (!TARGET_FLDI
+		     || ((!fp_one_operand (x) && !fp_zero_operand (x))
+			 || reload_completed)))
 	     || mode == DImode || GET_MODE (x) == VOIDmode));
 }
 
@@ -12542,4 +13114,599 @@ sh_init_sync_libfuncs (void)
   init_sync_libfuncs (UNITS_PER_WORD);
 }
 
+static int asm_size (char *s, int addr, int *seen_align)
+{
+  char *pt;
+
+  if (*s == ';' || *s == '\n' || *s == '\0')
+    return 0;
+
+  else if (strstr (s, ".long"))
+    {
+      int n = 1;
+      /* parse .long	1, 3 syntax.  */
+      while (*s != '\n' && *s != '\0')
+	{
+	  s++;
+	  if (*s == ',')  n++;
+	}
+      return 2 * n;
+    }
+
+  else if (strstr (s, ".short") || strstr (s, ".word"))
+    {
+      int n = 1;
+      /* parse .short	1, 3 syntax.  */
+      while (*s != '\n' && *s != '\0')
+	{
+	  s++;
+	  if (*s == ',')  n++;
+	}
+      return n;
+    }
+
+  else if (insn_current_address != -1 && strstr (s, ".balign"))
+    {
+      long int align;
+      int new_address;
+
+      while (*s != '\t' && *s != ' ') s++;
+      align = strtol (s, NULL, 10);
+      if (errno == ERANGE || errno == EINVAL)
+	{
+	  warning (OPT_mdb_page_bug, "unsupported %s asm for w/a.", s);
+	  return 0;
+	}
+
+      /* return log.  */
+      if (seen_align)
+	*seen_align = exact_log2 (align);
+      new_address = (addr + align - 1) & -align;
+      /* return size / insn_default_length().  */
+      return (new_address - addr) / 2;
+    }
+
+  else if (insn_current_address != -1 && strstr (s, ".align"))
+    {
+      long int align;
+      int new_address;
+
+      while (*s != '\t' && *s != ' ') s++;
+      align = strtol (s, NULL, 10);
+      if (errno == ERANGE || errno == EINVAL)
+	{
+	  warning (OPT_mdb_page_bug, "unsupported %s asm for w/a.", s);
+	  return 0;
+	}
+
+      /* return log.  */
+      if (seen_align)
+	*seen_align = align;
+
+      align = 1 << align;
+      new_address = (addr + align - 1) & -align;
+      /* return size / insn_default_length().  */
+      return (new_address - addr) / 2;
+    }
+
+  else if (strstr (s, ".space") || strstr (s, ".skip"))
+    {
+      long int align;
+      errno = 0;
+      while (*s != '\t' && *s != ' ') s++;
+      align = strtol (s, NULL, 10);
+      if (errno == ERANGE || errno == EINVAL)
+	{
+	  warning (OPT_mdb_page_bug, "unsupported %s asm for w/a.", s);
+	  return 0;
+	}
+      return align / 2;
+    }
+
+  else if (strstr (s, ".fill"))
+    {
+      char delim[] = ",\n";
+      char *size;
+      char *n;
+      while (*s != '\t' && *s != ' ') s++;
+      size = strtok(s, delim);
+      n = strtok(NULL, delim);
+      if (n)
+	{
+	  (void*) strtok(NULL, delim);
+	  return strtol (size, NULL, 10) * strtol (n, NULL, 10) / 2;
+	}
+      return 1;
+    }
+
+  else if ((pt = strrchr (s, ':')) != NULL)
+    {
+      while (*pt == '\t' || *pt == ' ' || *pt == ':') pt++;
+      return asm_size (pt, addr, seen_align);
+    }
+
+  if (*s == '.' && mdep_reorg_phase == SH_AFTER_MDEP_REORG)
+    warning (OPT_mdb_page_bug, "unsupported %s asm for w/a.", s);
+
+  return 1;
+}
+
+int
+sh_align_function_log(tree fn)
+{
+  int first, last;
+
+  if (! INSN_ADDRESSES_SET_P () || optimize <= 0)
+    return align_functions_log;
+
+  if (lookup_attribute ("cold", DECL_ATTRIBUTES (fn)))
+    return 1;
+
+  first = INSN_ADDRESSES (INSN_UID (get_insns()));
+  last = INSN_ADDRESSES (INSN_UID (get_last_insn ()));
+
+ if (last - first < 32)
+    return 2;
+  else
+    return align_functions_log;
+}
+
+int
+sh_asm_count (const char *templ, int *seen_align)
+{
+  int count = 0;
+  char *s, *lt;
+  char delim[] = ";\n";
+  int in_sub = 0;
+  int addr;
+
+  addr = mdep_reorg_phase == SH_AFTER_MDEP_REORG && INSN_ADDRESSES_SET_P () ?
+    insn_current_address : 2;
+
+  lt = (char *) alloca (strlen (templ) + 1);
+  strcpy (lt, templ);
+
+  s = strtok(lt, delim);
+
+  while (s != NULL)
+    {
+      while (*s == '\t' || *s == ' ') s++;
+
+      if (strstr (s, ".pushsection") || strstr (s, ".section"))
+	in_sub++;
+
+      if (! in_sub)
+	count += asm_size (s, addr + (count * 2), seen_align);
+
+      if (strstr (s, ".popsection") || strstr (s, ".previous"))
+	in_sub--;
+
+      s = strtok(NULL, delim);
+    }
+
+  return count;
+}
+
+static int align_next_insn;
+
+static void
+sh_hw_workaround (rtx insn)
+{
+  int uid_address = INSN_ADDRESSES (INSN_UID (insn));
+  int real_address = uid_address + fixup_addr;
+
+  if (GET_CODE (insn) == CODE_LABEL)
+    {
+      rtx barrier = prev_nonnote_insn (insn);
+      if (barrier && BARRIER_P (barrier))
+	{
+	  int log = label_to_alignment (insn);
+	  int align = 1 << log;
+
+	  int aligned_real_address;
+	  int last_unaligned_uid_address = INSN_ADDRESSES (INSN_UID (barrier));
+
+	  real_address -= (uid_address - last_unaligned_uid_address);
+	  aligned_real_address = (real_address + align - 1) & -align;
+
+	  fixup_addr = aligned_real_address - uid_address;
+	  return;
+	}
+    }
+
+  if (INSN_P (insn))
+    {
+      rtx body = PATTERN (insn);
+
+      if (GET_CODE (body) == UNSPEC_VOLATILE && XINT (body, 1) == UNSPECV_ALIGN)
+	{
+	  int log = INTVAL (XVECEXP (body, 0, 0));
+	  int align = 1 << log;
+	  int aligned_real_address = (real_address + align - 1) & -align;
+	  int aligned_current_address = (uid_address + align - 1) & -align;
+
+	  fixup_addr = aligned_real_address - aligned_current_address;
+	  return;
+	}
+
+      if (align_next_insn)
+	{
+	  align_next_insn = 0;
+	  fixup_addr = -uid_address;
+	  fprintf (asm_out_file, ".align 5\t\t! for hw workaround \n");
+	}
+
+      if (GET_CODE (body) == ASM_INPUT || asm_noperands (body) >= 0)
+	{
+	  int align = 0;
+	  const char *templ;
+
+	  if (GET_CODE (body) == ASM_INPUT)
+	    templ = XSTR (body, 0);
+	  else
+	    templ = decode_asm_operands (body, NULL, NULL, NULL, NULL, NULL);
+
+	  (void) sh_asm_count (templ, &align);
+	  if (align)
+	    {
+	      align_next_insn = 1;
+	      return;
+	    }
+	}
+
+      gcc_assert (! (real_address % 2));
+
+      /* +2 because we check that the ds is not aligned on 32.  */
+      real_address += 2;
+
+      if (recog_memoized (insn) == CODE_FOR_cmpgtudi_t
+	  || recog_memoized (insn) == CODE_FOR_cmpgeudi_t
+	  || recog_memoized (insn) == CODE_FOR_cmpgtdi_t
+	  || recog_memoized (insn) == CODE_FOR_cmpgedi_t)
+	if (!((real_address + 2) % 32))
+	  {
+	    fprintf (asm_out_file,
+		     "\tnop\t\t! for hw workaround @%d\n", real_address);
+	    fixup_addr += 2;
+	    return;
+	  }
+
+      if (recog_memoized (insn) == CODE_FOR_tls_global_dynamic
+	  || recog_memoized (insn) == CODE_FOR_tls_local_dynamic)
+	{
+	  int aligned_real_address;
+	  int align = 4;
+
+	  if (!((real_address + 8) % 32) || !((real_address + 12) % 32))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address);
+	      fixup_addr += 2;
+	      real_address += get_attr_length (insn);
+	    }
+	  else
+	    real_address += get_attr_length (insn) - 2;
+
+	  aligned_real_address = (real_address + align - 1) & -align;
+	  fixup_addr += aligned_real_address - real_address;
+	  return;
+	}
+
+      if (recog_memoized (insn) == CODE_FOR_tls_initial_exec)
+	{
+	  int aligned_real_address;
+	  int align = 4;
+
+	  if (!((real_address + 6) % 32))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address);
+	      fixup_addr += 2;
+	      real_address += get_attr_length (insn);
+	    }
+	  else
+	    real_address += get_attr_length (insn) - 2;
+
+	  aligned_real_address = (real_address + align - 1) & -align;
+	  fixup_addr += aligned_real_address - real_address;
+	  return;
+	}
+
+      if (recog_memoized (insn) == CODE_FOR_jump_compact)
+	{
+	  int offset = branch_dest (insn) - INSN_ADDRESSES (INSN_UID (insn));
+	  int far;
+
+	  far = ! (offset >= FAR_JUMP_MIN && offset <= FAR_JUMP_MAX);
+
+	  /* length = 6
+	     mov.w	.L8588,r1
+	     braf	r1
+	     mov	#81,r4	         [length = 2]
+	     .L8588: .word .L12-.L8588
+
+	     length = 8
+	     mov.w	.L8588,r1
+	     braf	r1
+	     nop
+	     .L8588: .word .L12-.L8588
+
+	     length = 10
+	     mov	#81,r4	         [length = 2]
+	     mov.l	r13,@-r15
+	     mov.w	.L,r13
+	     braf	r13
+	     mov.l	@r15+,r13
+	     .L: .word
+	  */
+	  if (!far)
+	    {
+	      if (get_attr_length (insn) == 6 || get_attr_length (insn) == 8)
+		real_address += 2;
+	      if (get_attr_length (insn) == 10)
+		{
+		  if (dbr_sequence_length ())
+		    real_address += 6;
+		  else
+		    real_address += 4;
+		}
+	    }
+	  /* length = 8/10 depending on alignment
+	     mov.l	.L8586,r4
+	     jmp	@r4
+	     mov	#81,r4         [length = 2]
+	     .align	2
+	     .L8586:  .long	.L5
+
+	     length = 10/12 depending on alignment
+	     mov.l	.L8589,r3
+	     jmp	@r3
+	     nop
+	     .align	2
+	     .L8589:  .long	.L2242
+
+	     length = 12/14 depending on alignment
+	     mov.l	r13,@-r15
+	     mov.l	.L4981,r13
+	     jmp	@r13
+	     mov.l	@r15+,r13
+	     .align	2
+	     .L4981:	.long	.L16
+	  */
+	  else
+	    {
+	      int long_address;
+	      int uid_long_address;
+	      int aligned_long_address;
+	      int aligned_uid_long_address;
+
+	      if (get_attr_length (insn) == 8 || get_attr_length (insn) == 10)
+		{
+		  if (!((real_address + 2) % 32))
+		    {
+		      fprintf (asm_out_file,
+			       "\tnop\t\t! for hw workaround @%d\n", real_address);
+		      real_address += 2;
+		    }
+
+		  long_address = real_address + 4;
+		  uid_long_address = uid_address + 6;
+		  aligned_long_address = (long_address + 3) & -4;
+		  aligned_uid_long_address = (uid_long_address + 3) & -4;
+
+		  fixup_addr = aligned_long_address - aligned_uid_long_address;
+		  if (fixup_addr && (fixup_addr % 4))
+		    fixup_addr -= 4;
+
+		  return;
+	      }
+	      else if (get_attr_length (insn) == 12)
+		{
+		  rtx prev;
+
+		  if (!((real_address + 2) % 32))
+		    {
+		      fprintf (asm_out_file,
+			       "\tnop\t\t! for hw workaround @%d\n", real_address);
+		      real_address += 2;
+		    }
+
+		if (GET_CODE ((prev = prev_nonnote_insn (insn))) == INSN
+		    && INSN_CODE (prev) == CODE_FOR_indirect_jump_scratch)
+		  {
+		    long_address = real_address + 4;
+		    uid_long_address = uid_address + 6;
+		  }
+		else
+		  {
+		    long_address = real_address + 6;
+		    uid_long_address = uid_address + 8;
+		  }
+
+		  aligned_long_address = (long_address + 3) & -4;
+		  aligned_uid_long_address = (uid_long_address + 3) & -4;
+
+		  fixup_addr = aligned_long_address - aligned_uid_long_address;
+		  if (fixup_addr && (fixup_addr % 4))
+		    fixup_addr -= 4;
+
+		  return;
+		}
+	      else if (get_attr_length (insn) == 14)
+		{
+		  if (!((real_address + 4) % 32))
+		    {
+		      fprintf (asm_out_file,
+			       "\tnop\t\t! for hw workaround @%d\n", real_address);
+		      real_address += 2;
+		    }
+
+		  long_address = real_address + 6;
+		  uid_long_address = uid_address + 8;
+		  aligned_long_address = (long_address + 3) & -4;
+		  aligned_uid_long_address = (uid_long_address + 3) & -4;
+
+		  fixup_addr = aligned_long_address - aligned_uid_long_address;
+		  if (fixup_addr && (fixup_addr % 4))
+		    fixup_addr -= 4;
+
+		  return;
+		}
+	    }
+
+	  if (!(real_address % 32))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround 1 @%d\n", real_address);
+	      fixup_addr += 2;
+	      return;
+	    }
+	}
+
+      /* we have bt; (aligned)bra; nop; avoid nop ; (aligned)bt; bra; nop.  */
+      /* or bt; delayed instruction; (aligned)bra.  */
+      if ((recog_memoized (insn) == CODE_FOR_branch_true
+	   || recog_memoized (insn) == CODE_FOR_branch_false)
+	  && get_attr_length (insn) == 6)
+	{
+	  if (!((real_address + 2) % 32))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address-2);
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address);
+	      fixup_addr += 4;
+	    }
+	  else if (!(real_address % 32))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address);
+	      fixup_addr += 2;
+	    }
+	  else if (!((real_address + 4) % 32)
+		   && (dbr_sequence_length ()
+		       && ! INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0))
+		       && get_attr_length (XVECEXP (final_sequence, 0, 1))))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address);
+	      fixup_addr += 2;
+	    }
+	}
+
+      if ((GET_CODE (insn) == JUMP_INSN
+	   && GET_CODE (PATTERN (insn)) != ADDR_DIFF_VEC
+	   && GET_CODE (PATTERN (insn)) != ADDR_VEC))
+	{
+	  if (!(real_address % 32)
+	      && (dbr_sequence_length ()
+		  && ! INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0))
+		  && get_attr_length (XVECEXP (final_sequence, 0, 1))))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address);
+	      fixup_addr += 2;
+	    }
+	  else if (GET_CODE (insn) == CALL_INSN
+		   && !(real_address % 32))
+	    {
+	      fprintf (asm_out_file,
+		       "\tnop\t\t! for hw workaround @%d\n", real_address);
+	      fixup_addr += 2;
+	    }
+	}
+
+	if ((GET_CODE (insn) == CALL_INSN || INSN_P (insn))
+	    && !(real_address % 32)
+	    && get_attr_needs_delay_slot (insn) == NEEDS_DELAY_SLOT_YES)
+	{
+	  fprintf (asm_out_file,
+		   "\tnop\t\t! for hw workaround @%d\n", real_address);
+	  fixup_addr += 2;
+	}
+    }
+}
+
+/* Return true if it is appropriate to emit `ret' instructions in the
+   body of a function.  */
+
+bool
+sh_can_use_simple_return_p (void)
+{
+  HARD_REG_SET live_regs_mask;
+  int d = calc_live_regs (&live_regs_mask);
+
+  if (! reload_completed || frame_pointer_needed)
+    return false;
+
+  /* Moving prologue around does't reduce the size.  */
+  if (optimize_function_for_size_p (cfun))
+    return false;
+
+  /* Finally, allow for pr save.  */
+  d = calc_live_regs (&live_regs_mask);
+
+  if (rounded_frame_size (d) > 4)
+   return false;
+
+  return true;
+}
+
+/* When the stack protector inserts codes after R0 is set,
+   a load of @(rX, r12) will cause a spill failure for R0.  Avoid combining
+   (set A (plus rX r12)) and (set op0 (mem A)) when R0 is alive.  */
+bool
+stack_protector_block (rtx insn, rtx op)
+{
+  rtx addr;
+
+  if (!MEM_P (op))
+    return false;
+
+  addr = XEXP (op, 0);
+
+  if (GET_CODE (addr) == PLUS
+      && REG_P (XEXP (addr, 0)) && REG_P (XEXP (addr, 1)))
+    {
+      bool r0_seen = false;
+
+      for (insn = next_real_insn (insn); insn; insn = next_real_insn (insn))
+	{
+	  rtx pat = PATTERN (insn);
+
+	  if (dead_or_set_regno_p (insn, R0_REG))
+	    return false;
+
+	  if (GET_CODE (insn) == JUMP_INSN)
+	    {
+	      /* Must have a jump_insn after the SP test.  Check if R0 is alive here.  */
+	      if (r0_seen)
+		for (insn = NEXT_INSN (JUMP_LABEL (insn));
+		     insn; insn = NEXT_INSN (insn))
+		  if (INSN_P (insn)
+		      && refers_to_regno_p (R0_REG, R0_REG+1, PATTERN (insn), (rtx *)0))
+		    return true;
+
+	      /* No need to check further.  */
+	      return false;
+	    }
+
+	  /* Find stack-protector test. After this point R0 is under pressure.  */
+	  if (GET_CODE (pat) == PARALLEL)
+	    {
+	      pat = XVECEXP (pat, 0, 0);
+	      if (GET_CODE (pat) == SET)
+		{
+		  rtx src = XEXP (pat, 1);
+		  if (GET_CODE (src) == UNSPEC && (XINT (src, 1) == UNSPEC_SP_TEST))
+		    r0_seen = true;
+		}
+	    }
+	}
+    }
+
+  return false;
+}
+
 #include "gt-sh.h"
+
--- gcc/gcc/config/sh/elf.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/elf.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -39,7 +39,6 @@ along with GCC; see the file COPYING3.  If not see
 #undef WCHAR_TYPE_SIZE
 #define WCHAR_TYPE_SIZE 32
 
-
 /* The prefix to add to user-visible assembler symbols.  */
 
 #undef LOCAL_LABEL_PREFIX
@@ -75,7 +74,7 @@ along with GCC; see the file COPYING3.  If not see
 
 #undef STARTFILE_SPEC
 #define STARTFILE_SPEC \
-  "%{!shared: crt1.o%s} crti.o%s \
+  "%{!shared: crt1.o%s trap-handler.o%s} crti.o%s \
    %{!shared:crtbegin.o%s} %{shared:crtbeginS.o%s}"
 
 #undef ENDFILE_SPEC
@@ -88,3 +87,9 @@ along with GCC; see the file COPYING3.  If not see
 /* ASM_OUTPUT_CASE_LABEL is defined in elfos.h.  With it,
    a redundant .align was generated.  */
 #undef  ASM_OUTPUT_CASE_LABEL
+
+#undef MAX_OFILE_ALIGNMENT
+#define MAX_OFILE_ALIGNMENT (((unsigned int) 1 << 20) * 8)
+
+
+
--- gcc/gcc/config/sh/superh.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/superh.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -78,11 +78,6 @@ along with GCC; see the file COPYING3.  If not see
 #undef SUBTARGET_ASM_SPEC
 #define SUBTARGET_ASM_SPEC "%{m4-100*|m4-200*:-isa=sh4} %{m4-400|m4-340:-isa=sh4-nommu-nofpu} %{m4-500:-isa=sh4-nofpu} %(asruntime)"
 
-/* Override the SUBTARGET_ASM_RELAX_SPEC so it doesn't interfere with the
-   runtime support by adding -isa=sh4 in the wrong place.  */
-#undef SUBTARGET_ASM_RELAX_SPEC
-#define SUBTARGET_ASM_RELAX_SPEC "%{!m4-100*:%{!m4-200*:%{!m4-300*:%{!m4-340:%{!m4-400:%{!m4-500:-isa=sh4}}}}}}"
-
 /* Create the CC1_SPEC to add the runtime support */
 #undef CC1_SPEC
 #define CC1_SPEC "%(cc1runtime)"
@@ -90,10 +85,18 @@ along with GCC; see the file COPYING3.  If not see
 #undef CC1PLUS_SPEC
 #define CC1PLUS_SPEC "%(cc1runtime)"
 
-
 /* Override the LIB_SPEC to add the runtime support */
+/* board_link defines both --defsyms and libraries. Libraries should be
+   imported from LIB_SPEC that goes at the end of the command line, but
+   redefines user --defsyms, that should to into LINK_SPEC. For the BSP
+   libraries to be added to the command line, make sure that the board_spec
+   is included.  */
 #undef LIB_SPEC
-#define LIB_SPEC "%{!shared:%{!symbolic:%(libruntime) -lc}} %{pg:-lprofile -lc}"
+#define LIB_SPEC "%{!shared:%{!symbolic:%{pg:-lprofile} \
+%{"PLUGIN_COND": \
+%{!nostdlib:%{!nodefaultlibs:%:pass-through-libs(%(board_link))}} \
+}"PLUGIN_COND_CLOSE" \
+%(libruntime) -lc}}"
 
 /* Override STARTFILE_SPEC to add profiling and MMU support.  */
 #undef STARTFILE_SPEC
@@ -101,4 +104,4 @@ along with GCC; see the file COPYING3.  If not see
   "%{!shared: %{!m4-400*:%{!m4-340*: %{pg:gcrt1-mmu.o%s}%{!pg:crt1-mmu.o%s}}}} \
    %{!shared: %{m4-340*|m4-400*: %{pg:gcrt1.o%s}%{!pg:crt1.o%s}}} \
    crti.o%s \
-   %{!shared:crtbegin.o%s} %{shared:crtbeginS.o%s}"
+   %{!shared:crtbegin.o%s trap-handler.o%s} %{shared:crtbeginS.o%s}"
--- gcc/gcc/config/sh/sh.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/sh.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -4,6 +4,7 @@
    Free Software Foundation, Inc.
    Contributed by Steve Chamberlain (sac@cygnus.com).
    Improved by Jim Wilson (wilson@cygnus.com).
+   Copyright (c) 2011  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -33,6 +34,10 @@ extern int code_for_indirect_jump_scratch;
 
 #define TARGET_CPU_CPP_BUILTINS() \
 do { \
+  if (TARGET_DBHWBUG) \
+      builtin_define_with_value ("DB_ST40300_BUG_WORKAROUND", "32", 0); \
+  if (TARGET_TAS) \
+      builtin_define ("__HAVE_TAS__"); \
   builtin_define ("__sh__"); \
   builtin_assert ("cpu=sh"); \
   builtin_assert ("machine=sh"); \
@@ -133,6 +138,11 @@ do { \
 #define TARGET_FPU_DOUBLE \
   ((target_flags & MASK_SH4) != 0 || TARGET_SH2A_DOUBLE)
 
+#define TARGET_SH1_SOFTFP (TARGET_SH1 && !TARGET_FPU_DOUBLE)
+
+#define TARGET_SH1_SOFTFP_MODE(MODE) \
+  (TARGET_SH1_SOFTFP && (!TARGET_SH2E || (MODE) == DFmode))
+
 /* Nonzero if an FPU is available.  */
 #define TARGET_FPU_ANY (TARGET_SH2E || TARGET_FPU_DOUBLE)
 
@@ -167,13 +177,16 @@ do { \
 /* Nonzero if we should generate code using SHmedia FPU instructions.  */
 #define TARGET_SHMEDIA_FPU (TARGET_SHMEDIA && TARGET_FPU_DOUBLE)
 
+/* builtin_trap can uses abort() by default.  */
+#define TARGET_BUILTIN_TRAPA 0
+
 /* This is not used by the SH2E calling convention  */
 #define TARGET_VARARGS_PRETEND_ARGS(FUN_DECL) \
   (TARGET_SH1 && ! TARGET_SH2E && ! TARGET_SH5 \
    && ! (TARGET_HITACHI || sh_attr_renesas_p (FUN_DECL)))
 
 #ifndef TARGET_CPU_DEFAULT
-#define TARGET_CPU_DEFAULT SELECT_SH1
+#define TARGET_CPU_DEFAULT SELECT_SH4
 #define SUPPORT_SH1 1
 #define SUPPORT_SH2E 1
 #define SUPPORT_SH4 1
@@ -197,7 +210,9 @@ do { \
 #define TARGET_DIVIDE_INV_CALL2 (sh_div_strategy == SH_DIV_INV_CALL2)
 #define TARGET_DIVIDE_CALL_DIV1 (sh_div_strategy == SH_DIV_CALL_DIV1)
 #define TARGET_DIVIDE_CALL_FP (sh_div_strategy == SH_DIV_CALL_FP)
-#define TARGET_DIVIDE_CALL_TABLE (sh_div_strategy == SH_DIV_CALL_TABLE)
+#define TARGET_DIVIDE_CALL_PRE1 (sh_div_strategy == SH_DIV_CALL_PRE1)
+#define TARGET_DIVIDE_CALL_TABLE (sh_div_strategy == SH_DIV_CALL_TABLE \
+   || TARGET_DIVIDE_CALL_PRE1)
 
 #define SELECT_SH1               (MASK_SH1)
 #define SELECT_SH2               (MASK_SH2 | SELECT_SH1)
@@ -230,6 +245,42 @@ do { \
 #define SELECT_SH5_COMPACT       (MASK_SH5 | MASK_SH4 | SELECT_SH3E)
 #define SELECT_SH5_COMPACT_NOFPU (MASK_SH5 | SELECT_SH3)
 
+/* Check if we have support for optimized software floating point using
+   dynamic shifts - then some function calls clobber fewer registers.  */
+#ifdef SUPPORT_SH3
+#define SUPPORT_SH3_OSFP 1
+#else
+#define SUPPORT_SH3_OSFP 0
+#endif
+
+#ifdef SUPPORT_SH3E
+#define SUPPORT_SH3E_OSFP 1
+#else
+#define SUPPORT_SH3E_OSFP 0
+#endif
+
+#if defined(SUPPORT_SH4_NOFPU) || defined(SUPPORT_SH3_OSFP)
+#define SUPPORT_SH4_NOFPU_OSFP 1
+#else
+#define SUPPORT_SH4_NOFPU_OSFP 0
+#endif
+
+#if defined(SUPPORT_SH4_SINGLE_ONLY) || defined (SUPPORT_SH3E_OSFP)
+#define SUPPORT_SH4_SINGLE_ONLY_OSFP 1
+#else
+#define SUPPORT_SH4_SINGLE_ONLY_OSFP 0
+#endif
+
+#ifdef notyet
+#define TARGET_OSFP (0 \
+ || (TARGET_SH3 && !TARGET_SH2E && SUPPORT_SH3_OSFP) \
+ || (TARGET_SH3E && SUPPORT_SH3E_OSFP) \
+ || (TARGET_HARD_SH4 && !TARGET_SH2E && SUPPORT_SH4_NOFPU_OSFP) \
+ || (TARGET_HARD_SH4 && TARGET_SH2E && SUPPORT_SH4_SINGLE_ONLY_OSFP))
+#else
+#define TARGET_OSFP (0)
+#endif
+
 #if SUPPORT_SH1
 #define SUPPORT_SH2 1
 #endif
@@ -290,7 +341,7 @@ do { \
 #endif
 
 #ifndef TARGET_OPT_DEFAULT
-#define TARGET_OPT_DEFAULT  MASK_ADJUST_UNROLL
+#define TARGET_OPT_DEFAULT 0
 #endif
 
 #define TARGET_DEFAULT \
@@ -323,20 +374,13 @@ do { \
   { "subtarget_link_emul_suffix", SUBTARGET_LINK_EMUL_SUFFIX },	\
   { "subtarget_link_spec", SUBTARGET_LINK_SPEC },		\
   { "subtarget_asm_endian_spec", SUBTARGET_ASM_ENDIAN_SPEC },	\
-  { "subtarget_asm_relax_spec", SUBTARGET_ASM_RELAX_SPEC },	\
   { "subtarget_asm_isa_spec", SUBTARGET_ASM_ISA_SPEC },		\
   { "subtarget_asm_spec", SUBTARGET_ASM_SPEC },			\
   SUBTARGET_EXTRA_SPECS
 
-#if TARGET_CPU_DEFAULT & MASK_HARD_SH4
-#define SUBTARGET_ASM_RELAX_SPEC "%{!m1:%{!m2:%{!m3*:%{!m5*:-isa=sh4-up}}}}"
-#else
-#define SUBTARGET_ASM_RELAX_SPEC "%{m4*:-isa=sh4-up}"
-#endif
-
 #define SH_ASM_SPEC \
- "%(subtarget_asm_endian_spec) %{mrelax:-relax %(subtarget_asm_relax_spec)}\
-%(subtarget_asm_isa_spec) %(subtarget_asm_spec)\
+"%(subtarget_asm_endian_spec) %{mrelax:-relax} \
+%(subtarget_asm_isa_spec) %(subtarget_asm_spec) \
 %{m2a:--isa=sh2a} \
 %{m2a-single:--isa=sh2a} \
 %{m2a-single-only:--isa=sh2a} \
@@ -344,7 +388,8 @@ do { \
 %{m5-compact*:--isa=SHcompact} \
 %{m5-32media*:--isa=SHmedia --abi=32} \
 %{m5-64media*:--isa=SHmedia --abi=64} \
-%{m4al:-dsp} %{mcut2-workaround:-cut2-workaround}"
+%{m4al:-dsp} %{mcut2-workaround:-cut2-workaround} \
+%{mtas:--tas}"
 
 #define ASM_SPEC SH_ASM_SPEC
 
@@ -360,14 +405,18 @@ do { \
 /* Strict nofpu means that the compiler should tell the assembler
    to reject FPU instructions. E.g. from ASM inserts.  */
 #if TARGET_CPU_DEFAULT & MASK_HARD_SH4 && !(TARGET_CPU_DEFAULT & MASK_SH_E)
-#define SUBTARGET_ASM_ISA_SPEC "%{!m1:%{!m2:%{!m3*:%{m4-nofpu|!m4*:%{!m5:-isa=sh4-nofpu}}}}}"
+#define SUBTARGET_ASM_ISA_SPEC "%{!m1:%{!m2:%{!m3*:%{m4-nofpu|!m4*:%{!m5:--isa=sh4-nofpu}}}}}"
 #else
 /* If there were an -isa option for sh5-nofpu then it would also go here. */
 #define SUBTARGET_ASM_ISA_SPEC \
- "%{m4-nofpu:-isa=sh4-nofpu} " ASM_ISA_DEFAULT_SPEC
+ "%{m4-nofpu:--isa=sh4-nofpu} " ASM_ISA_DEFAULT_SPEC
 #endif
 #else /* ! STRICT_NOFPU */
-#define SUBTARGET_ASM_ISA_SPEC ASM_ISA_DEFAULT_SPEC
+#define SUBTARGET_ASM_ISA_SPEC "%{m4-nofpu:--isa=sh4-nofpu-up} \
+ %{m4|m4-single*:--isa=sh4-up} \
+ %{m4-300-nofpu:--isa=st40-300-nofpu} \
+ %{m4-300|m4-300-single|m4-300-single-only:--isa=st40-300}" \
+ ASM_ISA_DEFAULT_SPEC
 #endif
 
 #ifndef SUBTARGET_ASM_SPEC
@@ -395,8 +444,14 @@ do { \
 #define ASM_ISA_DEFAULT_SPEC \
 " %{!m1:%{!m2*:%{!m3*:%{!m4*:%{!m5*:" ASM_ISA_SPEC_DEFAULT "}}}}}"
 #else /* !MASK_SH5 */
+#if TARGET_CPU_DEFAULT & MASK_SH4
+#define ASM_ISA_SPEC_DEFAULT "--isa=sh4-up"
+#define ASM_ISA_DEFAULT_SPEC \
+" %{!m1:%{!m2*:%{!m3*:%{!m4*:%{!m5*:" ASM_ISA_SPEC_DEFAULT "}}}}}"
+#else /* !MASK_SH4 */
+#define ASM_ISA_DEFAULT_SPEC ""
+#endif
 #define LINK_DEFAULT_CPU_EMUL ""
-#define ASM_ISA_DEFAULT_SPEC ""
 #endif /* MASK_SH5 */
 
 #define SUBTARGET_LINK_EMUL_SUFFIX ""
@@ -411,6 +466,8 @@ do { \
 %{m5-64media*:64}\
 %{!m1:%{!m2:%{!m3*:%{!m4*:%{!m5*:%(link_default_cpu_emul)}}}}}\
 %(subtarget_link_emul_suffix) \
+%{mdb-page-bug:--db-page-bug} \
+%{shared:-shared} \
 %{mrelax:-relax} %(subtarget_link_spec)"
 
 #ifndef SH_DIV_STR_FOR_SIZE
@@ -457,6 +514,7 @@ enum sh_divide_strategy_e {
   SH_DIV_CALL_DIV1, /* No FPU, medium size, highest latency.  */
   SH_DIV_CALL_FP,     /* FPU needed, small size, high latency.  */
   SH_DIV_CALL_TABLE,  /* No FPU, large size, medium latency. */
+  SH_DIV_CALL_PRE1,  /* Preheader to optimize return 1 cases. */
   SH_DIV_INTRINSIC
 };
 
@@ -571,6 +629,18 @@ extern enum sh_divide_strategy_e sh_div_strategy;
    multiple of this.  */
 #define STRUCTURE_SIZE_BOUNDARY (TARGET_PADSTRUCT ? 32 : 8)
 
+/* Define this macro as an expression for the alignment of a structure
+   (given by STRUCT as a tree node) if the alignment computed in the
+   usual way is COMPUTED and the alignment explicitly specified was
+   SPECIFIED.
+*/
+#define ROUND_TYPE_ALIGN(STRUCT, COMPUTED, SPECIFIED)	\
+    ((TARGET_ALIGN_DOUBLE &&						       \
+      TREE_CODE (STRUCT) == RECORD_TYPE && TYPE_FIELDS (STRUCT) != 0 && \
+      TREE_INT_CST_LOW (TYPE_SIZE (STRUCT)) > 64)	\
+     ? MAX (MAX ((COMPUTED), (SPECIFIED)), 64)		\
+     : MAX ((COMPUTED), (SPECIFIED)))
+
 /* Set this nonzero if move instructions will actually fail to work
    when given unaligned data.  */
 #define STRICT_ALIGNMENT 1
@@ -579,6 +649,8 @@ extern enum sh_divide_strategy_e sh_div_strategy;
 #define LABEL_ALIGN_AFTER_BARRIER(LABEL_AFTER_BARRIER) \
   barrier_align (LABEL_AFTER_BARRIER)
 
+#define JUMP_ALIGN(LABEL) sh_jump_align (LABEL)
+
 #define LOOP_ALIGN(A_LABEL) \
   ((! optimize || TARGET_HARD_SH4 || optimize_size) \
    ? 0 : sh_loop_align (A_LABEL))
@@ -597,12 +669,10 @@ extern enum sh_divide_strategy_e sh_div_strategy;
 #define ADDR_VEC_ALIGN(ADDR_VEC) 2
 
 /* The base two logarithm of the known minimum alignment of an insn length.  */
-#define INSN_LENGTH_ALIGNMENT(A_INSN)					\
-  (NONJUMP_INSN_P (A_INSN)						\
-   ? 1 << TARGET_SHMEDIA						\
-   : JUMP_P (A_INSN) || CALL_P (A_INSN)					\
-   ? 1 << TARGET_SHMEDIA						\
-   : CACHE_LOG)
+/* After a addr_diff_vec:HI the log align is 1.  Update it so the  next
+   insn_current_address can correctly be computed in final.  */
+#define INSN_LENGTH_ALIGNMENT(X) sh_insn_length_alignment (X)
+
 
 /* Standard register usage.  */
 
@@ -1054,6 +1124,7 @@ enum reg_class
 {
   NO_REGS,
   R0_REGS,
+  R0R3_REGS,
   PR_REGS,
   T_REGS,
   MAC_REGS,
@@ -1080,6 +1151,7 @@ enum reg_class
 {			\
   "NO_REGS",		\
   "R0_REGS",		\
+  "R0R3_REGS",		\
   "PR_REGS",		\
   "T_REGS",		\
   "MAC_REGS",		\
@@ -1108,6 +1180,8 @@ enum reg_class
   { 0x00000000, 0x00000000, 0x00000000, 0x00000000, 0x00000000 },	\
 /* R0_REGS:  */								\
   { 0x00000001, 0x00000000, 0x00000000, 0x00000000, 0x00000000 },	\
+/* R0R3_REGS:  */							\
+  { 0x0000000f, 0x00000000, 0x00000000, 0x00000000, 0x00000000 },	\
 /* PR_REGS:  */								\
   { 0x00000000, 0x00000000, 0x00000000, 0x00000000, 0x00040000 },	\
 /* T_REGS:  */								\
@@ -1261,7 +1335,7 @@ extern enum reg_class regno_reg_class[FIRST_PSEUDO
     && ! TARGET_SHMEDIA							\
     && immediate_operand ((X), (MODE))					\
     && ! ((fp_zero_operand (X) || fp_one_operand (X))			\
-	  && (MODE) == SFmode && fldi_ok ()))				\
+	  && (MODE) == SFmode && fldi_ok (true)))				\
    ? R0_REGS								\
    : ((CLASS) == FPUL_REGS						\
       && ((REG_P (X)							\
@@ -1749,12 +1823,13 @@ struct sh_args {
                                            ? 0 : TARGET_SH1)
 
 #define MOVE_BY_PIECES_P(SIZE, ALIGN) \
-  (move_by_pieces_ninsns (SIZE, ALIGN, MOVE_MAX_PIECES + 1) \
-   < (optimize_size ? 2 : ((ALIGN >= 32) ? 16 : 2)))
+     ((TARGET_ALIGN_DOUBLE) ? ((SIZE)*8 <= 64 || ALIGN != 64)	\
+      : (move_by_pieces_ninsns ((SIZE), ALIGN, MOVE_MAX_PIECES + 1)	\
+	 <= (optimize_size ? 2 : ((ALIGN >= 32) ? 16 : 4))))
 
 #define STORE_BY_PIECES_P(SIZE, ALIGN) \
   (move_by_pieces_ninsns (SIZE, ALIGN, STORE_MAX_PIECES + 1) \
-   < (optimize_size ? 2 : ((ALIGN >= 32) ? 16 : 2)))
+   <= (optimize_size ? 2 : ((ALIGN >= 32) ? 16 : 4)))
 
 #define SET_BY_PIECES_P(SIZE, ALIGN) STORE_BY_PIECES_P(SIZE, ALIGN)
 
@@ -1925,13 +2000,16 @@ struct sh_args {
 #define CASE_VECTOR_MODE ((! optimize || TARGET_BIGTABLE) ? SImode : HImode)
 
 #define CASE_VECTOR_SHORTEN_MODE(MIN_OFFSET, MAX_OFFSET, BODY) \
-((MIN_OFFSET) >= 0 && (MAX_OFFSET) <= 127 \
+(mdep_reorg_phase <= SH_FIXUP_PCLOAD ? SImode               \
+ : (MIN_OFFSET) >= 0 && (MAX_OFFSET) <= 127		    \
  ? (ADDR_DIFF_VEC_FLAGS (BODY).offset_unsigned = 0, QImode) \
  : (MIN_OFFSET) >= 0 && (MAX_OFFSET) <= 255 \
  ? (ADDR_DIFF_VEC_FLAGS (BODY).offset_unsigned = 1, QImode) \
  : (MIN_OFFSET) >= -32768 && (MAX_OFFSET) <= 32767 ? HImode \
  : SImode)
 
+#define VARYING_INSN_P(INSN) sh_varying_insn_p(INSN) 
+
 /* Define as C expression which evaluates to nonzero if the tablejump
    instruction expects the table to contain offsets from the address of the
    table.
@@ -1969,7 +2047,7 @@ struct sh_args {
 
 /* Max number of bytes we want move_by_pieces to be able to copy
    efficiently.  */
-#define MOVE_MAX_PIECES (TARGET_SH4 || TARGET_SHMEDIA ? 8 : 4)
+#define MOVE_MAX_PIECES (TARGET_SH1 || TARGET_SH5 ? 8 : 4)
 
 /* Define if operations between registers always perform the operation
    on the full register even if a narrower mode is specified.  */
@@ -2239,9 +2317,13 @@ struct sh_args {
   if ((LOG) != 0)			\
     fprintf ((FILE), "\t.align %d\n", (LOG))
 
+#define ASM_ALIGN_FUNCTION_LOG(DECL) sh_align_function_log(DECL)
+
 /* Globalizing directive for a label.  */
 #define GLOBAL_ASM_OP "\t.global\t"
 
+#define TARGET_ASM_COUNT(TEMP, ALIGNP) sh_asm_count (TEMP, ALIGNP)
+
 /* #define ASM_OUTPUT_CASE_END(STREAM,NUM,TABLE)	    */
 
 /* Output a relative address table.  */
@@ -2357,7 +2439,8 @@ extern int current_function_interrupt;
    sh-dsp parallel processing insns are four bytes long.  */
 
 #define ADJUST_INSN_LENGTH(X, LENGTH)				\
-  (LENGTH) += sh_insn_length_adjustment (X);
+  (LENGTH) += sh_insn_length_adjustment (X, LENGTH);
+
 
 /* Define this macro if it is advisable to hold scalars in registers
    in a wider mode than that declared by the program.  In such cases,
@@ -2387,7 +2470,6 @@ extern int current_function_interrupt;
   (TARGET_HARD_SH4 ? 1	\
    : (TARGET_SH3 || TARGET_SH2A) ? (optimize_size ? 1 : 2) : 20)
 
-
 #define NUM_MODES_FOR_MODE_SWITCHING { FP_MODE_NONE }
 
 #define OPTIMIZE_MODE_SWITCHING(ENTITY) (TARGET_SH4 || TARGET_SH2A_DOUBLE)
@@ -2423,11 +2505,16 @@ extern int current_function_interrupt;
 #define MODE_PRIORITY_TO_MODE(ENTITY, N) \
   ((TARGET_FPU_SINGLE != 0) ^ (N) ? FP_MODE_SINGLE : FP_MODE_DOUBLE)
 
-#define EMIT_MODE_SET(ENTITY, MODE, HARD_REGS_LIVE) \
-  fpscr_set_from_mem ((MODE), (HARD_REGS_LIVE))
+#define EMIT_MODE_SET(ENTITY, MODE, FLIP, HARD_REGS_LIVE) \
+  ((TARGET_SH4A_FP || TARGET_SH4_300)                     \
+   && (FLIP) ? emit_fpu_flip ()                           \
+   : fpscr_set_from_mem ((MODE), (HARD_REGS_LIVE)))
 
+/* Too conservative, if distances are not computed get_attr_length is too
+   much conservative. better let it go and split_branches afterwards.
 #define MD_CAN_REDIRECT_BRANCH(INSN, SEQ) \
   sh_can_redirect_branch ((INSN), (SEQ))
+ */
 
 #define DWARF_FRAME_RETURN_COLUMN \
   (TARGET_SH5 ? DWARF_FRAME_REGNUM (PR_MEDIA_REG) : DWARF_FRAME_REGNUM (PR_REG))
@@ -2475,4 +2562,7 @@ extern int current_function_interrupt;
 /* FIXME: middle-end support for highpart optimizations is missing.  */
 #define high_life_started reload_in_progress
 
+#define TARGET_USES_LEB128 \
+  (! TARGET_RELAX || (!flag_unwind_tables && !flag_exceptions))
+
 #endif /* ! GCC_SH_H */
--- gcc/gcc/config/sh/embed-elf.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/embed-elf.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -2,6 +2,7 @@
    non-Linux embedded targets.
    Copyright (C) 2002, 2003, 2007, 2010, 2011 Free Software Foundation, Inc.
    Contributed by J"orn Rennecke <joern.rennecke@superh.com>
+   Copyright (c) 2009  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -22,15 +23,20 @@ along with GCC; see the file COPYING3.  If not see
 #undef USER_LABEL_PREFIX
 #define USER_LABEL_PREFIX "_"
 
+/* builtin_trap can use trapa.  */
+#undef TARGET_BUILTIN_TRAPA
+#define TARGET_BUILTIN_TRAPA 1
+
 /* While the speed-optimized implementations of udivsi3_i4i / sdivsi3_i4i
    in libgcc are not available for SH2, the space-optimized ones in
    libgcc-Os-4-200 are.  Thus, when not optimizing for space, link
    libgcc-Os-4-200 after libgcc, so that -mdiv=call-table works for -m2.  */
 #define LIBGCC_SPEC "%{!shared: \
-  %{m4-100*:-lic_invalidate_array_4-100} \
-  %{m4-200*:-lic_invalidate_array_4-200} \
-  %{m4-300*|m4-340:-lic_invalidate_array_4a %{!Os: -lgcc-4-300}} \
-  %{m4a*:-lic_invalidate_array_4a}} \
-  %{Os: -lgcc-Os-4-200} \
+  %{!m4-100*:%{!m4-200*:%{!m4-300*:%{!m4a*:-lic_invalidate}}}}	\
+  %{m4-100*:-lic_invalidate_4-100}				\
+  %{m4-200*:-lic_invalidate_4-200}				\
+  %{m4-300*|-m4-340:-lic_invalidate_4a}			        \
+  %{m4a*:-lic_invalidate_4a}}					\
   -lgcc \
-  %{!Os: -lgcc-Os-4-200}"
+  %{Os: -lgcc-Os-4-200}					        \
+  %{!Os: %{m4-300*|-m4-340: -lgcc-4-300} %{!m4-300*:%{!m4-340: -lgcc-4-200}}}"
--- gcc/gcc/config/sh/newlib.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/newlib.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -27,3 +27,5 @@ along with GCC; see the file COPYING3.  If not see
 #undef  NO_IMPLICIT_EXTERN_C
 #define NO_IMPLICIT_EXTERN_C 1
 
+
+
--- gcc/gcc/config/sh/t-sh	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/t-sh	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -22,6 +22,11 @@ sh-c.o: $(srcdir)/config/sh/sh-c.c \
 	$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \
 		$(srcdir)/config/sh/sh-c.c
 
+sh-mem.o: $(srcdir)/config/sh/sh-mem.c \
+  $(CONFIG_H) $(SYSTEM_H) $(TREE_H) $(TM_H) $(TM_P_H) 
+	$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \
+		$(srcdir)/config/sh/sh-mem.c
+
 DEFAULT_ENDIAN = $(word 1,$(TM_ENDIAN_CONFIG))
 OTHER_ENDIAN = $(word 2,$(TM_ENDIAN_CONFIG))
 
--- gcc/gcc/config/sh/sync.md	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/sync.md	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -89,15 +89,11 @@
 ;; operand such as add #imm,Rn.  However, since the original value before
 ;; the operation also needs to be available, this is not so handy.
 
-(define_c_enum "unspec" [
-  UNSPEC_ATOMIC
+(define_constants
+[(UNSPECV_CMPXCHG_1 0)
+  (UNSPECV_CMPXCHG_2 1)
+  (UNSPECV_CMPXCHG_3 2)
 ])
- 
-(define_c_enum "unspecv" [
-  UNSPECV_CMPXCHG_1
-  UNSPECV_CMPXCHG_2
-  UNSPECV_CMPXCHG_3
-])
 
 (define_mode_iterator I124 [QI HI SI])
 
--- gcc/gcc/config/sh/sh-modes.def	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/sh-modes.def	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -22,6 +22,11 @@ PARTIAL_INT_MODE (SI);
 /* PDI mode is used to represent a function address in a target register.  */
 PARTIAL_INT_MODE (DI);
 
+/* For software floating point comparisons.  */
+CC_MODE (CC_FP_NE);
+CC_MODE (CC_FP_GT);
+CC_MODE (CC_FP_UNLT);
+
 /* Vector modes.  */
 VECTOR_MODE  (INT, QI, 2);    /*                 V2QI */
 VECTOR_MODES (INT, 4);        /*            V4QI V2HI */
--- gcc/gcc/config/sh/ieee-754-sf.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/config/sh/ieee-754-sf.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,703 @@
+/* Copyright (C) 2006, 2008, 2009 Free Software Foundation, Inc.
+   Copyright (c) 2010 STMicroelectronics.
+	
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 3, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   Under Section 7 of GPL version 3, you are granted additional
+   permissions described in the GCC Runtime Library Exception, version
+   3.1, as published by the Free Software Foundation.
+
+   You should have received a copy of the GNU General Public License and
+   a copy of the GCC Runtime Library Exception along with this program;
+   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+   <http://www.gnu.org/licenses/>.  */
+
+!! libgcc software floating-point routines for Renesas SH /
+!! STMicroelectronics ST40 CPUs
+!! Contributed by J"orn Rennecke joern.rennecke@st.com
+
+#ifndef __SH_FPU_ANY__
+
+#include "lib1funcs.h"
+#include "insn-constants.h"
+
+/* Single-precision floating-point emulation.
+   We handle NANs, +-infinity, and +-zero.
+   However, we assume that for NANs, the topmost bit of the fraction is set.  */
+#ifdef L_nesf2f
+/* -fno-finite-math-only inline version, T := r4:SF == r5:SF
+	cmp/eq	r4,r5
+	mov	r4,r0
+	bt	0f
+	or	r5,r0
+	add	r0,r0
+	tst	r0,r0	! test for +0.0 == -0.0 ; -0.0 == +0.0
+	0:			*/
+	.balign 4
+	.global GLOBAL(nesf2f)
+	HIDDEN_FUNC(GLOBAL(nesf2f))
+GLOBAL(nesf2f):
+        /* If the raw values are unequal, the result is unequal, unless
+	   both values are +-zero.
+	   If the raw values are equal, the result is equal, unless
+	   the values are NaN.  */
+	cmp/eq	r4,r5
+	mov.l   LOCAL(c_SF_NAN_MASK),r1
+	bt.s	LOCAL(check_nan)
+	not	r4,r0
+	mov	r4,r0
+	or	r5,r0
+	rts
+	add	r0,r0
+LOCAL(check_nan):
+	tst	r1,r0
+	bt.s 	LOCAL(nan)
+	mov	#96,r2
+	shll16  r2
+	xor 	r2,r1
+	tst	r1,r0	
+LOCAL(nan):		
+	rts
+	movt	r0
+	
+	.balign 4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+LOCAL(c_SF_SNAN_MASK):
+	ENDFUNC(GLOBAL(nesf2f))
+#endif /* L_nesf2f */
+
+#ifdef L_unord_sf
+	.balign 4
+	.global GLOBAL(unordsf2)
+	HIDDEN_FUNC(GLOBAL(unordsf2))
+GLOBAL(unordsf2):
+	mov.l	LOCAL(c_SF_NAN_MASK),r1
+	not	r4,r0
+	tst	r1,r0
+	not	r5,r0
+	bt	LOCAL(unord)
+	tst	r1,r0
+LOCAL(unord):
+	rts
+	movt	r0
+	.balign	4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+	ENDFUNC(GLOBAL(unordsf2))
+#endif /* L_unord_sf */
+
+#if defined(L_gtsf2t) || defined(L_gtsf2t_trap)
+/* -fno-finite-math-only inline version, T := r4:SF > r5:SF ? 0 : 1
+	cmp/pz	r4
+	mov	r4,r0
+	bf/s	0f
+	 cmp/hs	r5,r4
+	cmp/ge	r4,r5
+	or	r5,r0
+	bt	0f
+	add	r0,r0
+	tst	r0,r0
+	0:			*/
+#ifdef L_gtsf2t
+#define fun_label GLOBAL(gtsf2t)
+#else
+#define fun_label GLOBAL(gtsf2t_trap)
+#endif
+	.balign 4
+	.global fun_label
+	HIDDEN_FUNC(fun_label)
+fun_label:
+	/* If the raw values compare greater, the result true, unless
+	   any of them is a nan (but infinity is fine), or both values are
+	   +- zero.  Otherwise, the result false.  */
+	mov.l	LOCAL(c_SF_NAN_MASK),r1
+	cmp/pz	r4
+	not	r5,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	r4,r0
+	bt	LOCAL(nan)
+	cmp/gt	r5,r4
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/gt	r4,r1)
+	bf	LOCAL(nan)
+	or	r5,r0
+	rts
+	add	r0,r0
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan)
+	not	r4,r0
+	tst	r1,r0
+	bt	LOCAL(nan)
+	cmp/hi	r4,r5
+#if defined(L_gtsf2t) && defined(DELAYED_BRANCHES)
+LOCAL(check_nan):
+#endif /* DELAYED_BRANCHES */
+	rts
+	movt	r0
+#ifdef L_gtsf2t
+LOCAL(check_nan):
+LOCAL(nan):
+	rts
+	mov	#0,r0
+#else /* ! L_gtsf2t */
+LOCAL(check_nan):
+	SLI(cmp/gt	r4,r1)
+	bf	LOCAL(nan)
+	rts
+	movt	r0
+LOCAL(nan):
+	mov	#0,r0
+	trapa	#0
+#endif /* ! L_gtsf2t */
+	.balign	4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+	ENDFUNC(fun_label)
+#endif /* L_gtsf2t */
+
+#if defined(L_gesf2f) || defined(L_gesf2f_trap)
+/* -fno-finite-math-only inline version, T := r4:SF >= r5:SF */
+	cmp/pz	r5
+	mov	r4,r0
+	bf/s	0f
+	 cmp/hs	r4,r5
+	cmp/ge	r5,r4
+	or	r5,r0
+	bt	0f
+	add	r0,r0
+	tst	r0,r0
+	0:
+#ifdef L_gesf2f
+#define fun_label GLOBAL(gesf2f)
+#else
+#define fun_label GLOBAL(gesf2f_trap)
+#endif
+	.balign 4
+	.global fun_label
+	HIDDEN_FUNC(fun_label)
+fun_label:
+	/* If the raw values compare greater or equal, the result is
+	   true, unless any of them is a nan.  If both are -+zero, the
+	   result is true; otherwise, it is false.
+	   We use 0 as true and nonzero as false for this function.  */
+	mov.l	LOCAL(c_SF_NAN_MASK),r1
+	cmp/pz	r5
+	not	r4,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	r4,r0
+	bt	LOCAL(nan)
+	cmp/gt	r4,r5
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/ge	r1,r5)
+	bt	LOCAL(nan)
+	or	r5,r0
+	rts
+	add	r0,r0
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan)
+	not	r5,r0
+	tst	r1,r0
+	bt	LOCAL(nan)
+	cmp/hi	r5,r4
+#if defined(L_gesf2f) && defined(DELAYED_BRANCHES)
+LOCAL(nan): LOCAL(check_nan):
+#endif
+	rts
+	movt	r0
+#if defined(L_gesf2f) && ! defined(DELAYED_BRANCHES)
+LOCAL(check_nan):
+	cmp/ge	r1,r5
+LOCAL(nan):
+	rts
+	movt	r0
+#endif /* ! DELAYED_BRANCHES */
+#ifdef L_gesf2f_trap
+LOCAL(check_nan):
+	SLI(cmp/ge	r1,r5)
+	bt	LOCAL(nan)
+	rts
+LOCAL(nan):
+	movt	r0
+	trapa	#0
+#endif /* L_gesf2f_trap */
+	.balign	4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+	ENDFUNC(GLOBAL(gesf2f))
+#endif /* L_gesf2f */
+
+#ifndef DYN_SHIFT /* SH1 / SH2 code */
+#ifdef L_addsub_sf
+#include "IEEE-754/addsf3.S"
+#endif /* _addsub_sf */
+
+#ifdef L_mul_sf
+#include "IEEE-754/mulsf3.S"
+#endif /* L_mul_sf */
+
+#ifdef L__fixunssfsi
+#include "IEEE-754/fixunssfsi.S"
+#endif /* L_fixunssfsi */
+
+#ifdef L_sf_to_si
+#include "IEEE-754/fixsfsi.S"
+#endif /* L_sf_to_si */
+
+#ifdef L_usi_to_sf
+#include "IEEE-754/floatunssisf.S"
+#endif /* L_usi_to_sf */
+
+#ifdef L_si_to_sf
+#include "IEEE-754/floatsisf.S"
+#endif /* L_si_to_sf */
+
+#ifdef L_div_sf
+#include "IEEE-754/divsf3.S"
+#endif /* L_div_sf */
+#endif /* ! DYN_SHIFT */
+
+/* The actual arithmetic uses dynamic shift.  Supporting SH1 / SH2 here would
+   make this code too hard to maintain, so if you want to add SH1 / SH2
+   support, do it in a separate copy.  */
+#ifdef DYN_SHIFT
+#ifdef L_addsub_sf
+#include "IEEE-754/m3/addsf3.S"
+#endif /* L_addsub_sf */
+
+#ifdef L_mul_sf
+#include "IEEE-754/m3/mulsf3.S"
+#endif /* L_mul_sf */
+
+#ifdef L_fixunssfsi
+	! What is a bit unusal about this implementation is that the
+	! sign bit influences the result for NANs: for cleared sign bit, you
+	! get UINT_MAX, for set sign bit, you get 0.
+	! However, since the result for NANs is undefined, this should be no
+	! problem.
+	! N.B. This is scheduled both for SH4-200 and SH4-300
+	.balign 4
+	.global GLOBAL(fixunssfsi)
+	FUNC(GLOBAL(fixunssfsi))
+GLOBAL(fixunssfsi):
+	mov.l	LOCAL(max),r2
+	mov	#-23,r1
+	mov	r4,r0
+	shad	r1,r4
+	mov.l	LOCAL(mask),r1
+	add	#-127,r4
+	cmp/ge	r2,r0
+	or	r2,r0
+	bt	LOCAL(retmax)
+	cmp/pz	r4
+	and	r1,r0
+	bf	LOCAL(ret0)
+	add	#-23,r4
+	rts
+	shld	r4,r0
+LOCAL(ret0):
+LOCAL(retmax):
+	rts
+	subc	r0,r0
+	.balign 4
+LOCAL(mask):
+	.long	0x00ffffff
+LOCAL(max):
+	.long	0x4f800000
+	ENDFUNC(GLOBAL(fixunssfsi))
+#endif /* L_fixunssfsi */
+
+#ifdef L_sf_to_si
+	! What is a bit unusal about this implementation is that the
+	! sign bit influences the result for NANs: for cleared sign bit, you
+	! get INT_MAX, for set sign bit, you get INT_MIN.
+	! However, since the result for NANs is undefined, this should be no
+	! problem.
+	! N.B. This is scheduled both for SH4-200 and SH4-300
+	.balign 4
+	.global GLOBAL(fixsfsi)
+	FUNC(GLOBAL(fixsfsi))
+	.balign	4
+GLOBAL(fixsfsi):
+	mov	r4,r0
+	shll	r4
+	mov	#-24,r1
+	bt	LOCAL(neg)
+	mov.l	LOCAL(max),r2
+	shld	r1,r4
+	mov.l	LOCAL(mask),r1
+	add	#-127,r4
+	cmp/pz	r4
+	add	#-23,r4
+	bf	LOCAL(ret0)
+	cmp/gt	r0,r2
+	bf	LOCAL(retmax)
+	and	r1,r0
+	addc	r1,r0
+	rts
+	shld	r4,r0
+
+	.balign	4
+LOCAL(neg):
+	mov.l	LOCAL(min),r2
+	shld	r1,r4
+	mov.l	LOCAL(mask),r1
+	add	#-127,r4
+	cmp/pz	r4
+	add	#-23,r4
+	bf	LOCAL(ret0)
+	cmp/gt	r0,r2
+	bf	LOCAL(retmin)
+	and	r1,r0
+	addc	r1,r0
+	shld	r4,r0	! SH4-200 will start this insn on a new cycle
+	rts
+	neg	r0,r0
+
+	.balign	4
+LOCAL(ret0):
+	rts
+	mov	#0,r0
+
+LOCAL(retmax):
+	mov	#-1,r0
+	rts
+	shlr	r0
+
+LOCAL(retmin):
+	mov	#1,r0
+	rts
+	rotr	r0
+
+	.balign 4
+LOCAL(mask):
+	.long	0x007fffff
+LOCAL(max):
+	.long	0x4f000000
+LOCAL(min):
+	.long	0xcf000000
+	ENDFUNC(GLOBAL(fixsfsi))
+#endif /* L_sf_to_si */
+
+#ifdef L_usi_to_sf
+#include "IEEE-754/m3/floatunssisf.S"
+#endif /* L_usi_to_sf */
+
+#ifdef L_si_to_sf
+#include "IEEE-754/m3/floatsisf.S"
+#endif /* L_si_to_sf */
+
+#ifdef L_div_sf
+#include "IEEE-754/m3/divsf3.S"
+#endif /* L_div_sf */
+
+#ifdef L_hypotf
+	.balign 4
+	.global GLOBAL(hypotf)
+	FUNC(GLOBAL(hypotf))
+GLOBAL(hypotf):
+/* This integer implementation takes 71 to 72 cycles in the main path.
+   This is a bit slower than the SH4 can do this computation using double
+   precision hardware floating point - 57 cycles, or 69 with mode switches.  */
+ /* First, calculate x (r4) as the sum of the square of the fractions -
+    the exponent is calculated separately in r3.
+    Then, calculate sqrt(x) for the fraction by reciproot iteration.
+    We get an 7.5 bit inital value using linear approximation with two slopes
+    that are powers of two.
+    x (- [1. .. 2.)  y0 := 1.25 - x/4 - tab(x)   y (- (0.8 .. 1.0)
+    x (- [2. .. 4.)  y0 := 1.   - x/8 - tab(x)   y (- (0.5 .. 0.8)
+ x is represented with two bits before the point,
+ y with 0 bits before the binary point.
+ Thus, to calculate y0 := 1. - x/8 - tab(x), all you have to do is to shift x
+ right by 1, negate it, and subtract tab(x).  */
+
+ /* y1 := 1.5*y0 - 0.5 * (x * y0) * (y0 * y0)
+    z0 := x * y1
+    z1 := z0 + 0.5 * (y1 - (y1*y1) * z0) */
+
+	mov.l	LOCAL(xff000000),r1
+	add	r4,r4
+	mov	r4,r0
+	add	r5,r5
+	cmp/hs	r5,r4
+	sub	r5,r0
+	mov	#-24,r2
+	bf/s	LOCAL(r5_large)
+	shad	r2,r0
+	mov	r4,r3
+	shll8	r4
+	rotcr	r4
+	tst	#0xe0,r0
+	neg	r0,r0
+	bt	LOCAL(ret_abs_r3)
+	tst	r1,r5
+	shll8	r5
+	bt/s	LOCAL(denorm_r5)
+	cmp/hi	r3,r1
+	dmulu.l	r4,r4
+	bf	LOCAL(inf_nan)
+	rotcr	r5
+	shld	r0,r5
+LOCAL(denorm_r5_done):
+	sts	mach,r4
+	dmulu.l	r5,r5
+	mov.l	r6,@-r15
+	mov	#20,r6
+
+	sts	mach,r5
+LOCAL(add_frac):
+	mova	LOCAL(tab)-32,r0
+	mov.l	r7,@-r15
+	mov.w	LOCAL(x1380),r7
+	and	r1,r3
+	addc	r5,r4
+	mov.w	LOCAL(m25),r2	! -25
+	bf	LOCAL(frac_ok)
+	sub	r1,r3
+	rotcr	r4
+	cmp/eq	r1,r3	! did we generate infinity ?
+	bt	LOCAL(inf_nan)
+	shlr	r4
+	mov	r4,r1
+	shld	r2,r1
+	mov.b	@(r0,r1),r0
+	mov	r4,r1
+	shld	r6,r1
+	bra	LOCAL(frac_low2)
+	sub	r1,r7
+
+LOCAL(frac_ok):
+	mov	r4,r1
+	shld	r2,r1
+	mov.b	@(r0,r1),r1
+	cmp/pz	r4
+	mov	r4,r0
+	bt/s	LOCAL(frac_low)
+	shld	r6,r0
+	mov.w	LOCAL(xf80),r7
+	shlr	r0
+LOCAL(frac_low):
+	sub	r0,r7
+LOCAL(frac_low2):
+	mov.l	LOCAL(x40000080),r0 ! avoid denorm results near 1. << r3
+	sub	r1,r7	! {0.12}
+	mov.l	LOCAL(xfffe0000),r5 ! avoid rounding overflow near 4. << r3
+	swap.w	r7,r1	! {0.28}
+	dmulu.l	r1,r4 /* two issue cycles */
+	mulu.w	r7,r7  /* two issue cycles */
+	sts	mach,r2	! {0.26}
+	mov	r1,r7
+	shlr	r1
+	sts	macl,r6	! {0.24}
+	cmp/hi	r0,r4
+	shlr2	r2
+	bf	LOCAL(near_one)
+	shlr	r2	! {0.23} systemic error of linear approximation keeps y1 < 1
+	dmulu.l	r2,r6
+	cmp/hs	r5,r4
+	add	r7,r1	! {1.28}
+	bt	LOCAL(near_four)
+	shlr2	r1	! {1.26}
+	sts	mach,r0	! {0.15} x*y0^3 == {0.16} 0.5*x*y0^3
+	shlr2	r1	! {1.24}
+	shlr8	r1	! {1.16}
+	sett		! compensate for truncation of subtrahend, keep y1 < 1
+	subc	r0,r1   ! {0.16} y1;  max error about 3.5 ulp
+	swap.w	r1,r0
+	dmulu.l	r0,r4	! { 1.30 }
+	mulu.w	r1,r1
+	sts	mach,r2
+	shlr2	r0
+	sts	macl,r1
+	add	r2,r0
+	mov.l	LOCAL(xff000000),r6
+	add	r2,r0
+	dmulu.l	r1,r2
+	add	#127,r0
+	add	r6,r3	! precompensation for adding leading 1
+	sts	mach,r1
+	shlr	r3
+	mov.l	@r15+,r7
+	sub	r1,r0	! {0.31} max error about 50 ulp (+127)
+	mov.l	@r15+,r6
+	shlr8	r0	! {0.23} max error about 0.7 ulp
+	rts
+	add	r3,r0
+	
+LOCAL(r5_large):
+	mov	r5,r3
+	mov	#-31,r2
+	cmp/ge	r2,r0
+	shll8	r5
+	bf	LOCAL(ret_abs_r3)
+	rotcr	r5
+	tst	r1,r4
+	shll8	r4
+	bt/s	LOCAL(denorm_r4)
+	cmp/hi	r3,r1
+	dmulu.l	r5,r5
+	bf	LOCAL(inf_nan)
+	rotcr	r4
+LOCAL(denorm_r4_done):
+	shld	r0,r4
+	sts	mach,r5
+	dmulu.l	r4,r4
+	mov.l	r6,@-r15
+	mov	#20,r6
+	bra	LOCAL(add_frac)
+	sts	mach,r4
+
+LOCAL(near_one):
+	bra	LOCAL(assemble_sqrt)
+	mov	#0,r0
+LOCAL(near_four):
+	! exact round-to-nearest would add 255.  We add 256 for speed & compactness.
+	mov	r4,r0
+	shlr8	r0
+	add	#1,r0
+	tst	r0,r0
+	addc	r0,r3	! might generate infinity.
+LOCAL(assemble_sqrt):
+	mov.l	@r15+,r7
+	shlr	r3
+	mov.l	@r15+,r6
+	rts
+	add	r3,r0
+LOCAL(inf_nan):
+LOCAL(ret_abs_r3):
+	mov	r3,r0
+	rts
+	shlr	r0
+LOCAL(denorm_r5):
+	bf	LOCAL(inf_nan)
+	tst	r1,r4
+	bt	LOCAL(denorm_both)
+	dmulu.l	r4,r4
+	bra	LOCAL(denorm_r5_done)
+	shld	r0,r5
+LOCAL(denorm_r4):
+	bf	LOCAL(inf_nan)
+	tst	r1,r5
+	dmulu.l	r5,r5
+	bf	LOCAL(denorm_r4_done)
+LOCAL(denorm_both):	! normalize according to r3.
+	extu.w	r3,r2
+	mov.l	LOCAL(c__clz_tab),r0
+	cmp/eq	r3,r2
+	mov	#-8,r2
+	bt	0f
+	tst	r1,r3
+	mov	#-16,r2
+	bt	0f
+	mov	#-24,r2
+0:
+	shld	r2,r3
+	mov.l	r7,@-r15
+#ifdef __pic__
+	add	r0,r3
+	mova	 LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r3),r0
+	add	#32,r2
+	sub	r0,r2
+	shld	r2,r4
+	mov	r2,r7
+	dmulu.l	r4,r4
+	sts.l	pr,@-r15
+	mov	#1,r3
+	bsr	LOCAL(denorm_r5_done)
+	shld	r2,r5
+	mov.l	LOCAL(x01000000),r1
+	neg	r7,r2
+	lds.l	@r15+,pr
+	tst	r1,r0
+	mov.l	@r15+,r7
+	bt	0f
+	add	#1,r2
+	sub	r1,r0
+0:
+	rts
+	shld	r2,r0
+
+LOCAL(m25):
+	.word	-25
+LOCAL(x1380):
+	.word	0x1380
+LOCAL(xf80):
+	.word	0xf80
+	.balign	4
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(x40000080):
+	.long	0x40000080
+LOCAL(xfffe0000):
+	.long	0xfffe0000
+LOCAL(x01000000):
+	.long	0x01000000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+
+/*
+double err(double x)
+{
+  return (x < 2. ? 1.25 - x/4. : 1. - x/8.) - 1./sqrt(x);
+}
+
+int
+main ()
+{
+  int i = 0;
+  double x, s, v;
+  double lx, hx;
+
+  s = 1./32.;
+  for (x = 1.; x < 4; x += s, i++)
+    {
+      lx = x;
+      hx = x + s - 1. / (1 << 30);
+      v = 0.5 * (err (lx) + err (hx));
+      printf ("%s% 4d%c",
+              (i & 7) == 0 ? "\t.byte\t" : "",
+              (int)(v * 4096 + 0.5) - 128,
+              (i & 7) == 7 ? '\n' : ',');
+    }
+  return 0;
+} */
+
+	.balign	4
+LOCAL(tab):
+	.byte	-113, -84, -57, -33, -11,   8,  26,  41
+	.byte	  55,  67,  78,  87,  94, 101, 106, 110
+	.byte	 113, 115, 115, 115, 114, 112, 109, 106
+	.byte	 101,  96,  91,  84,  77,  69,  61,  52
+	.byte	  51,  57,  63,  68,  72,  77,  80,  84
+	.byte	  87,  89,  91,  93,  95,  96,  97,  97
+	.byte	  97,  97,  97,  96,  95,  94,  93,  91
+	.byte	  89,  87,  84,  82,  79,  76,  72,  69
+	.byte	  65,  61,  57,  53,  49,  44,  39,  34
+	.byte	  29,  24,  19,  13,   8,   2,  -4, -10
+	.byte	 -17, -23, -29, -36, -43, -50, -57, -64
+	.byte	 -71, -78, -85, -93,-101,-108,-116,-124
+	ENDFUNC(GLOBAL(hypotf))
+#endif /* L_hypotf */
+#endif /* DYN_SHIFT */
+
+#endif /* __SH_FPU_ANY__ */
--- gcc/gcc/config/sh/sh4-300.md	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/sh4-300.md	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -186,9 +186,9 @@
 ;; Scheduling runs before reorg, so we approximate this by saying that we
 ;; want the call to be paired with a preceding insn.
 ;; In most cases, the insn that loads the address of the call should have
-;; a nonzero latency (mov rn,rm doesn't make sense since we could use rn
+;; a non-zero latency (mov rn,rm doesn't make sense since we could use rn
 ;; for the address then).  Thus, a preceding insn that can be paired with
-;; a call should be eligible for the delay slot.
+;; a call should be elegible for the delay slot.
 ;;
 ;; calls introduce a longisch delay that is likely to flush the pipelines
 ;; of the caller's instructions.  Ordinary functions tend to end with a
--- gcc/gcc/config/sh/constraints.md	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/constraints.md	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -90,6 +90,9 @@
 (define_register_constraint "z" "R0_REGS"
   "R0 register.")
 
+(define_register_constraint "R03" "R0R3_REGS"
+  "R0/R3 registers.")
+
 ;; Integer constraints
 (define_constraint "I06"
   "A signed 6-bit constant, as used in SHmedia beqi, bnei and xori."
@@ -167,12 +170,12 @@
 (define_constraint "G"
   "Double constant 0."
   (and (match_code "const_double")
-       (match_test "fp_zero_operand (op) && fldi_ok ()")))
+       (match_test "fp_zero_operand (op) && fldi_ok (false)")))
 
 (define_constraint "H"
   "Double constant 1."
   (and (match_code "const_double")
-       (match_test "fp_one_operand (op) && fldi_ok ()")))
+       (match_test "fp_one_operand (op) && fldi_ok (false)")))
 
 ;; Extra constraints
 (define_constraint "Q"
--- gcc/gcc/config/sh/sh-mem.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/config/sh/sh-mem.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,507 @@
+/* Output routines for GCC for Renesas / SuperH SH.
+   Copyright (C) 2011.
+   Free Software Foundation, Inc.
+   Copyright (c) 2011 STMicroelectronics.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 3, or (at your option)
+any later version.
+
+GCC is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING3.  If not see
+<http://www.gnu.org/licenses/>.  */
+
+#include "config.h"
+#include "system.h"
+#include "coretypes.h"
+#include "tm.h"
+#include "insn-config.h"
+#include "rtl.h"
+#include "tree.h"
+#include "flags.h"
+#include "expr.h"
+#include "optabs.h"
+#include "function.h"
+#include "regs.h"
+#include "insn-attr.h"
+#include "recog.h"
+#include "integrate.h"
+#include "dwarf2.h"
+#include "tm_p.h"
+#include "target.h"
+#include "basic-block.h"
+
+/* Like force_operand, but guarantees that VALUE ends up in TARGET.  */
+static void
+force_into (rtx value, rtx target)
+{
+  value = force_operand (value, target);
+  if (! rtx_equal_p (value, target))
+    emit_insn (gen_move_insn (target, value));
+}
+
+/* Emit code to perform a block move.  Choose the best method.
+
+   OPERANDS[0] is the destination.
+   OPERANDS[1] is the source.
+   OPERANDS[2] is the size.
+   OPERANDS[3] is the alignment safe to use.  */
+
+int
+expand_block_move (rtx *operands)
+{
+  int align = INTVAL (operands[3]);
+  int constp = (CONST_INT_P (operands[2]));
+  int bytes = (constp ? INTVAL (operands[2]) : 0);
+
+  if (! constp)
+    return 0;
+
+  if ((TARGET_SH4 || TARGET_SH2A_DOUBLE)
+      && TARGET_FMOVD
+      && !optimize_size && align == 8 && bytes > 4)
+  {
+      rtx dest = copy_rtx (operands[0]);
+      rtx src = copy_rtx (operands[1]);
+      rtx src_addr = copy_addr_to_reg (XEXP (src, 0));
+
+      int copied = 0;
+
+      while (copied + 48 <= bytes)
+	{
+	  rtx temp0 = gen_reg_rtx (DFmode);
+	  rtx temp1 = gen_reg_rtx (DFmode);
+	  rtx temp2 = gen_reg_rtx (DFmode);
+	  rtx temp3 = gen_reg_rtx (DFmode);
+	  rtx temp4 = gen_reg_rtx (DFmode);
+	  rtx temp5 = gen_reg_rtx (DFmode);
+
+	  rtx to0 = adjust_address (dest, DFmode, copied);
+	  rtx to1 = adjust_address (dest, DFmode, copied+8);
+	  rtx to2 = adjust_address (dest, DFmode, copied+16);
+	  rtx to3 = adjust_address (dest, DFmode, copied+24);
+	  rtx to4 = adjust_address (dest, DFmode, copied+32);
+	  rtx to5 = adjust_address (dest, DFmode, copied+40);
+
+	  rtx from0 = adjust_automodify_address (src, DFmode, src_addr, copied);
+	  rtx from1 = adjust_automodify_address (src, DFmode, src_addr, copied + 8);
+	  rtx from2 = adjust_automodify_address (src, DFmode, src_addr, copied + 16);
+	  rtx from3 = adjust_automodify_address (src, DFmode, src_addr, copied + 24);
+	  rtx from4 = adjust_automodify_address (src, DFmode, src_addr, copied + 32);
+	  rtx from5 = adjust_automodify_address (src, DFmode, src_addr, copied + 40);
+
+	  emit_move_insn (temp0, from0);
+	  emit_move_insn (src_addr, plus_constant (src_addr, 8));
+
+	  emit_move_insn (temp1, from1);
+	  emit_move_insn (src_addr, plus_constant (src_addr, 8));
+
+	  emit_move_insn (temp2, from2);
+	  emit_move_insn (src_addr, plus_constant (src_addr, 8));
+
+	  emit_move_insn (temp3, from3);
+	  emit_move_insn (src_addr, plus_constant (src_addr, 8));
+
+	  emit_move_insn (temp4, from4);
+	  emit_move_insn (src_addr, plus_constant (src_addr, 8));
+
+	  emit_move_insn (temp5, from5);
+	  emit_move_insn (src_addr, plus_constant (src_addr, 8));
+
+	  emit_move_insn (to0, temp0);
+	  emit_move_insn (to1, temp1);
+	  emit_move_insn (to2, temp2);
+	  emit_move_insn (to3, temp3);
+	  emit_move_insn (to4, temp4);
+	  emit_move_insn (to5, temp5);
+
+	  copied += 48;
+	}
+
+      while (copied + 8 <= bytes)
+	{
+	  rtx temp0 = gen_reg_rtx (DFmode);
+	  rtx to0 = adjust_address (dest, DFmode, copied);
+	  rtx from0 = adjust_automodify_address (src, DFmode, src_addr, copied);
+
+	  emit_move_insn (temp0, from0);
+	  emit_move_insn (src_addr, plus_constant (src_addr, 8));
+	  emit_move_insn (to0, temp0);
+	  copied += 8;
+	}
+
+      if (copied < bytes)
+	move_by_pieces (adjust_address (dest, BLKmode, copied),
+			adjust_automodify_address (src, BLKmode,
+						   src_addr, copied),
+			bytes - copied, align, 0);
+
+      return 1;
+  }
+
+
+  /* If we could use mov.l to move words and dest is word-aligned, we
+     can use movua.l for loads and still generate a relatively short
+     and efficient sequence.  */
+  if (TARGET_SH4A_ARCH && align < 4
+      && MEM_ALIGN (operands[0]) >= 32
+      && can_move_by_pieces (bytes, 32))
+    {
+      rtx dest = copy_rtx (operands[0]);
+      rtx src = copy_rtx (operands[1]);
+      /* We could use different pseudos for each copied word, but
+	 since movua can only load into r0, it's kind of
+	 pointless.  */
+      rtx temp = gen_reg_rtx (SImode);
+      rtx src_addr = copy_addr_to_reg (XEXP (src, 0));
+      int copied = 0;
+
+      while (copied + 4 <= bytes)
+	{
+	  rtx to = adjust_address (dest, SImode, copied);
+	  rtx from = adjust_automodify_address (src, BLKmode,
+						src_addr, copied);
+
+	  set_mem_size (from, 4);
+	  emit_insn (gen_movua (temp, from));
+	  emit_move_insn (src_addr, plus_constant (src_addr, 4));
+	  emit_move_insn (to, temp);
+	  copied += 4;
+	}
+
+      if (copied < bytes)
+	move_by_pieces (adjust_address (dest, BLKmode, copied),
+			adjust_automodify_address (src, BLKmode,
+						   src_addr, copied),
+			bytes - copied, align, 0);
+
+      return 1;
+    }
+
+  /* If it isn't a constant number of bytes, or if it doesn't have 4 byte
+     alignment, or if it isn't a multiple of 4 bytes, then fail.  */
+  if (align < 4 || (bytes % 4 != 0))
+    return 0;
+
+  if (TARGET_HARD_SH4)
+    {
+      if (bytes < 12)
+	return 0;
+      else if (bytes == 12)
+	{
+	  rtx func_addr_rtx = gen_reg_rtx (Pmode);
+	  rtx r4 = gen_rtx_REG (SImode, 4);
+	  rtx r5 = gen_rtx_REG (SImode, 5);
+
+	  function_symbol (func_addr_rtx, "__movmemSI12_i4", SFUNC_STATIC);
+	  force_into (XEXP (operands[0], 0), r4);
+	  force_into (XEXP (operands[1], 0), r5);
+	  emit_insn (gen_block_move_real_i4 (func_addr_rtx));
+	  return 1;
+	}
+      else if (! optimize_size)
+	{
+	  const char *entry_name;
+	  rtx func_addr_rtx = gen_reg_rtx (Pmode);
+	  int dwords;
+	  rtx r4 = gen_rtx_REG (SImode, 4);
+	  rtx r5 = gen_rtx_REG (SImode, 5);
+	  rtx r6 = gen_rtx_REG (SImode, 6);
+
+	  entry_name = (bytes & 4 ? "__movmem_i4_odd" : "__movmem_i4_even");
+	  function_symbol (func_addr_rtx, entry_name, SFUNC_STATIC);
+	  force_into (XEXP (operands[0], 0), r4);
+	  force_into (XEXP (operands[1], 0), r5);
+
+	  dwords = bytes >> 3;
+	  emit_insn (gen_move_insn (r6, GEN_INT (dwords - 1)));
+	  emit_insn (gen_block_lump_real_i4 (func_addr_rtx));
+	  return 1;
+	}
+      else
+	return 0;
+    }
+  if (bytes < 64)
+    {
+      char entry[30];
+      rtx func_addr_rtx = gen_reg_rtx (Pmode);
+      rtx r4 = gen_rtx_REG (SImode, 4);
+      rtx r5 = gen_rtx_REG (SImode, 5);
+
+      sprintf (entry, "__movmemSI%d", bytes);
+      function_symbol (func_addr_rtx, entry, SFUNC_STATIC);
+      force_into (XEXP (operands[0], 0), r4);
+      force_into (XEXP (operands[1], 0), r5);
+      emit_insn (gen_block_move_real (func_addr_rtx));
+      return 1;
+    }
+
+  /* This is the same number of bytes as a memcpy call, but to a different
+     less common function name, so this will occasionally use more space.  */
+  if (! optimize_size)
+    {
+      rtx func_addr_rtx = gen_reg_rtx (Pmode);
+      int final_switch, while_loop;
+      rtx r4 = gen_rtx_REG (SImode, 4);
+      rtx r5 = gen_rtx_REG (SImode, 5);
+      rtx r6 = gen_rtx_REG (SImode, 6);
+
+      function_symbol (func_addr_rtx, "__movmem", SFUNC_STATIC);
+      force_into (XEXP (operands[0], 0), r4);
+      force_into (XEXP (operands[1], 0), r5);
+
+      /* r6 controls the size of the move.  16 is decremented from it
+	 for each 64 bytes moved.  Then the negative bit left over is used
+	 as an index into a list of move instructions.  e.g., a 72 byte move
+	 would be set up with size(r6) = 14, for one iteration through the
+	 big while loop, and a switch of -2 for the last part.  */
+
+      final_switch = 16 - ((bytes / 4) % 16);
+      while_loop = ((bytes / 4) / 16 - 1) * 16;
+      emit_insn (gen_move_insn (r6, GEN_INT (while_loop + final_switch)));
+      emit_insn (gen_block_lump_real (func_addr_rtx));
+      return 1;
+    }
+
+  return 0;
+}
+
+
+/* Emit code to perform a str[n]cmp.  
+
+   OPERANDS[0] is the destination.
+   OPERANDS[1] is the first string.
+   OPERANDS[2] is the second string.
+   OPERANDS[3] is the align.  */
+
+int
+sh4_expand_cmpstr (rtx *operands)
+{ 
+  rtx s1 = copy_rtx (operands[1]);
+  rtx s2 = copy_rtx (operands[2]);
+  rtx s1_addr = copy_addr_to_reg (XEXP (s1, 0));
+  rtx s2_addr = copy_addr_to_reg (XEXP (s2, 0));
+  rtx tmp0 = gen_reg_rtx (SImode);
+  rtx tmp1 = gen_reg_rtx (SImode);
+  rtx tmp2 = gen_reg_rtx (SImode);
+  rtx low_1 = gen_lowpart (HImode, tmp1);
+  rtx low_2 = gen_lowpart (HImode, tmp2);
+  rtx tmp3 = gen_reg_rtx (SImode);
+  rtx tmp4 = gen_reg_rtx (SImode);
+
+  rtx addr1 = adjust_automodify_address (s1, QImode, s1_addr, 0);
+  rtx addr2 = adjust_automodify_address (s2, QImode, s2_addr, 0);
+
+  rtx addr1_l = adjust_automodify_address (s1, SImode, s1_addr, 0);
+  rtx addr2_l = adjust_automodify_address (s2, SImode, s2_addr, 0);
+
+  rtx end_byte = gen_label_rtx ();
+
+  rtx ret = gen_label_rtx ();
+  rtx loop_long = gen_label_rtx ();
+  rtx loop_end = gen_label_rtx ();
+  rtx loop_end1 = gen_label_rtx ();
+  rtx loop_byte = gen_label_rtx ();
+  rtx loop_byte11 = gen_label_rtx ();
+  rtx loop_byte01 = gen_label_rtx ();
+#if 0
+  rtx loop_byte12 = gen_label_rtx ();
+#endif
+  rtx jump;
+
+  emit_move_insn (tmp2, s2_addr);
+  emit_move_insn (tmp0, GEN_INT (3));
+  emit_insn (gen_tstsi_t (tmp0, tmp2));
+  jump = emit_jump_insn (gen_branch_false (loop_byte));
+  add_reg_note (jump, REG_BR_PROB, GEN_INT (REG_BR_PROB_BASE / 4));
+
+  /* tmp2 is aligned, OK to load. */
+  emit_move_insn (tmp2, addr2_l); 
+  emit_move_insn (s2_addr, plus_constant (s2_addr, 4));
+
+  emit_move_insn (tmp1, s1_addr);
+  emit_insn (gen_tstsi_t (tmp0, tmp1));
+      
+  emit_move_insn (tmp0, const0_rtx);
+
+  jump = emit_jump_insn (gen_branch_false (loop_byte01));
+  add_reg_note (jump, REG_BR_PROB, GEN_INT (REG_BR_PROB_BASE / 4));
+
+  /* tmp1 is aligned, OK to load. */
+  emit_move_insn (tmp1, addr1_l); 
+  emit_move_insn (s1_addr, plus_constant (s1_addr, 4));
+
+  /* first iteration out of the loop */
+  /* tmp1, tmp2 ready. */
+  emit_insn (gen_cmpstr_t (tmp0, tmp2));
+
+  jump = emit_jump_insn (gen_branch_true (loop_byte11));
+  add_reg_note (jump, REG_BR_PROB, GEN_INT (REG_BR_PROB_BASE / 4));
+
+  emit_insn (gen_cmpeqsi_t (tmp1, tmp2));
+
+  jump = emit_jump_insn (gen_branch_false (loop_end));
+  add_reg_note (jump, REG_BR_PROB, GEN_INT (REG_BR_PROB_BASE / 4));
+
+  /* ------ gcc/start loop -------------- gcc/*/
+  emit_label (loop_long);
+  
+  emit_move_insn (tmp4, addr2_l); 
+  emit_move_insn (s2_addr, plus_constant (s2_addr, 4));
+
+  emit_move_insn (tmp3, addr1_l); 
+  emit_move_insn (s1_addr, plus_constant (s1_addr, 4));
+
+  /* --- gcc/tmp3, tmp4 ready */
+  emit_insn (gen_cmpstr_t (tmp0, tmp4));
+  jump = emit_jump_insn (gen_branch_true (loop_byte11));
+  add_reg_note (jump, REG_BR_PROB, GEN_INT (REG_BR_PROB_BASE / 10));
+
+  emit_move_insn (tmp2, addr2_l); 
+  emit_move_insn (s2_addr, plus_constant (s2_addr, 4));
+
+  emit_insn (gen_cmpeqsi_t (tmp3, tmp4));
+  jump = emit_jump_insn (gen_branch_false (loop_end1));
+  add_reg_note (jump, REG_BR_PROB, GEN_INT (REG_BR_PROB_BASE / 10));
+
+  emit_move_insn (tmp1, addr1_l); 
+  emit_move_insn (s1_addr, plus_constant (s1_addr, 4));
+
+  /* --- gcc/tmp1, tmp2 ready */
+  emit_insn (gen_cmpstr_t (tmp0, tmp2));
+  jump = emit_jump_insn (gen_branch_true (loop_byte11));
+  add_reg_note (jump, REG_BR_PROB, GEN_INT (REG_BR_PROB_BASE / 10));
+
+  emit_move_insn (tmp4, addr2_l); 
+  emit_move_insn (s2_addr, plus_constant (s2_addr, 4));
+
+  emit_insn (gen_cmpeqsi_t (tmp1, tmp2));
+  jump = emit_jump_insn (gen_branch_false (loop_end));
+  add_reg_note (jump, REG_BR_PROB, GEN_INT (REG_BR_PROB_BASE / 10));
+
+  emit_move_insn (tmp3, addr1_l); 
+  emit_move_insn (s1_addr, plus_constant (s1_addr, 4));
+
+  /* --- gcc/tmp3, tmp4 ready */
+  emit_insn (gen_cmpstr_t (tmp0, tmp4));
+  jump = emit_jump_insn (gen_branch_true (loop_byte11));
+  add_reg_note (jump, REG_BR_PROB, GEN_INT (REG_BR_PROB_BASE / 10));
+
+  emit_move_insn (tmp2, addr2_l); 
+  emit_move_insn (s2_addr, plus_constant (s2_addr, 4));
+
+  emit_insn (gen_cmpeqsi_t (tmp3, tmp4));
+  jump = emit_jump_insn (gen_branch_false (loop_end1));
+  add_reg_note (jump, REG_BR_PROB, GEN_INT (REG_BR_PROB_BASE / 10));
+
+  emit_move_insn (tmp1, addr1_l); 
+  emit_move_insn (s1_addr, plus_constant (s1_addr, 4));
+
+  /* --- gcc/tmp1, tmp2 ready */
+  emit_insn (gen_cmpstr_t (tmp0, tmp2));
+  jump = emit_jump_insn (gen_branch_true (loop_byte11));
+  add_reg_note (jump, REG_BR_PROB, GEN_INT (REG_BR_PROB_BASE / 10));
+
+  emit_insn (gen_cmpeqsi_t (tmp1, tmp2));
+
+  /* speculated from falltru. */
+  if (TARGET_LITTLE_ENDIAN) 
+    emit_insn (gen_rotlhi3_8 (low_1, low_1));
+
+  jump = emit_jump_insn (gen_branch_true (loop_long));
+  add_reg_note (jump, REG_BR_PROB, GEN_INT (REG_BR_PROB_BASE / 10));
+  /* end loop */
+
+  /* fall thru. */
+  if (TARGET_LITTLE_ENDIAN) 
+    {
+      emit_insn (gen_rotlhi3_8 (low_2, low_2));
+      emit_insn (gen_rotlsi3_16 (tmp1, tmp1));
+      emit_insn (gen_rotlsi3_16 (tmp2, tmp2));
+      emit_insn (gen_rotlhi3_8 (low_1, low_1));
+      emit_insn (gen_rotlhi3_8 (low_2, low_2));
+    }
+  
+  emit_move_insn (operands[0], GEN_INT (-1));
+  emit_insn (gen_cmpgtusi_t (tmp2, tmp1));
+  emit_insn (gen_rotcr_t (operands[0]));
+  jump = emit_jump_insn (gen_jump_compact (ret));
+  emit_barrier_after (jump);
+
+  /* last long words not equal, but no zero inside. */
+  /* we have the values, can just walk the bytes */
+  emit_label (loop_end);
+  if (TARGET_LITTLE_ENDIAN) 
+    {
+      emit_insn (gen_rotlhi3_8 (low_1, low_1));
+      emit_insn (gen_rotlhi3_8 (low_2, low_2));
+      emit_insn (gen_rotlsi3_16 (tmp1, tmp1));
+      emit_insn (gen_rotlsi3_16 (tmp2, tmp2));
+      emit_insn (gen_rotlhi3_8 (low_1, low_1));
+      emit_insn (gen_rotlhi3_8 (low_2, low_2));
+    }
+  
+  emit_move_insn (operands[0], GEN_INT (-1));
+  emit_insn (gen_cmpgtusi_t (tmp2, tmp1));
+  emit_insn (gen_rotcr_t (operands[0]));
+  jump = emit_jump_insn (gen_jump_compact (ret));
+  emit_barrier_after (jump);
+
+  emit_label (loop_end1);
+  emit_move_insn (tmp1, tmp3);
+  emit_move_insn (tmp2, tmp4);
+  jump = emit_jump_insn (gen_jump_compact (loop_end));
+  emit_barrier_after (jump);
+
+  emit_label (loop_byte01);
+  emit_move_insn (s2_addr, plus_constant (s2_addr, -4));
+  jump = emit_jump_insn (gen_jump_compact (loop_byte));
+  emit_barrier_after (jump);
+
+#if 0
+  emit_label (loop_byte12);
+  emit_move_insn (s1_addr, plus_constant (s1_addr, -4));
+  emit_move_insn (s2_addr, plus_constant (s2_addr, -8));
+  jump = emit_jump_insn (gen_jump_compact (loop_byte));
+  emit_barrier_after (jump);
+#endif
+
+  emit_label (loop_byte11);
+  emit_move_insn (s1_addr, plus_constant (s1_addr, -4));
+  emit_move_insn (s2_addr, plus_constant (s2_addr, -4));
+
+  emit_label (loop_byte);
+
+  /* start loop byte */
+  emit_insn (gen_extendqisi2 (tmp1, addr1));
+  emit_move_insn (s1_addr, plus_constant (s1_addr, 1));
+  emit_insn (gen_extendqisi2 (tmp2, addr2));
+  emit_move_insn (s2_addr, plus_constant (s2_addr, 1));
+  emit_insn (gen_cmpeqsi_t (tmp1, const0_rtx));
+  jump = emit_jump_insn (gen_branch_true (end_byte));
+  add_reg_note (jump, REG_BR_PROB, GEN_INT (REG_BR_PROB_BASE / 4));
+  emit_insn (gen_cmpeqsi_t (tmp1, tmp2));
+  emit_jump_insn (gen_branch_true (loop_byte));
+
+  /* end loop byte */
+
+  emit_label (end_byte);
+
+  emit_insn (gen_zero_extendqisi2 (tmp1, gen_lowpart (QImode, tmp1)));
+  emit_insn (gen_zero_extendqisi2 (tmp2, gen_lowpart (QImode, tmp2)));
+
+  emit_insn (gen_subsi3 (operands[0], tmp1, tmp2));
+
+  emit_label (ret);
+
+  return 1;
+}
+
--- gcc/gcc/config/sh/sh.md	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/sh.md	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -4,6 +4,7 @@
 ;;  Free Software Foundation, Inc.
 ;;  Contributed by Steve Chamberlain (sac@cygnus.com).
 ;;  Improved by Jim Wilson (wilson@cygnus.com).
+;;  Copyright (c) 2009  STMicroelectronics.
 
 ;; This file is part of GCC.
 
@@ -33,9 +34,6 @@
 ;; ??? The MAC.W and MAC.L instructions are not supported.  There is no
 ;; way to generate them.
 
-;; ??? The cmp/str instruction is not supported.  Perhaps it can be used
-;; for a str* inline function.
-
 ;; BSR is not generated by the compiler proper, but when relaxing, it
 ;; generates .uses pseudo-ops that allow linker relaxation to create
 ;; BSR.  This is actually implemented in bfd/{coff,elf32}-sh.c
@@ -47,6 +45,8 @@
 ;;    l -- pr
 ;;    z -- r0
 ;;
+;;    R03 -- r0, r1, r2 or r3  - experimental constraint for SH4-300
+;;
 ;; Special formats used for outputting SH instructions:
 ;;
 ;;   %.  --  print a .s if insn needs delay slot
@@ -107,6 +107,7 @@
   (DR0_REG	64)
   (DR2_REG	66)
   (DR4_REG	68)
+  (FR4_REG	68)
   (FR23_REG	87)
 
   (TR0_REG	128)
@@ -150,7 +151,6 @@
   (UNSPEC_DIV_INV_TABLE	37)
   (UNSPEC_ASHIFTRT	35)
   (UNSPEC_THUNK		36)
-  (UNSPEC_CHKADD	38)
   (UNSPEC_SP_SET	40)
   (UNSPEC_SP_TEST	41)
   (UNSPEC_MOVUA		42)
@@ -166,6 +166,16 @@
   ;; (unspec [OFFSET ANCHOR] UNSPEC_PCREL_SYMOFF) == OFFSET - (ANCHOR - .).
   (UNSPEC_PCREL_SYMOFF	46)
 
+  (UNSPEC_BUILTIN_ROUND	47)
+  (UNSPEC_ATOMIC        48)
+
+  (UNSPEC_STR		49)
+  (UNSPEC_ROT		50)
+
+  (UNSPECV_SP_SWITCH_B 51)
+  (UNSPECV_SP_SWITCH_E 52)
+
+
   ;; These are used with unspec_volatile.
   (UNSPECV_BLOCKAGE	0)
   (UNSPECV_ALIGN	1)
@@ -175,8 +185,25 @@
   (UNSPECV_WINDOW_END	10)
   (UNSPECV_CONST_END	11)
   (UNSPECV_EH_RETURN	12)
+  (UNSPECV_DB_INSN	13)
+
+  ;; NaN handling for software floating point:
+  ;; We require one bit specific for a precision to be set in all NaNs,
+  ;; so that we can test them with a not / tst sequence.
+  ;; ??? Ironically, this is the quiet bit for now, because that is the
+  ;; only bit set by __builtin_nan ("").
+  ;; ??? Should really use one bit lower and force it set by using
+  ;; a custom encoding function.
+  (SF_NAN_MASK		0x7fc00000)
+  (DF_NAN_MASK		0x7ff80000)
 ])
 
+;; <optab> expands to the name of the optab for a particular code.
+
+(define_code_iterator any_return [return simple_return])
+;;(define_code_attr code [(return "return")
+;;			 (simple_return "simple_return")])
+
 ;; -------------------------------------------------------------------------
 ;; Attributes
 ;; -------------------------------------------------------------------------
@@ -760,6 +787,14 @@
 	cmp/eq	%1,%0"
    [(set_attr "type" "mt_group")])
 
+(define_insn "fpcmp_i1"
+  [(set (reg:SI T_REG)
+	(match_operator:SI 1 "soft_fp_comparison_operator"
+	  [(match_operand 0 "soft_fp_comparison_operand" "r") (const_int 0)]))]
+  "TARGET_SH1_SOFTFP"
+  "tst	%0,%0"
+   [(set_attr "type" "mt_group")])
+
 (define_insn "cmpgtsi_t"
   [(set (reg:SI T_REG)
 	(gt:SI (match_operand:SI 0 "arith_reg_operand" "r,r")
@@ -881,18 +916,16 @@
     }
 }")
 
-(define_insn_and_split "cbranchdi4_i"
+(define_split
   [(set (pc)
 	(if_then_else (match_operator 0 "comparison_operator"
-			[(match_operand:DI 1 "arith_operand" "r,r")
-			 (match_operand:DI 2 "arith_operand" "rN,I08")])
+			[(match_operand:DI 1 "arith_operand" "")
+			 (match_operand:DI 2 "arith_operand" "")])
 		      (label_ref (match_operand 3 "" ""))
 		      (pc)))
-   (clobber (match_scratch:SI 4 "=X,&r"))
+   (clobber (match_scratch:SI 4 ""))
    (clobber (reg:SI T_REG))]
-  "TARGET_CBRANCHDI4"
-  "#"
-  "&& reload_completed"
+  "TARGET_CBRANCHDI4 && reload_completed"
   [(pc)]
   "
 {
@@ -1299,7 +1332,7 @@
 
 (define_insn "*movsicc_t_false"
   [(set (match_operand:SI 0 "arith_reg_dest" "=r,r")
-	(if_then_else (eq (reg:SI T_REG) (const_int 0))
+	(if_then_else:SI (eq (reg:SI T_REG) (const_int 0))
 		      (match_operand:SI 1 "general_movsrc_operand" "r,I08")
 		      (match_operand:SI 2 "arith_reg_operand" "0,0")))]
   "TARGET_PRETEND_CMOVE
@@ -1312,7 +1345,7 @@
 
 (define_insn "*movsicc_t_true"
   [(set (match_operand:SI 0 "arith_reg_dest" "=r,r")
-	(if_then_else (ne (reg:SI T_REG) (const_int 0))
+	(if_then_else:SI (ne (reg:SI T_REG) (const_int 0))
 		      (match_operand:SI 1 "general_movsrc_operand" "r,I08")
 		      (match_operand:SI 2 "arith_reg_operand" "0,0")))]
   "TARGET_PRETEND_CMOVE
@@ -1415,6 +1448,65 @@
     }
 }")
 
+;; Combiner can transform add sequences into
+;;  r1 = r0 + x
+;;  r2 = r1 + x
+;;  r3 = r2 + x
+;;  r4 = r3 + x
+;;  =>
+;;  r2 = r0 + 2x
+;;  r4 = r2 + 2x
+;;  =>
+;;  r4 = r0 + 4x
+
+(define_insn_and_split "*muladdsi3"
+  [(set (match_operand:SI 0 "arith_reg_dest" "=r")
+        (plus:SI (mult:SI (match_operand:SI 1 "arith_reg_operand" "%r")
+			  (match_operand:SI 2 "const_int_operand" "i"))
+	  (match_operand:SI 3 "arith_operand" "rI08")))
+   (clobber (reg:SI MACL_REG))]
+  "TARGET_SH2
+    && ! (REG_P(operands[0]) && REG_POINTER (operands[0]))
+    && ! (REG_P(operands[1]) && REG_POINTER (operands[1]))"
+  "#"
+  "&& can_create_pseudo_p ()"
+  [(set (match_dup 0) (plus:SI (match_dup 4) (match_dup 3)))]
+  "
+{
+   rtx tmp = gen_reg_rtx (SImode);
+   int n = INTVAL (operands[2]);
+   operands[4] = gen_reg_rtx (SImode);
+
+   if ((exact_log2 (n) == -1 && n > 16) || n < 0)
+   {
+      rtx macl = gen_rtx_REG (SImode, MACL_REG);
+      emit_move_insn (tmp, operands[2]);
+      emit_insn (gen_mul_l (tmp, operands[1]));
+      emit_insn (gen_movsi_i (operands[4], macl));
+   }
+   else
+   {
+      int log;
+
+      log = floor_log2 (n);
+      gcc_assert (log > 0);
+
+      emit_insn (gen_ashlsi3_std (operands[4], operands[1], GEN_INT (log)));
+
+      while ((n = (n - (1 << log))))
+      {
+          log = floor_log2 (n);
+
+          emit_move_insn (tmp, operands[1]);
+	  if (log)
+             emit_insn (gen_ashlsi3_std (tmp, tmp, GEN_INT (log)));
+
+	  emit_insn (gen_addsi3 (operands[4], operands[4], tmp));
+      }
+   }
+}"
+  [(set_attr "type" "arith")])
+
 (define_insn "*adddi3_media"
   [(set (match_operand:DI 0 "arith_reg_dest" "=r,r")
 	(plus:DI (match_operand:DI 1 "arith_reg_operand" "%r,r")
@@ -2140,6 +2232,7 @@
   "
 {
   rtx last;
+  rtx lab2 = NULL_RTX;
 
   operands[3] = gen_reg_rtx (Pmode);
   /* Emit the move of the address to a pseudo outside of the libcall.  */
@@ -2278,9 +2371,44 @@
       function_symbol (operands[3], sh_divsi3_libfunc, SFUNC_GOT);
       last = gen_divsi3_i1 (operands[0], operands[3]);
     }
+
+   if (TARGET_DIVIDE_CALL_PRE1) 
+   {
+      rtx tmp = gen_reg_rtx (SImode);
+      rtx lab = gen_label_rtx ();
+      rtx jump;
+
+      lab2 = gen_label_rtx ();
+
+      operands[1] = force_reg (SImode, operands[1]);
+      operands[2] = force_reg (SImode, operands[2]);
+
+      emit_move_insn (tmp, operands[1]);
+      emit_insn (gen_iorsi3 (tmp, tmp, operands[2]));
+      emit_insn (gen_ashlsi3_k (tmp, tmp, GEN_INT (1)));
+      emit_jump_insn (gen_branch_true (lab));
+      emit_move_insn (tmp, operands[1]); 
+      emit_insn (gen_subc (tmp, tmp, operands[2]));
+
+      emit_jump_insn (gen_branch_true (lab));
+
+      emit_insn (gen_subc (tmp, tmp, operands[2]));
+      emit_jump_insn (gen_branch_false (lab));
+
+      emit_move_insn (operands[0], GEN_INT (1));
+      jump = emit_jump_insn (gen_jump_compact (lab2));
+      emit_barrier_after (jump);
+
+      emit_label (lab);
+    }
+
   emit_move_insn (gen_rtx_REG (SImode, 4), operands[1]);
   emit_move_insn (gen_rtx_REG (SImode, 5), operands[2]);
   emit_insn (last);
+
+  if (TARGET_DIVIDE_CALL_PRE1)
+    emit_label (lab2);
+
   DONE;
 }")
 
@@ -2295,8 +2423,7 @@
 				    (match_operand:DI 2 "register_operand" "r")]
 			 UNSPEC_DIV_INV_TABLE)))]
   "TARGET_SHMEDIA"
-  "@
-	ldx.ub	%1, %2, %0"
+  "ldx.ub	%1, %2, %0"
   [(set_attr "type" "load_media")
    (set_attr "highpart" "user")])
 
@@ -2307,8 +2434,7 @@
 				    (match_operand:DI 2 "register_operand" "r")]
 			 UNSPEC_DIV_INV_TABLE)))]
   "TARGET_SHMEDIA"
-  "@
-	ldx.w	%1, %2, %0"
+  "ldx.w	%1, %2, %0"
   [(set_attr "type" "load_media")
    (set_attr "highpart" "user")])
 
@@ -2854,6 +2980,16 @@ label:
   "mul.l	%1,%0"
   [(set_attr "type" "dmpy")])
 
+(define_insn "mulr03"
+  [(set (match_operand:SI 0 "arith_reg_operand" "=r")
+	(mult:SI (match_operand:SI 1 "arith_reg_operand" "%0")
+		 (match_operand:SI 2 "arith_reg_operand" "R03")))]
+  "TARGET_R0R3_TO_REG_MUL - !reload_completed >= 1"
+  "mulr	%2,%0"
+  [(set_attr "type" "dmpy")])
+
+;; ??? should we also use mulr if we'd need two reg-reg copies?
+
 (define_expand "mulsi3"
   [(set (reg:SI MACL_REG)
 	(mult:SI  (match_operand:SI 1 "arith_reg_operand" "")
@@ -2863,8 +2999,13 @@ label:
   "TARGET_SH1"
   "
 {
-  if (!TARGET_SH2)
+    if (TARGET_R0R3_TO_REG_MUL == 2)
     {
+       emit_insn (gen_mulr03 (operands[0], operands[1], operands[2]));
+       DONE;
+    }
+   else if (!TARGET_SH2)
+    {
       /* The address must be set outside the libcall,
 	 since it goes into a pseudo.  */
       rtx sym = function_symbol (NULL, \"__mulsi3\", SFUNC_STATIC);
@@ -3546,7 +3687,7 @@ label:
   DONE;
 }")
 
-(define_insn "*rotlhi3_8"
+(define_insn "rotlhi3_8"
   [(set (match_operand:HI 0 "arith_reg_dest" "=r")
 	(rotate:HI (match_operand:HI 1 "arith_reg_operand" "r")
 		   (const_int 8)))]
@@ -3568,6 +3709,17 @@ label:
 ;;
 ;; shift left
 
+(define_insn "ashlsi3_k"
+  [(set (match_operand:SI 0 "arith_reg_dest" "=r")
+	(ashift:SI (match_operand:SI 1 "arith_reg_operand" "0")
+		   (match_operand:SI 2 "const_int_operand" "M")))
+   (set (reg:SI T_REG)
+	(lt:SI (match_dup 1) (const_int 0)))]
+  "TARGET_SH1"
+  "shal	%0"
+  [(set_attr "type" "arith")])
+
+
 ;; This pattern is used by init_expmed for computing the costs of shift
 ;; insns.
 
@@ -3733,44 +3885,28 @@ label:
 ;; code, so just let the machine independent code widen the mode.
 ;; That's why we don't have ashrhi3_k / lshrhi3_k / lshrhi3_m / lshrhi3 .
 
-
-;; ??? This should be a define expand.
-
-(define_insn "ashrsi2_16"
-  [(set (match_operand:SI 0 "arith_reg_dest" "=r")
-        (ashiftrt:SI (match_operand:SI 1 "arith_reg_operand" "r")
-                     (const_int 16)))]
-  "TARGET_SH1"
-  "#"
-  [(set_attr "length" "4")])
-
-(define_split
-  [(set (match_operand:SI 0 "arith_reg_dest" "")
+(define_expand "ashrsi2_16"
+  [(parallel [(set (match_operand:SI 0 "arith_reg_dest" "")
         (ashiftrt:SI (match_operand:SI 1 "arith_reg_operand" "")
-		     (const_int 16)))]
+				(const_int 16)))
+	      (clobber (reg:SI T_REG))])]
   "TARGET_SH1"
-  [(set (match_dup 0) (rotate:SI (match_dup 1) (const_int 16)))
-   (set (match_dup 0) (sign_extend:SI (match_dup 2)))]
-  "operands[2] = gen_lowpart (HImode, operands[0]);")
+  "
+{
+  rtx low0 = gen_lowpart (HImode, operands[0]);
 
-;; ??? This should be a define expand.
+  emit_insn (gen_rotlsi3_16 (operands[0], operands[1]));
+  emit_insn (gen_extendhisi2 (operands[0], low0));
+  DONE;
+}
+")
 
-(define_insn "ashrsi2_31"
-  [(set (match_operand:SI 0 "arith_reg_dest" "=r")
+(define_expand "ashrsi2_31"
+  [(parallel [(set (match_operand:SI 0 "arith_reg_dest" "=r")
 	(ashiftrt:SI (match_operand:SI 1 "arith_reg_operand" "0")
 		     (const_int 31)))
-   (clobber (reg:SI T_REG))]
+	      (clobber (reg:SI T_REG))])]
   "TARGET_SH1"
-  "#"
-  [(set_attr "length" "4")])
-
-(define_split
-  [(set (match_operand:SI 0 "arith_reg_dest" "")
-	(ashiftrt:SI (match_operand:SI 1 "arith_reg_operand" "")
-		     (const_int 31)))
-   (clobber (reg:SI T_REG))]
-  "TARGET_SH1"
-  [(const_int 0)]
   "
 {
   emit_insn (gen_ashlsi_c (operands[0], operands[1]));
@@ -4960,12 +5096,22 @@ label:
   "")
 
 (define_insn "pop_fpul"
-  [(set (reg:SF FPUL_REG) (mem:SF (post_inc:SI (reg:SI SP_REG))))]
-  "TARGET_SH2E && ! TARGET_SH5"
+  [(parallel [(set (reg:SF FPUL_REG) (mem:SF (post_inc:SI (reg:SI SP_REG))))
+	      (clobber (scratch:SI))])]
+  "TARGET_SH1 && ! TARGET_SH5"
   "lds.l	@r15+,fpul"
   [(set_attr "type" "load")
    (set_attr "hit_stack" "yes")])
 
+(define_insn "pop_fpul2"
+  [(parallel [(set (reg:SF FPUL_REG)
+	(mem:SF (post_inc:SI (match_operand:SI 0 "register_operand" "r"))))
+	      (clobber (scratch:SI))])]
+  ""
+  "lds.l      @%0+,fpul"
+  [(set_attr "type" "load")
+   (set_attr "hit_stack" "no")])
+
 (define_expand "pop_4"
   [(parallel [(set (match_operand:DF 0 "" "")
 		   (mem:DF (post_inc:SI (reg:SI SP_REG))))
@@ -5041,7 +5187,10 @@ label:
    && ! TARGET_SH2E
    && ! TARGET_SH2A
    && (register_operand (operands[0], SImode)
-       || register_operand (operands[1], SImode))"
+       || register_operand (operands[1], SImode))
+   && (!flag_stack_protect || TARGET_SHMEDIA
+       || reload_completed || reload_in_progress || !virtuals_instantiated
+       || !stack_protector_block (insn, operands[1]))"
   "@
 	mov.l	%1,%0
 	mov	%1,%0
@@ -5075,7 +5224,10 @@ label:
 	 "Q,r,I08,I20,I28,r,mr,x,l,t,r,x,l,r,r,>,>,>,y,i,r,y,y,*f,*f,y"))]
   "(TARGET_SH2E || TARGET_SH2A)
    && (register_operand (operands[0], SImode)
-       || register_operand (operands[1], SImode))"
+       || register_operand (operands[1], SImode))
+   && (!flag_stack_protect || TARGET_SHMEDIA
+       || reload_completed || reload_in_progress || !virtuals_instantiated
+       || !stack_protector_block (insn, operands[1]))"
   "@
 	mov.l	%1,%0
 	mov	%1,%0
@@ -5972,7 +6124,7 @@ label:
 ;; instructions.  And when not optimizing, no splits are done before fixing
 ;; up pcloads, so we need usable length information for that.
 (define_insn "movdf_i4"
-  [(set (match_operand:DF 0 "general_movdst_operand" "=d,r,d,d,m,r,r,m,!??r,!???d")
+  [(set (match_operand:DF 0 "general_movdst_operand" "=d,r,d,d,m,r,*r,*m,!??r,!???d")
 	(match_operand:DF 1 "general_movsrc_operand"  "d,r,F,m,d,FQ,m,r,d,r"))
    (use (match_operand:PSI 2 "fpscr_operand"          "c,c,c,c,c,c,c,c,c,c"))
    (clobber (match_scratch:SI 3                      "=X,X,&z,X,X,X,X,X,X,X"))]
@@ -6000,8 +6152,8 @@ label:
      [(if_then_else (eq_attr "fmovd" "yes") (const_int 4) (const_int 8))
       (const_int 4)
       (if_then_else (eq_attr "fmovd" "yes") (const_int 4) (const_int 6))
+      (const_int 4)
       (if_then_else (eq_attr "fmovd" "yes") (const_int 4) (const_int 6))
-      (if_then_else (eq_attr "fmovd" "yes") (const_int 4) (const_int 6))
       (const_int 4)
       (const_int 8) (const_int 8) ;; these need only 8 bytes for @(r0,rn)
       ;; We can't use 4-byte push/pop on SHcompact, so we have to
@@ -6820,9 +6972,26 @@ label:
       (const_int 2)
       (const_int 2)
       (const_int 0)])
-   (set (attr "fp_mode") (if_then_else (eq_attr "fmovd" "yes")
+  (set_attr_alternative "fp_mode"
+     [(if_then_else (eq_attr "fmovd" "yes") (const_string "single") (const_string "none"))
+      (const_string "none")
+      (const_string "single")
 					   (const_string "single")
-					   (const_string "single")))])
+      (const_string "none")
+      (if_then_else (eq_attr "fmovd" "yes") (const_string "single") (const_string "none"))
+      (if_then_else (eq_attr "fmovd" "yes") (const_string "single") (const_string "none"))
+      (const_string "none")
+      (const_string "none")
+      (const_string "none")
+      (const_string "none")
+      (const_string "none")
+      (const_string "none")
+      (const_string "none")
+      (const_string "none")
+      (const_string "none")
+      (const_string "none")
+      (const_string "none")
+      (const_string "none")])])
 
 (define_split
   [(set (match_operand:SF 0 "register_operand" "")
@@ -6943,7 +7112,7 @@ label:
 
 (define_insn "*movsi_y"
   [(set (match_operand:SI 0 "register_operand" "=y,y")
-	(match_operand:SI 1 "immediate_operand" "Qi,I08"))
+	(match_operand 1 "immediate_operand" "Qi,I08"))
    (clobber (match_scratch:SI 2 "=&z,r"))]
   "TARGET_SH2E
    && (reload_in_progress || reload_completed)"
@@ -6951,6 +7120,16 @@ label:
   [(set_attr "length" "4")
    (set_attr "type" "pcload,move")])
 
+(define_insn "*movsf_y"
+  [(set (match_operand:SF 0 "register_operand" "=y,y")
+	(match_operand 1 "immediate_operand" "Qi,I08"))
+   (clobber (match_scratch:SI 2 "=&z,r"))]
+  "TARGET_SH2E
+   && (reload_in_progress || reload_completed)"
+  "#"
+  [(set_attr "length" "4")
+   (set_attr "type" "pcload,move")])
+
 (define_split
   [(set (match_operand:SI 0 "register_operand" "")
 	(match_operand:SI 1 "immediate_operand" ""))
@@ -7173,6 +7352,50 @@ label:
   "b%o3%'	%N2, %N1, %0%>"
   [(set_attr "type" "cbranch_media")])
 
+(define_expand "cmpun_sdf"
+  [(unordered (match_operand 0 "" "") (match_operand 1 "" ""))]
+  ""
+  "
+{
+  HOST_WIDE_INT mask;
+  switch (GET_MODE (operands[0]))
+    {
+    case SFmode:
+      mask = SF_NAN_MASK;
+      break;
+    case DFmode:
+      mask = DF_NAN_MASK;
+      break;
+    default:
+      FAIL;
+    }
+  emit_insn (gen_cmpunsf_i1 (operands[0], operands[1],
+			     force_reg (SImode, GEN_INT (mask))));
+  DONE;
+}")
+
+(define_expand "cmpuneq_sdf"
+  [(uneq (match_operand 0 "" "") (match_operand 1 "" ""))]
+  ""
+  "
+{
+  HOST_WIDE_INT mask;
+  switch (GET_MODE (operands[0]))
+    {
+    case SFmode:
+      mask = SF_NAN_MASK;
+      break;
+    case DFmode:
+      mask = DF_NAN_MASK;
+      break;
+    default:
+      FAIL;
+    }
+  emit_insn (gen_cmpuneqsf_i1 (operands[0], operands[1],
+			       force_reg (SImode, GEN_INT (mask))));
+  DONE;
+}")
+
 ;; combiner splitter for test-and-branch on single bit in register.  This
 ;; is endian dependent because the non-paradoxical subreg looks different
 ;; on big endian.
@@ -7217,7 +7440,7 @@ label:
 	      (set (match_dup 0)
 		   (plus:SI (match_dup 0) (const_int -1)))
 	      (clobber (reg:SI T_REG))])]
-  "TARGET_SH2"
+  "TARGET_SH2 && !optimize_size"
   "
 {
   if (GET_MODE (operands[0]) != SImode)
@@ -8371,7 +8594,7 @@ label:
   ""
   "
 {
-  sh_expand_epilogue (1);
+  sh_expand_epilogue (true);
   if (TARGET_SHCOMPACT)
     {
       rtx insn, set;
@@ -8478,6 +8701,14 @@ label:
   DONE;
 }")
 
+(define_insn "dup_db_insn"
+  [(unspec_volatile [(const_int 0)] UNSPECV_DB_INSN)]
+  "TARGET_DEAD_DELAY"
+  ""
+  [(set_attr "length" "0")
+   (set_attr "in_delay_slot" "yes")])
+
+
 ;; ------------------------------------------------------------------------
 ;; Misc insns
 ;; ------------------------------------------------------------------------
@@ -8631,22 +8862,6 @@ label:
   i++;
 }")
 
-;; op0 = op1 + r12 but hide it before reload completed.  See the comment
-;; in symGOT_load expand.
-
-(define_insn_and_split "chk_guard_add"
-  [(set (match_operand:SI 0 "register_operand" "=&r")
-	(unspec:SI [(match_operand:SI 1 "register_operand" "r")
-		    (reg:SI PIC_REG)]
-		   UNSPEC_CHKADD))]
-  "TARGET_SH1"
-  "#"
-  "TARGET_SH1 && reload_completed"
-  [(set (match_dup 0) (reg:SI PIC_REG))
-   (set (match_dup 0) (plus:SI (match_dup 0) (match_dup 1)))]
-  ""
-  [(set_attr "type" "arith")])
-
 (define_expand "sym_label2reg"
   [(set (match_operand:SI 0 "" "")
 	(const:SI (unspec:SI [(match_operand:SI 1 "" "")
@@ -8689,20 +8904,6 @@ label:
   else
     emit_move_insn (operands[2], operands[1]);
 
-  /* When stack protector inserts codes after the result is set to
-     R0, @(rX, r12) will cause a spill failure for R0.  Use a unspec
-     insn to avoid combining (set A (plus rX r12)) and (set op0 (mem A))
-     when rX is a GOT address for the guard symbol.  Ugly but doesn't
-     matter because this is a rare situation.  */
-  if (!TARGET_SHMEDIA
-      && flag_stack_protect
-      && GET_CODE (operands[1]) == CONST
-      && GET_CODE (XEXP (operands[1], 0)) == UNSPEC
-      && GET_CODE (XVECEXP (XEXP (operands[1], 0), 0, 0)) == SYMBOL_REF
-      && strcmp (XSTR (XVECEXP (XEXP (operands[1], 0), 0, 0), 0),
-		 \"__stack_chk_guard\") == 0)
-    emit_insn (gen_chk_guard_add (operands[3], operands[2]));
-  else
     emit_move_insn (operands[3], gen_rtx_PLUS (Pmode, operands[2],
 					       gen_rtx_REG (Pmode, PIC_REG)));
 
@@ -9035,8 +9236,8 @@ mov.l\\t1f,r0\\n\\
 
 (define_insn "casesi_worker_0"
   [(set (match_operand:SI 0 "register_operand" "=r,r")
-	(unspec:SI [(match_operand:SI 1 "register_operand" "0,r")
-		 (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI))
+	(mem:SI (unspec:SI [(match_operand:SI 1 "register_operand" "0,r")
+		 (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI)))
    (clobber (match_scratch:SI 3 "=X,1"))
    (clobber (match_scratch:SI 4 "=&z,z"))]
   "TARGET_SH1"
@@ -9044,38 +9245,38 @@ mov.l\\t1f,r0\\n\\
 
 (define_split
   [(set (match_operand:SI 0 "register_operand" "")
-	(unspec:SI [(match_operand:SI 1 "register_operand" "")
-		    (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI))
+	(mem:SI (unspec:SI [(match_operand:SI 1 "register_operand" "")
+		    (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI)))
    (clobber (match_scratch:SI 3 ""))
    (clobber (match_scratch:SI 4 ""))]
   "TARGET_SH1 && ! TARGET_SH2 && reload_completed"
   [(set (reg:SI R0_REG) (unspec:SI [(label_ref (match_dup 2))] UNSPEC_MOVA))
    (parallel [(set (match_dup 0)
-	      (unspec:SI [(reg:SI R0_REG) (match_dup 1)
-			  (label_ref (match_dup 2))] UNSPEC_CASESI))
+	      (mem:SI (unspec:SI [(reg:SI R0_REG) (match_dup 1)
+			  (label_ref (match_dup 2))] UNSPEC_CASESI)))
 	      (clobber (match_dup 3))])
    (set (match_dup 0) (plus:SI (match_dup 0) (reg:SI R0_REG)))]
   "if (GET_CODE (operands[2]) == CODE_LABEL) LABEL_NUSES (operands[2])++;")
 
 (define_split
   [(set (match_operand:SI 0 "register_operand" "")
-	(unspec:SI [(match_operand:SI 1 "register_operand" "")
-		    (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI))
+	(mem:SI (unspec:SI [(match_operand:SI 1 "register_operand" "")
+		    (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI)))
    (clobber (match_scratch:SI 3 ""))
    (clobber (match_scratch:SI 4 ""))]
   "TARGET_SH2 && reload_completed"
   [(set (reg:SI R0_REG) (unspec:SI [(label_ref (match_dup 2))] UNSPEC_MOVA))
    (parallel [(set (match_dup 0)
-	      (unspec:SI [(reg:SI R0_REG) (match_dup 1)
-			  (label_ref (match_dup 2))] UNSPEC_CASESI))
+	      (mem:SI (unspec:SI [(reg:SI R0_REG) (match_dup 1)
+			  (label_ref (match_dup 2))] UNSPEC_CASESI)))
 	      (clobber (match_dup 3))])]
   "if (GET_CODE (operands[2]) == CODE_LABEL) LABEL_NUSES (operands[2])++;")
 
 (define_insn "casesi_worker_1"
   [(set (match_operand:SI 0 "register_operand" "=r,r")
-	(unspec:SI [(reg:SI R0_REG)
+	(mem:SI (unspec:SI [(reg:SI R0_REG)
 		    (match_operand:SI 1 "register_operand" "0,r")
-		    (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI))
+		    (label_ref (match_operand 2 "" ""))] UNSPEC_CASESI)))
    (clobber (match_scratch:SI 3 "=X,1"))]
   "TARGET_SH1"
   "*
@@ -9102,10 +9303,10 @@ mov.l\\t1f,r0\\n\\
 
 (define_insn "casesi_worker_2"
   [(set (match_operand:SI 0 "register_operand" "=r,r")
-	(unspec:SI [(reg:SI R0_REG)
+	(mem:SI (unspec:SI [(reg:SI R0_REG)
 		    (match_operand:SI 1 "register_operand" "0,r")
 		    (label_ref (match_operand 2 "" ""))
-		    (label_ref (match_operand 3 "" ""))] UNSPEC_CASESI))
+		    (label_ref (match_operand 3 "" ""))] UNSPEC_CASESI)))
    (clobber (match_operand:SI 4 "" "=X,1"))]
   "TARGET_SH2 && reload_completed && flag_pic"
   "*
@@ -9197,9 +9398,13 @@ mov.l\\t1f,r0\\n\\
 }"
   [(set_attr "type" "load_media")])
 
+(define_expand "simple_return"
+  [(simple_return)]
+ "0 && sh_can_use_simple_return_p ()")
+
 (define_expand "return"
   [(return)]
-  "reload_completed && ! sh_need_epilogue ()"
+ "reload_completed && epilogue_completed"
   "
 {
   if (TARGET_SHMEDIA)
@@ -9216,8 +9421,8 @@ mov.l\\t1f,r0\\n\\
     }
 }")
 
-(define_insn "*return_i"
-  [(return)]
+(define_insn "*<code>_i"
+  [(any_return)]
   "TARGET_SH1 && ! (TARGET_SHCOMPACT
 		    && (crtl->args.info.call_cookie
 			& CALL_COOKIE_RET_TRAMP (1)))
@@ -9243,6 +9448,20 @@ mov.l\\t1f,r0\\n\\
   "%@"
   [(set_attr "type" "return")])
 
+
+;; Unconditional traps are assumed to have (const_int 1) for the condition.
+(define_insn "trap"
+  [(trap_if (const_int 1) (const_int 42))]
+  "TARGET_BUILTIN_TRAPA"
+  "*
+{
+  static char templ[16];
+
+  sprintf (templ, \"trapa\t#%d\", 42);
+  return templ;
+}"
+  [(set_attr "in_delay_slot" "no")])
+
 (define_expand "shcompact_return_tramp"
   [(return)]
   "TARGET_SHCOMPACT
@@ -9352,12 +9571,7 @@ mov.l\\t1f,r0\\n\\
 (define_expand "epilogue"
   [(return)]
   ""
-  "
-{
-  sh_expand_epilogue (0);
-  emit_jump_insn (gen_return ());
-  DONE;
-}")
+  "sh_expand_epilogue (false);")
 
 (define_expand "eh_return"
   [(use (match_operand 0 "register_operand" ""))]
@@ -9578,6 +9792,19 @@ mov.l\\t1f,r0\\n\\
    DONE;
 ")
 
+(define_expand "sunle"
+  [(set (match_operand:SI 0 "arith_reg_operand" "")
+	(match_dup 1))]
+  "TARGET_SH1_SOFTFP"
+  "
+{
+  if (! currently_expanding_to_rtl)
+    FAIL;
+    sh_emit_compare_and_branch (operands, DFmode);
+  emit_insn (gen_movt (operands[0]));
+  DONE;
+}")
+
 ;; sne moves the complement of the T reg to DEST like this:
 ;;      cmp/eq ...
 ;;      mov    #-1,temp
@@ -9786,7 +10013,8 @@ mov.l\\t1f,r0\\n\\
   [(unspec_volatile [(const_int 0)] UNSPECV_CONST_END)]
   ""
   "* return output_jump_label_table ();"
-  [(set_attr "in_delay_slot" "no")])
+  [(set_attr "length" "0")
+   (set_attr "in_delay_slot" "no")])
 
 ; emitted at the end of the window in the literal table.
 
@@ -9879,6 +10107,43 @@ mov.l\\t1f,r0\\n\\
   [(set_attr "type" "sfunc")
    (set_attr "needs_delay_slot" "yes")])
 
+(define_insn "cmpstr_t"
+  [(set (reg:SI T_REG)
+	(unspec:SI [
+	  (match_operand:SI 0 "register_operand" "r")
+          (match_operand:SI 1 "register_operand" "r")
+        ] UNSPEC_STR))]
+  "TARGET_SH1"
+  "cmp/str	%0,%1"
+  [(set_attr "type" "mt_group")])
+
+(define_insn "rotcr_t"
+  [(set (match_operand:SI 0 "register_operand" "+r")
+	(unspec:SI [
+          (match_dup 0)
+          (reg:SI T_REG)
+	] UNSPEC_ROT))
+   (clobber (reg:SI T_REG))]
+  "TARGET_SH1"
+  "rotcr	%0"
+  [(set_attr "type" "mt_group")])
+
+(define_expand "cmpstrsi"
+  [(match_operand:SI 0 "register_operand" "")
+   (match_operand 1 "register_operand" "")
+   (match_operand 2 "register_operand" "")
+   (match_operand 3 "register_operand" "")
+   ]
+  "0 && TARGET_HARD_SH4"
+  "
+{   
+   if (! optimize_insn_for_size_p () && sh4_expand_cmpstr(operands))
+      DONE; 
+
+   else FAIL;
+}")
+
+
 ;; -------------------------------------------------------------------------
 ;; Floating point instructions.
 ;; -------------------------------------------------------------------------
@@ -9970,15 +10235,10 @@ mov.l\\t1f,r0\\n\\
   "fschg"
   [(set_attr "type" "fpscr_toggle") (set_attr "fp_set" "unknown")])
 
-;; There's no way we can use it today, since optimize mode switching
-;; doesn't enable us to know from which mode we're switching to the
-;; mode it requests, to tell whether we can use a relative mode switch
-;; (like toggle_pr) or an absolute switch (like loading fpscr from
-;; memory).
 (define_insn "toggle_pr"
   [(set (reg:PSI FPSCR_REG)
 	(xor:PSI (reg:PSI FPSCR_REG) (const_int 524288)))]
-  "TARGET_SH4A_FP && ! TARGET_FPU_SINGLE"
+  "(TARGET_SH4A_FP || TARGET_SH4_300)" 
   "fpchg"
   [(set_attr "type" "fpscr_toggle")])
 
@@ -9986,7 +10246,7 @@ mov.l\\t1f,r0\\n\\
   [(set (match_operand:SF 0 "arith_reg_operand" "")
 	(plus:SF (match_operand:SF 1 "arith_reg_operand" "")
 		 (match_operand:SF 2 "arith_reg_operand" "")))]
-  "TARGET_SH2E || TARGET_SHMEDIA_FPU"
+  "TARGET_SH2E || (TARGET_SH3 && TARGET_OSFP) || TARGET_SHMEDIA_FPU"
   "
 {
   if (TARGET_SH2E)
@@ -9994,6 +10254,12 @@ mov.l\\t1f,r0\\n\\
       expand_sf_binop (&gen_addsf3_i, operands);
       DONE;
     }
+  else if (TARGET_OSFP)
+    {
+      expand_sfunc_binop (SFmode, &gen_addsf3_i3, \"__addsf3\", PLUS,
+			  operands);
+      DONE;
+    }
 }")
 
 (define_insn "*addsf3_media"
@@ -10092,6 +10358,22 @@ mov.l\\t1f,r0\\n\\
 }"
   [(set_attr "type" "fparith_media")])
 
+(define_insn "addsf3_i3"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=z")
+	(plus:SF (reg:SF R4_REG) (reg:SF R5_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (clobber (reg:SI R6_REG))
+   (clobber (reg:SI R7_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_OSFP && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_insn "addsf3_i"
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
 	(plus:SF (match_operand:SF 1 "fp_arith_reg_operand" "%0")
@@ -10106,7 +10388,7 @@ mov.l\\t1f,r0\\n\\
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "")
 	(minus:SF (match_operand:SF 1 "fp_arith_reg_operand" "")
 		  (match_operand:SF 2 "fp_arith_reg_operand" "")))]
-  "TARGET_SH2E || TARGET_SHMEDIA_FPU"
+  "TARGET_SH2E || (TARGET_SH3 && TARGET_OSFP) || TARGET_SHMEDIA_FPU"
   "
 {
   if (TARGET_SH2E)
@@ -10114,6 +10396,12 @@ mov.l\\t1f,r0\\n\\
       expand_sf_binop (&gen_subsf3_i, operands);
       DONE;
     }
+  else if (TARGET_OSFP)
+    {
+      expand_sfunc_binop (SFmode, &gen_subsf3_i3, \"__subsf3\", MINUS,
+			  operands);
+      DONE;
+    }
 }")
 
 (define_insn "*subsf3_media"
@@ -10124,6 +10412,23 @@ mov.l\\t1f,r0\\n\\
   "fsub.s	%1, %2, %0"
   [(set_attr "type" "fparith_media")])
 
+(define_insn "subsf3_i3"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=z")
+	(minus:SF (reg:SF R4_REG) (reg:SF R5_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (clobber (reg:SI R5_REG))
+   (clobber (reg:SI R6_REG))
+   (clobber (reg:SI R7_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_OSFP && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_insn "subsf3_i"
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
 	(minus:SF (match_operand:SF 1 "fp_arith_reg_operand" "0")
@@ -10138,8 +10443,16 @@ mov.l\\t1f,r0\\n\\
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "")
 	(mult:SF (match_operand:SF 1 "fp_arith_reg_operand" "")
 		 (match_operand:SF 2 "fp_arith_reg_operand" "")))]
-  "TARGET_SH2E || TARGET_SHMEDIA_FPU"
-  "")
+  "TARGET_SH2E || (TARGET_SH3 && TARGET_OSFP) || TARGET_SHMEDIA_FPU"
+  "
+{
+  if (!TARGET_SH2E && TARGET_OSFP)
+    {
+      expand_sfunc_binop (SFmode, &gen_mulsf3_i3, \"__mulsf3\", MULT,
+                         operands);
+      DONE;
+    }
+}")
 
 (define_insn "*mulsf3_media"
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
@@ -10180,6 +10493,22 @@ mov.l\\t1f,r0\\n\\
   [(set_attr "type" "fp")
    (set_attr "fp_mode" "single")])
 
+(define_insn "mulsf3_i3"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=z")
+	(mult:SF (reg:SF R4_REG) (reg:SF R5_REG)))
+   (clobber (reg:SI MACH_REG))
+   (clobber (reg:SI MACL_REG))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_OSFP && ! TARGET_SH2E"
+   "jsr	@%1%#"
+   [(set_attr "type" "sfunc")
+    (set_attr "needs_delay_slot" "yes")])
+
 (define_insn "mac_media"
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
 	(plus:SF (mult:SF (match_operand:SF 1 "fp_arith_reg_operand" "%f")
@@ -10349,6 +10678,155 @@ mov.l\\t1f,r0\\n\\
   [(set_attr "type" "fp_cmp")
    (set_attr "fp_mode" "single")])
 
+ (define_insn "cmpnesf_i1"
+   [(set (match_operand:CC_FP_NE 0 "register_operand" "=z")
+ 	(compare:CC_FP_NE (reg:SF R4_REG) (reg:SF R5_REG)))
+    (clobber (reg:SI T_REG))
+    (clobber (reg:SI PR_REG))
+    (clobber (reg:SI R1_REG))
+    (clobber (reg:SI R2_REG))
+    (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+   "TARGET_SH1 && ! TARGET_SH2E"
+   "jsr	@%1%#"
+   [(set_attr "type" "sfunc")
+    (set_attr "needs_delay_slot" "yes")])
+
+ (define_insn "cmpgtsf_i1"
+   [(set (match_operand:CC_FP_GT 0 "register_operand" "=z")
+ 	(compare:CC_FP_GT (reg:SF R4_REG) (reg:SF R5_REG)))
+    (clobber (reg:SI T_REG))
+    (clobber (reg:SI PR_REG))
+    (clobber (reg:SI R1_REG))
+    (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+   "TARGET_SH1 && ! TARGET_SH2E"
+   "jsr	@%1%#"
+   [(set_attr "type" "sfunc")
+    (set_attr "needs_delay_slot" "yes")])
+
+ (define_insn "cmpunltsf_i1"
+   [(set (match_operand:CC_FP_UNLT 0 "register_operand" "=z")
+ 	(compare:CC_FP_UNLT (reg:SF R4_REG) (reg:SF R5_REG)))
+    (clobber (reg:SI T_REG))
+    (clobber (reg:SI PR_REG))
+    (clobber (reg:SI R1_REG))
+    (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+   "TARGET_SH1 && ! TARGET_SH2E"
+   "jsr	@%1%#"
+   [(set_attr "type" "sfunc")
+    (set_attr "needs_delay_slot" "yes")])
+
+ (define_insn "cmpeqsf_i1_finite"
+   [(set (reg:SI T_REG)
+ 	(eq:SI (match_operand:SF 0 "arith_reg_operand" "r,r,r")
+ 	       (match_operand:SF 1 "arith_reg_operand" "r,r,r")))
+    (clobber (match_scratch:SI 2 "=0,1,?r"))]
+   "TARGET_SH1 && ! TARGET_SH2E && flag_finite_math_only"
+   "*
+ {
+   if (which_alternative == 0)
+      output_asm_insn (\"cmp/eq\t%0,%1\;or\t%1,%2\;bt\t0f\", operands);
+   else if (which_alternative == 1)
+      output_asm_insn (\"cmp/eq\t%0,%1\;or\t%0,%2\;bt\t0f\", operands);
+   else
+     output_asm_insn (\"cmp/eq\t%0,%1\;mov\t%0,%2\;bt\t0f\;or\t%1,%2\",
+ 		     operands);
+   return \"add\t%2,%2\;tst\t%2,%2\\n0:\";
+ }"
+   [(set_attr "length" "10,10,12")])
+
+ (define_insn "cmplesf_i1_finite"
+   [(set (reg:SI T_REG)
+ 	(le:SI (match_operand:SF 0 "arith_reg_operand" "r,r,r")
+ 	       (match_operand:SF 1 "arith_reg_operand" "r,r,r")))
+    (clobber (match_scratch:SI 2 "=0,1,r"))]
+   "TARGET_SH1 && ! TARGET_SH2E && flag_finite_math_only"
+   "*
+ {
+   output_asm_insn (\"cmp/pz\t%0\", operands);
+   if (which_alternative == 2)
+     output_asm_insn (\"mov\t%0,%2\", operands);
+   if (TARGET_SH2)
+     output_asm_insn (\"bf/s\t0f\;cmp/hs\t%1,%0\;cmp/ge\t%0,%1\", operands);
+   else
+     output_asm_insn (\"bt\t1f\;bra\t0f\;cmp/hs\t%1,%0\\n1:\tcmp/ge\t%0,%1\",
+ 		     operands);
+   if (which_alternative == 1)
+     output_asm_insn (\"or\t%0,%2\", operands);
+   else
+     output_asm_insn (\"or\t%1,%2\", operands);
+   return \"bt\t0f\;add\t%2,%2\;tst\t%2,%2\\n0:\";
+ }"
+   [(set_attr "length" "18,18,20")])
+
+ (define_insn "cmpunsf_i1"
+   [(set (reg:SI T_REG)
+ 	(unordered:SI (match_operand:SF 0 "arith_reg_operand" "r")
+ 		      (match_operand:SF 1 "arith_reg_operand" "r")))
+    (use (match_operand:SI 2 "arith_reg_operand" "r"))
+    (clobber (match_scratch:SI 3 "=&r"))]
+   "TARGET_SH1 && ! TARGET_SH2E"
+   "not\t%0,%3\;tst\t%2,%3\;bt.s\t0f
+\tnot\t%1,%3\;tst\t%2,%3\;bt.s\t0f
+\tmov\t#96,%3\;shll16\t%3\;xor\t%3,%2
+\tnot\t%0,%3\;tst\t%2,%3\;bt.s\t0f
+\tnot\t%1,%3\;tst\t%2,%3
+0:"
+   [(set_attr "length" "28")])
+
+ ;; ??? This is a lot of code with a lot of branches; a library function
+ ;; might be better.
+ (define_insn "cmpuneqsf_i1"
+   [(set (reg:SI T_REG)
+ 	(uneq:SI (match_operand:SF 0 "arith_reg_operand" "r")
+ 		 (match_operand:SF 1 "arith_reg_operand" "r")))
+    (use (match_operand:SI 2 "arith_reg_operand" "r"))
+    (clobber (match_scratch:SI 3 "=&r"))]
+   "TARGET_SH1 && ! TARGET_SH2E"
+   "*
+ {
+   output_asm_insn (\"not\t%0,%3\;tst\t%2,%3\;not\t%1,%3\", operands);
+   output_asm_insn (\"bt\t0f\;tst\t%2,%3\;bt\t0f\;cmp/eq\t%0,%1\", operands);
+   output_asm_insn (\"mov\t%0,%3\;bt\t0f\;or\t%1,%3\", operands);
+   return \"add\t%3,%3\;tst\t%3,%3\\n0:\";
+ }"
+   [(set_attr "length" "24")])
+
+ (define_insn "movcc_fp_ne"
+   [(set (match_operand:CC_FP_NE 0 "general_movdst_operand"
+ 	    "=r,r,m")
+ 	(match_operand:CC_FP_NE 1 "general_movsrc_operand"
+ 	 "rI08,mr,r"))]
+   "TARGET_SH1"
+   "@
+ 	mov	%1,%0
+ 	mov.l	%1,%0
+ 	mov.l	%1,%0"
+   [(set_attr "type" "move,load,store")])
+
+ (define_insn "movcc_fp_gt"
+   [(set (match_operand:CC_FP_GT 0 "general_movdst_operand"
+ 	    "=r,r,m")
+ 	(match_operand:CC_FP_GT 1 "general_movsrc_operand"
+ 	 "rI08,mr,r"))]
+   "TARGET_SH1"
+   "@
+ 	mov	%1,%0
+ 	mov.l	%1,%0
+ 	mov.l	%1,%0"
+   [(set_attr "type" "move,load,store")])
+
+ (define_insn "movcc_fp_unlt"
+   [(set (match_operand:CC_FP_UNLT 0 "general_movdst_operand"
+ 	    "=r,r,m")
+ 	(match_operand:CC_FP_UNLT 1 "general_movsrc_operand"
+ 	 "rI08,mr,r"))]
+   "TARGET_SH1"
+   "@
+ 	mov	%1,%0
+ 	mov.l	%1,%0
+ 	mov.l	%1,%0"
+   [(set_attr "type" "move,load,store")])
+
 (define_insn "cmpeqsf_t"
   [(set (reg:SI T_REG)
 	(eq:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
@@ -10367,7 +10845,22 @@ mov.l\\t1f,r0\\n\\
   "* return output_ieee_ccmpeq (insn, operands);"
   [(set_attr "length" "4")])
 
+(define_insn "*cmpltgtsf_t"
+  [(set (reg:SI T_REG)
+	(ltgt:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
+		 (match_operand:SF 1 "fp_arith_reg_operand" "f")))]
+  "TARGET_SH2E && ! (TARGET_SH4 || TARGET_SH2A_SINGLE)"
+  "fcmp/gt\t%1,%0\;bt\t0f\;fcmp/gt\t%0,%1\\n0:"
+  [(set_attr "length" "6")])
 
+(define_insn "*cmporderedsf_t"
+  [(set (reg:SI T_REG)
+	(ordered:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
+		    (match_operand:SF 1 "fp_arith_reg_operand" "f")))]
+  "TARGET_SH2E && ! (TARGET_SH4 || TARGET_SH2A_SINGLE)"
+  "fcmp/eq\t%0,%0\;bf\t0f\;fcmp/eq\t%1,%1\\n0:"
+  [(set_attr "length" "6")])
+
 (define_insn "cmpgtsf_t_i4"
   [(set (reg:SI T_REG)
 	(gt:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
@@ -10399,6 +10892,26 @@ mov.l\\t1f,r0\\n\\
   [(set_attr "length" "4")
    (set_attr "fp_mode" "single")])
 
+(define_insn "*cmpltgtsf_t_4"
+  [(set (reg:SI T_REG)
+	(ltgt:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
+		 (match_operand:SF 1 "fp_arith_reg_operand" "f")))
+   (use (match_operand:PSI 2 "fpscr_operand" "c"))]
+  "TARGET_SH4 || TARGET_SH2A_SINGLE"
+  "fcmp/gt\t%1,%0\;bt\t0f\;fcmp/gt\t%0,%1\\n0:"
+  [(set_attr "length" "6")
+   (set_attr "fp_mode" "single")])
+
+(define_insn "*cmporderedsf_t_4"
+  [(set (reg:SI T_REG)
+	(ordered:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
+		    (match_operand:SF 1 "fp_arith_reg_operand" "f")))
+   (use (match_operand:PSI 2 "fpscr_operand" "c"))]
+  "TARGET_SH4 || TARGET_SH2A_SINGLE"
+  "fcmp/eq\t%0,%0\;bf\t0f\;fcmp/eq\t%1,%1\\n0:"
+  [(set_attr "length" "6")
+   (set_attr "fp_mode" "single")])
+
 (define_insn "cmpeqsf_media"
   [(set (match_operand:SI 0 "register_operand" "=r")
 	(eq:SI (match_operand:SF 1 "fp_arith_reg_operand" "f")
@@ -10647,11 +11160,39 @@ mov.l\\t1f,r0\\n\\
   [(set_attr "type" "fmove")
    (set_attr "fp_mode" "single")])
 
+(define_expand "abssc2"
+  [(set (match_operand:SF 0 "fp_arith_reg_operand" "")
+	(abs:SF (match_operand:SC 1 "fp_arith_reg_operand" "")))]
+  "TARGET_OSFP && ! TARGET_SH2E"
+  "
+{
+  expand_sfunc_unop (SCmode, &gen_abssc2_i3, \"__hypotf\", ABS, operands);
+  DONE;
+}")
+
+(define_insn "abssc2_i3"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=z")
+	(abs:SF (reg:SC R4_REG)))
+   (clobber (reg:SI MACH_REG))
+   (clobber (reg:SI MACL_REG))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (clobber (reg:SI R4_REG))
+   (clobber (reg:SI R5_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_OSFP && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_expand "adddf3"
   [(set (match_operand:DF 0 "fp_arith_reg_operand" "")
 	(plus:DF (match_operand:DF 1 "fp_arith_reg_operand" "")
 		 (match_operand:DF 2 "fp_arith_reg_operand" "")))]
-  "(TARGET_SH4 || TARGET_SH2A_DOUBLE) || TARGET_SHMEDIA_FPU"
+  "TARGET_FPU_DOUBLE || (TARGET_SH3 && TARGET_OSFP)"
   "
 {
   if (TARGET_SH4 || TARGET_SH2A_DOUBLE)
@@ -10659,6 +11200,12 @@ mov.l\\t1f,r0\\n\\
       expand_df_binop (&gen_adddf3_i, operands);
       DONE;
     }
+  else if (TARGET_SH3 && TARGET_OSFP)
+    {
+      expand_sfunc_binop (DFmode, &gen_adddf3_i3_wrap, \"__adddf3\", PLUS,
+			  operands);
+      DONE;
+    }
 }")
 
 (define_insn "*adddf3_media"
@@ -10679,6 +11226,30 @@ mov.l\\t1f,r0\\n\\
   [(set_attr "type" "dfp_arith")
    (set_attr "fp_mode" "double")])
 
+(define_expand "adddf3_i3_wrap"
+  [(match_operand:DF 0 "" "") (match_operand:SI 1 "" "")]
+  "TARGET_SH3"
+  "
+{
+  emit_insn (gen_adddf3_i3 (operands[1]));
+  emit_move_insn (operands[0], gen_rtx_REG (DFmode, R0_REG));
+  DONE;
+}")
+
+(define_insn "adddf3_i3"
+  [(set (reg:DF R0_REG)
+	(plus:DF (reg:DF R4_REG) (reg:DF R6_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:DI R2_REG))
+   (clobber (reg:DF R4_REG))
+   (clobber (reg:DF R6_REG))
+   (use (match_operand:SI 0 "arith_reg_operand" "r"))]
+  "TARGET_SH3"
+  "jsr	@%0%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_expand "subdf3"
   [(set (match_operand:DF 0 "fp_arith_reg_operand" "")
 	(minus:DF (match_operand:DF 1 "fp_arith_reg_operand" "")
@@ -10715,7 +11286,7 @@ mov.l\\t1f,r0\\n\\
   [(set (match_operand:DF 0 "fp_arith_reg_operand" "")
 	(mult:DF (match_operand:DF 1 "fp_arith_reg_operand" "")
 		 (match_operand:DF 2 "fp_arith_reg_operand" "")))]
-  "(TARGET_SH4 || TARGET_SH2A_DOUBLE) || TARGET_SHMEDIA_FPU"
+  "TARGET_FPU_DOUBLE || (TARGET_SH3 && TARGET_OSFP)"
   "
 {
   if (TARGET_SH4 || TARGET_SH2A_DOUBLE)
@@ -10723,6 +11294,12 @@ mov.l\\t1f,r0\\n\\
       expand_df_binop (&gen_muldf3_i, operands);
       DONE;
     }
+  else if (TARGET_SH3 && TARGET_OSFP)
+    {
+      expand_sfunc_binop (DFmode, &gen_muldf3_i3_wrap, \"__muldf3\", MULT,
+			  operands);
+      DONE;
+    }
 }")
 
 (define_insn "*muldf3_media"
@@ -10743,6 +11320,32 @@ mov.l\\t1f,r0\\n\\
   [(set_attr "type" "dfp_mul")
    (set_attr "fp_mode" "double")])
 
+(define_expand "muldf3_i3_wrap"
+  [(match_operand:DF 0 "" "") (match_operand:SI 1 "" "")]
+  "TARGET_SH3"
+   "
+ {
+   emit_insn (gen_muldf3_i3 (operands[1]));
+   emit_move_insn (operands[0], gen_rtx_REG (DFmode, R0_REG));
+   DONE;
+ }")
+
+ (define_insn "muldf3_i3"
+   [(set (reg:DF R0_REG)
+ 	(mult:DF (reg:DF R4_REG) (reg:DF R6_REG)))
+    (clobber (reg:SI MACH_REG))
+    (clobber (reg:SI MACL_REG))
+    (clobber (reg:SI T_REG))
+    (clobber (reg:SI PR_REG))
+    (clobber (reg:DI R2_REG))
+    (clobber (reg:DF R4_REG))
+    (clobber (reg:DF R6_REG))
+    (use (match_operand:SI 0 "arith_reg_operand" "r"))]
+   "TARGET_SH3"
+   "jsr	@%0%#"
+   [(set_attr "type" "sfunc")
+    (set_attr "needs_delay_slot" "yes")])
+
 (define_expand "divdf3"
   [(set (match_operand:DF 0 "fp_arith_reg_operand" "")
 	(div:DF (match_operand:DF 1 "fp_arith_reg_operand" "")
@@ -10872,6 +11475,79 @@ mov.l\\t1f,r0\\n\\
 ;; 	      (use (match_dup 2))])
 ;;    (set (match_dup 0) (reg:SI FPUL_REG))])
 
+(define_insn "cmpnedf_i1"
+  [(set (match_operand:CC_FP_NE 0 "register_operand" "=z")
+	(compare:CC_FP_NE (reg:DF R4_REG) (reg:DF R6_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "cmpgtdf_i1"
+  [(set (match_operand:CC_FP_GT 0 "register_operand" "=z")
+	(compare:CC_FP_GT (reg:DF R4_REG) (reg:DF R6_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "cmpunltdf_i1"
+  [(set (match_operand:CC_FP_UNLT 0 "register_operand" "=z")
+	(compare:CC_FP_UNLT (reg:DF R4_REG) (reg:DF R6_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "cmpeqdf_i1_finite"
+  [(set (reg:SI T_REG)
+	(eq:SI (match_operand:DF 0 "arith_reg_operand" "r")
+	       (match_operand:DF 1 "arith_reg_operand" "r")))
+   (clobber (match_scratch:SI 2 "=&r"))]
+  "TARGET_SH1_SOFTFP && flag_finite_math_only"
+  "cmp/eq\t%R0,%R1\;mov\t%S0,%2\;bf\t0f\;cmp/eq\t%S0,%S1\;bt\t0f\;or\t%S1,%2\;add\t%2,%2\;or\t%R0,%2\;tst\t%2,%2\\n0:"
+  [(set_attr "length" "18")])
+
+(define_insn "cmpundf_i1"
+  [(set (reg:SI T_REG)
+	(unordered:SI (match_operand:DF 0 "arith_reg_operand" "r")
+		      (match_operand:DF 1 "arith_reg_operand" "r")))
+   (use (match_operand:SI 2 "arith_reg_operand" "r"))
+   (clobber (match_scratch:SI 3 "=&r"))]
+  "TARGET_SH1 && ! TARGET_SH2E"
+   "not\t%S0,%3\;tst\t%2,%3\;bt.s\t0f
+  \tnot\t%S1,%3\;tst\t%2,%3\;bt.s\t0f
+  \tmov\t#12,%3\;shll16\t%3\;xor\t%3,%2
+  \tnot\t%S0,%3\;tst\t%2,%3\;bt.s\t0f
+  \tnot\t%S1,%3\;tst\t%2,%3
+0:"
+  [(set_attr "length" "28")])
+
+;; ??? This is a lot of code with a lot of branches; a library function
+;; might be better.
+(define_insn "cmpuneqdf_i1"
+  [(set (reg:SI T_REG)
+	(uneq:SI (match_operand:DF 0 "arith_reg_operand" "r")
+		 (match_operand:DF 1 "arith_reg_operand" "r")))
+   (use (match_operand:SI 2 "arith_reg_operand" "r"))
+   (clobber (match_scratch:SI 3 "=&r"))]
+  "TARGET_SH1_SOFTFP"
+  "not\t%S0,%3\;tst\t%2,%3\;not\t%S1,%3\;bt\t0f\;tst\t%2,%3\;bt\t0f\;cmp/eq\t%R0,%R1\; bf\t0f\;cmp/eq\t%S0,%S1\;bt\t0f\;mov\t%S0,%3\;or\t%S1,%3\;add\t%3,%3\;or\t%R0,%3\;tst\t%3,%3\\n0:"
+  [(set_attr "length" "30")])
+
 (define_insn "cmpgtdf_t"
   [(set (reg:SI T_REG)
 	(gt:SI (match_operand:DF 0 "arith_reg_operand" "f")
@@ -10903,6 +11579,26 @@ mov.l\\t1f,r0\\n\\
   [(set_attr "length" "4")
    (set_attr "fp_mode" "double")])
 
+(define_insn "*cmpltgtdf_t"
+  [(set (reg:SI T_REG)
+	(ltgt:SI (match_operand:DF 0 "fp_arith_reg_operand" "f")
+		 (match_operand:DF 1 "fp_arith_reg_operand" "f")))
+   (use (match_operand:PSI 2 "fpscr_operand" "c"))]
+  "TARGET_SH4 || TARGET_SH2A_DOUBLE"
+  "fcmp/gt\t%1,%0\;bt\t0f\;fcmp/gt\t%0,%1\\n0:"
+  [(set_attr "length" "6")
+   (set_attr "fp_mode" "double")])
+
+(define_insn "*cmpordereddf_t_4"
+  [(set (reg:SI T_REG)
+	(ordered:SI (match_operand:DF 0 "fp_arith_reg_operand" "f")
+		    (match_operand:DF 1 "fp_arith_reg_operand" "f")))
+   (use (match_operand:PSI 2 "fpscr_operand" "c"))]
+  "TARGET_SH4 || TARGET_SH2A_SINGLE"
+  "fcmp/eq\t%0,%0\;bf\t0f\;fcmp/eq\t%1,%1\\n0:"
+  [(set_attr "length" "6")
+   (set_attr "fp_mode" "double")])
+
 (define_insn "cmpeqdf_media"
   [(set (match_operand:SI 0 "register_operand" "=r")
 	(eq:SI (match_operand:DF 1 "fp_arith_reg_operand" "f")
@@ -11044,7 +11740,7 @@ mov.l\\t1f,r0\\n\\
 (define_expand "extendsfdf2"
   [(set (match_operand:DF 0 "fp_arith_reg_operand" "")
 	(float_extend:DF (match_operand:SF 1 "fpul_operand" "")))]
-  "(TARGET_SH4 || TARGET_SH2A_DOUBLE) || TARGET_SHMEDIA_FPU"
+  "TARGET_FPU_DOUBLE || (TARGET_SH3 && TARGET_OSFP) || TARGET_SHMEDIA_FPU"
   "
 {
   if (TARGET_SH4 || TARGET_SH2A_DOUBLE)
@@ -11053,6 +11749,18 @@ mov.l\\t1f,r0\\n\\
 					get_fpscr_rtx ()));
       DONE;
     }
+  if (TARGET_SH2E && TARGET_OSFP)
+    {
+      expand_sfunc_unop (SFmode, &gen_extendsfdf2_i2e, \"__extendsfdf2\",
+ 		 FLOAT_EXTEND, operands);
+      DONE;
+    }
+  else if (TARGET_SH1 && TARGET_OSFP)
+    {
+      expand_sfunc_unop (SFmode, &gen_extendsfdf2_i1, \"__extendsfdf2\",
+			 FLOAT_EXTEND, operands);
+      DONE;
+    }
 }")
 
 (define_insn "*extendsfdf2_media"
@@ -11071,10 +11779,76 @@ mov.l\\t1f,r0\\n\\
   [(set_attr "type" "fp")
    (set_attr "fp_mode" "double")])
 
+;; ??? In order to use this efficiently, we'd have to have an extra
+;; register class for r0 and r1 - and that would cause repercussions in
+;; register allocation elsewhere.  So just say we clobber r0 / r1, and
+;; that we can use an arbitrary target.  */
+(define_insn_and_split "extendsfdf2_i1"
+  [(set (match_operand:DF 0 "arith_reg_dest" "=r")
+	(float_extend:DF (reg:SF R4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R0_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && !TARGET_SH2E"
+  "#"
+  "&& reload_completed"
+  [(set (match_dup 0) (reg:DF R0_REG))]
+  "emit_insn (gen_extendsfdf2_i1_r0 (operands[1]));"
+  [(set_attr "type" "sfunc")])
+
+(define_insn "extendsfdf2_i1_r0"
+  [(set (reg:DF R0_REG) (float_extend:DF (reg:SF R4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (use (match_operand:SI 0 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && !TARGET_SH2E"
+  "jsr	@%0%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn_and_split "extendsfdf2_i2e"
+  [(set (match_operand:DF 0 "arith_reg_dest" "=r")
+	(float_extend:DF (reg:SF FR4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R0_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (clobber (reg:SI R4_REG))
+   (clobber (reg:SI FPUL_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && TARGET_SH2E"
+  "#"
+  "&& reload_completed"
+  [(set (match_dup 0) (reg:DF R0_REG))]
+  "emit_insn (gen_extendsfdf2_i2e_r0 (operands[1]));"
+  [(set_attr "type" "sfunc")])
+
+(define_insn "extendsfdf2_i2e_r0"
+  [(set (reg:DF R0_REG) (float_extend:DF (reg:SF FR4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (clobber (reg:SI R4_REG))
+   (clobber (reg:SI FPUL_REG))
+   (use (match_operand:SI 0 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && TARGET_SH2E"
+  "jsr	@%0%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_expand "truncdfsf2"
   [(set (match_operand:SF 0 "fpul_operand" "")
 	(float_truncate:SF (match_operand:DF 1 "fp_arith_reg_operand" "")))]
-  "(TARGET_SH4 || TARGET_SH2A_DOUBLE) || TARGET_SHMEDIA_FPU"
+  "TARGET_FPU_DOUBLE || (TARGET_SH3 && TARGET_OSFP) || TARGET_SHMEDIA_FPU"
   "
 {
   if (TARGET_SH4 || TARGET_SH2A_DOUBLE)
@@ -11083,6 +11857,18 @@ mov.l\\t1f,r0\\n\\
 				       get_fpscr_rtx ()));
       DONE;
     }
+  else if (TARGET_SH2E && TARGET_OSFP)
+    {
+      expand_sfunc_unop (DFmode, &gen_truncdfsf2_i2e, \"__truncdfsf2\",
+			 FLOAT_TRUNCATE, operands);
+      DONE;
+    }
+  else if (TARGET_SH1 && TARGET_OSFP)
+    {
+      expand_sfunc_unop (DFmode, &gen_truncdfsf2_i1, \"__truncdfsf2\",
+			 FLOAT_TRUNCATE, operands);
+      DONE;
+    }
 }")
 
 (define_insn "*truncdfsf2_media"
@@ -11101,6 +11887,36 @@ mov.l\\t1f,r0\\n\\
   [(set_attr "type" "fp")
    (set_attr "fp_mode" "double")])
 
+(define_insn "truncdfsf2_i1"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=z")
+	(float_truncate:SF (reg:DF R4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && !TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "truncdfsf2_i2e"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=w")
+	(float_truncate:SF (reg:DF R4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI FPUL_REG))
+   (clobber (reg:SI R0_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 ;; Bit field extract patterns.  These give better code for packed bitfields,
 ;; because they allow auto-increment addresses to be generated.
 
@@ -11474,6 +12290,28 @@ mov.l\\t1f,r0\\n\\
 	bxor.b\\t%2,@(0,%t1)\;movt\\t%0"
   [(set_attr "length" "6,6")])
 
+(define_expand "lrintsfsi2"
+  [(set (match_operand:SI 0 "general_operand" "")
+	(unspec:SI [(match_operand:SF 1 "fp_arith_reg_operand" "")]
+		   UNSPEC_BUILTIN_ROUND))]
+  "(TARGET_SH4 || TARGET_SH2A_SINGLE) && !optimize_size"
+  "
+{
+  sh_expand_lround (operand0, operand1, 1);
+  DONE;
+}")
+
+(define_expand "lroundsfsi2"
+  [(set (match_operand:SI 0 "general_operand" "")
+	(unspec:SI [(match_operand:SF 1 "fp_arith_reg_operand" "")]
+		   UNSPEC_BUILTIN_ROUND))]
+  "(TARGET_SH4 || TARGET_SH2A_SINGLE) && !optimize_size"
+  "
+{
+  sh_expand_lround (operand0, operand1, 0);
+  DONE;
+}")
+
 
 ;; -------------------------------------------------------------------------
 ;; Peepholes
@@ -11650,24 +12488,30 @@ mov.l\\t1f,r0\\n\\
    && reg_unused_after (operands[0], insn)"
   "fmov{.s|}	@(%0,%1),%2")
 
-;; Switch to a new stack with its address in sp_switch (a SYMBOL_REF).  */
+;; Switch to a new stack with its address in sp_switch (a SYMBOL_REF).
 (define_insn "sp_switch_1"
-  [(const_int 1) (match_operand:SI 0 "symbol_ref_operand" "s")]
+  [(set (reg:SI SP_REG) (unspec_volatile [(match_operand:SI 0 "" "")]
+    UNSPECV_SP_SWITCH_B))]
   "TARGET_SH1"
-  "*
 {
-  output_asm_insn (\"mov.l r0,@-r15\;mov.l %0,r0\", operands);
-  output_asm_insn (\"mov.l @r0,r0\;mov.l r15,@-r0\", operands);
-  return \"mov r0,r15\";
-}"
+  return       "mov.l	r0,@-r15"	"\n"
+	 "	mov.l	%0,r0"		"\n"
+	 "	mov.l	@r0,r0"		"\n"
+	 "	mov.l	r15,@-r0"	"\n"
+	 "	mov	r0,r15";
+}
   [(set_attr "length" "10")])
 
 ;; Switch back to the original stack for interrupt functions with the
-;; sp_switch attribute.  */
+;; sp_switch attribute.
 (define_insn "sp_switch_2"
-  [(const_int 2)]
+  [(unspec_volatile [(const_int 0)]
+    UNSPECV_SP_SWITCH_E)]
   "TARGET_SH1"
-  "mov.l @r15+,r15\;mov.l @r15+,r0"
+{
+  return       "mov.l	@r15,r15"	"\n"
+	 "	mov.l	@r15+,r0";
+}
   [(set_attr "length" "4")])
 
 ;; Integer vector moves
--- gcc/gcc/config/sh/sh.opt	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/config/sh/sh.opt	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -2,6 +2,7 @@
 
 ; Copyright (C) 2005, 2006, 2007, 2008, 2009, 2010, 2011
 ; Free Software Foundation, Inc.
+; Copyright (c) 2011  STMicroelectronics.
 ;
 ; This file is part of GCC.
 ;
@@ -22,7 +23,7 @@
 ;; Used for various architecture options.
 Mask(SH_E)
 
-;; Set if the default precision of th FPU is single.
+;; Set if the default precision of the FPU is single.
 Mask(FPU_SINGLE)
 
 ;; Set if we should generate code using type 2A insns.
@@ -205,9 +206,9 @@ maccumulate-outgoing-args
 Target Report Var(TARGET_ACCUMULATE_OUTGOING_ARGS) Init(1)
 Reserve space for outgoing arguments in the function prologue
 
-madjust-unroll
-Target Report Mask(ADJUST_UNROLL) Condition(SUPPORT_ANY_SH5)
-Throttle unrolling to avoid thrashing target registers unless the unroll benefit outweighs this
+malign-small-blocks=
+Target RejectNegative Joined UInteger Var(sh_align_small_blocks) Init(32)
+Honor align-jump-loops for basic block bigger than number of instructions.
 
 mb
 Target Report RejectNegative InverseMask(LITTLE_ENDIAN)
@@ -241,9 +242,13 @@ mdalign
 Target Report RejectNegative Mask(ALIGN_DOUBLE)
 Align doubles at 64-bit boundaries
 
+mdb-page-bug
+Target Report RejectNegative Var(TARGET_DBHWBUG)
+Undocumented
+
 mdiv=
 Target RejectNegative Joined Var(sh_div_str) Init("")
-Division strategy, one of: call, call2, fp, inv, inv:minlat, inv20u, inv20l, inv:call, inv:call2, inv:fp, call-div1, call-fp, call-table
+Division strategy, one of: call, call2, fp, inv, inv:minlat, inv20u, inv20l, inv:call, inv:call2, inv:fp, call-div1, call-fp, call-table, call-pre1
 
 mdivsi3_libfunc=
 Target RejectNegative Joined Var(sh_divsi3_libfunc) Init("")
@@ -253,6 +258,10 @@ mfmovd
 Target RejectNegative Mask(FMOVD)
 Enable the use of 64-bit floating point registers in fmov instructions.  See -mdalign if 64-bit alignment is required.
 
+mfldi
+Target Var(TARGET_FLDI) Init(1)
+Allow the use of fldi0/1 instructions.
+
 mfixed-range=
 Target RejectNegative Joined Var(sh_fixed_range_str)
 Specify range of registers to make fixed
@@ -293,6 +302,14 @@ ml
 Target Report RejectNegative Mask(LITTLE_ENDIAN)
 Generate code in little endian mode
 
+mlate-r0r3-to-reg-mul
+Target RejectNegative Var(TARGET_R0R3_TO_REG_MUL, 1) VarExists
+Assume availability of integer multiply instruction (src only opd in r0-r3), but only try to use this instruction after register allocation.
+
+mdead-delay
+Target Report Mask(DEAD_DELAY)
+Try to eliminate a dead delay slot instruction.
+
 mnomacsave
 Target Report RejectNegative Mask(NOMACSAVE)
 Mark MAC register as call-clobbered
@@ -311,6 +328,10 @@ mpt-fixed
 Target Report Mask(PT_FIXED) Condition(SUPPORT_ANY_SH5)
 Assume pt* instructions won't trap
 
+mr0r3-to-reg-mul
+Target Var(TARGET_R0R3_TO_REG_MUL, 2) Init(-1)
+Assume availability of integer multiply instruction (src only opd in r0-r3)
+
 mrelax
 Target Report RejectNegative Mask(RELAX)
 Shorten address references during linking
@@ -323,9 +344,9 @@ msoft-atomic
 Target Report Mask(SOFT_ATOMIC)
 Use software atomic sequences supported by kernel
 
-mspace
-Target RejectNegative Alias(Os)
-Deprecated.  Use -Os instead
+mtas
+Target Var(TARGET_TAS) 
+Allow TAS.B instruction.
 
 multcost=
 Target RejectNegative Joined UInteger Var(sh_multcost) Init(-1)
@@ -340,3 +361,6 @@ Don't generate privileged-mode only code; implies
 mpretend-cmove
 Target Var(TARGET_PRETEND_CMOVE)
 Pretend a branch-around-a-move is a conditional move.
+
+
+
--- gcc/gcc/config/sh/ieee-754-df.S	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/gcc/config/sh/ieee-754-df.S	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,794 @@
+/* Copyright (C) 2006, 2008, 2009 Free Software Foundation, Inc.
+   Copyright (c) 2010 STMicroelectronics.
+	
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 3, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   Under Section 7 of GPL version 3, you are granted additional
+   permissions described in the GCC Runtime Library Exception, version
+   3.1, as published by the Free Software Foundation.
+
+   You should have received a copy of the GNU General Public License and
+   a copy of the GCC Runtime Library Exception along with this program;
+   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+   <http://www.gnu.org/licenses/>.  */
+
+!! libgcc software floating-point routines for Renesas SH /
+!! STMicroelectronics ST40 CPUs
+!! Contributed by J"orn Rennecke joern.rennecke@st.com
+
+#ifndef __SH_FPU_DOUBLE__
+
+#include "lib1funcs.h"
+#include "insn-constants.h"
+
+/* Double-precision floating-point emulation.
+   We handle NANs, +-infinity, and +-zero.
+   However, we assume that for NANs, the topmost bit of the fraction is set.  */
+
+#ifdef __LITTLE_ENDIAN__
+#define DBL0L r4
+#define DBL0H r5
+#define DBL1L r6
+#define DBL1H r7
+#define DBLRL r0
+#define DBLRH r1
+#else
+#define DBL0L r5
+#define DBL0H r4
+#define DBL1L r7
+#define DBL1H r6
+#define DBLRL r1
+#define DBLRH r0
+#endif
+
+#ifdef __SH_FPU_ANY__
+#define RETURN_R0_MAIN
+#define RETURN_R0 bra LOCAL(return_r0)
+#define RETURN_FR0 \
+LOCAL(return_r0): \
+ lds r0,fpul; \
+ rts; \
+ fsts fpul,fr0
+#define ARG_TO_R4 \
+ flds fr4,fpul; \
+ sts fpul,r4
+#else /* ! __SH_FPU_ANY__ */
+#define RETURN_R0_MAIN rts
+#define RETURN_R0 rts
+#define RETURN_FR0
+#define ARG_TO_R4
+#endif /* ! __SH_FPU_ANY__ */
+
+#ifdef L_nedf2f
+/* -fno-finite-math-only -mb inline version, T := r4:DF == r6:DF
+	cmp/eq	r5,r7
+	mov	r4,r0
+	bf	0f
+	cmp/eq	r4,r6
+	bt	0f
+	or	r6,r0
+	add	r0,r0
+	or	r5,r0
+	tst	r0,r0
+	0:			*/
+	.balign 4
+	.global GLOBAL(nedf2f)
+	HIDDEN_FUNC(GLOBAL(nedf2f))
+GLOBAL(nedf2f):
+	cmp/eq	DBL0L,DBL1L
+	bf.s 	LOCAL(ne)
+	mov     #1,r0
+	cmp/eq	DBL0H,DBL1H
+	mov.l   LOCAL(c_DF_NAN_MASK),r1
+	bt.s	LOCAL(check_nan)
+	not	DBL0H,r0	
+	mov	DBL0H,r0
+	or	DBL1H,r0
+	add	r0,r0
+	rts
+	or	DBL0L,r0
+LOCAL(check_nan):
+	tst	r1,r0
+	bt.s 	LOCAL(nan)
+	mov	#12,r2
+	shll16  r2
+	xor 	r2,r1
+	tst 	r1,r0
+LOCAL(nan):	
+	movt	r0
+LOCAL(ne):
+	rts
+	nop
+	
+	.balign 4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(GLOBAL(nedf2f))
+#endif /* L_nedf2f */
+
+#ifdef L_unord_df
+	.balign 4
+	.global GLOBAL(unorddf2)
+	HIDDEN_FUNC(GLOBAL(unorddf2))
+GLOBAL(unorddf2):
+	mov.l	LOCAL(c_DF_NAN_MASK),r1
+	not	DBL0H,r0
+	tst	r1,r0
+	not	r6,r0
+	bt	LOCAL(unord)
+	tst	r1,r0
+LOCAL(unord):
+	rts
+	movt	r0
+	.balign	4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(GLOBAL(unorddf2))
+#endif /* L_unord_df */
+
+#if defined(L_gtdf2t) || defined(L_gtdf2t_trap)
+/* -fno-finite-math-only version of _gt_df */
+#ifdef L_gtdf2t
+#define fun_label GLOBAL(gtdf2t)
+#else
+#define fun_label GLOBAL(gtdf2t_trap)
+#endif
+	.balign 4
+	.global fun_label
+	HIDDEN_FUNC(fun_label)
+fun_label:
+	/* If the raw values compare greater, the result true, unless
+	   any of them is a nan (but infinity is fine), or both values are
+	   +- zero.  Otherwise, the result false.  */
+	mov.l	LOCAL(c_DF_NAN_MASK),r1
+	cmp/pz	DBL0H
+	not	DBL1H,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	DBL0H,r0
+	bt	LOCAL(nan) /* return zero if DBL1 is NAN.  */
+	cmp/eq	DBL1H,DBL0H
+	bt	LOCAL(cmp_low)
+	cmp/gt	DBL1H,DBL0H
+	or	DBL1H,r0
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/gt	DBL0H,r1)
+	add	r0,r0
+	bf	LOCAL(nan) /* return zero if DBL0 is NAN.  */
+	or	DBL0L,r0
+	rts
+	or	DBL1L,r0 /* non-zero unless both DBL0 and DBL1 are +-zero.  */
+LOCAL(cmp_low):
+	cmp/hi	DBL1L,DBL0L
+	rts
+	movt	r0
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan) /* return zero if DBL1 is NAN.  */
+	cmp/eq	DBL1H,DBL0H
+	SLC(bt,	LOCAL(neg_cmp_low),
+	 cmp/hi	DBL0L,DBL1L)
+	not	DBL0H,r0
+	tst	r1,r0
+	bt	LOCAL(nan) /* return zero if DBL0 is NAN.  */
+	cmp/hi	DBL0H,DBL1H
+	SLI(rts	!,)
+	SLI(movt r0 !,)
+LOCAL(neg_cmp_low):
+	SLI(cmp/hi	DBL0L,DBL1L)
+	rts
+	movt	r0
+LOCAL(check_nan):
+#ifdef L_gtdf2t
+LOCAL(nan):
+	rts
+	mov	#0,r0
+#else
+	SLI(cmp/gt DBL0H,r1)
+	bf	LOCAL(nan) /* return zero if DBL0 is NAN.  */
+	rts
+	mov	#0,r0
+LOCAL(nan):
+	mov	#0,r0
+	trapa	#0
+#endif
+	.balign	4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(fun_label)
+#endif /* defined(L_gtdf2t) || defined(L_gtdf2t_trap) */
+
+#ifdef L_gedf2f
+	.balign 4
+	.global GLOBAL(gedf2f)
+	HIDDEN_FUNC(GLOBAL(gedf2f))
+GLOBAL(gedf2f):
+	/* -fno-finite-math-only version of _ge_df */
+	/* If the raw values compare greater or equal, the result is
+	   true, unless any of them is a nan, or both are the
+	   same infinity.  If both are -+zero, the result is true;
+	   otherwise, it is false.
+	   We use 0 as true and nonzero as false for this function.  */
+	mov.l	LOCAL(c_DF_NAN_MASK),r1
+	cmp/pz	DBL1H
+	not	DBL0H,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	DBL0H,r0
+	bt	LOCAL(nan)
+	cmp/eq	DBL0H,DBL1H
+	bt	LOCAL(cmp_low)
+	cmp/gt	DBL0H,DBL1H
+	or	DBL1H,r0
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/ge	r1,DBL1H)
+	add	r0,r0
+	bt	LOCAL(nan)
+	or	DBL0L,r0
+	rts
+	or	DBL1L,r0
+LOCAL(cmp_low):
+	cmp/hi	DBL0L,DBL1L
+#if defined(L_gedf2f) && defined(DELAYED_BRANCHES)
+LOCAL(nan): LOCAL(check_nan):
+#endif
+	rts
+	movt	r0
+#if defined(L_gedf2f) && ! defined(DELAYED_BRANCHES)
+LOCAL(check_nan):
+	SLI(cmp/ge	r1,DBL1H)
+LOCAL(nan):
+	rts
+	movt	r0
+#elif defined(L_gedf2f_trap)
+LOCAL(check_nan):
+	SLI(cmp/ge	r1,DBL1H)
+	bt	LOCAL(nan)
+	rts
+LOCAL(nan):
+	movt	r0
+	trapa	#0
+#endif /* L_gedf2f_trap */
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan)
+	cmp/eq	DBL0H,DBL1H
+	not	DBL1H,r0
+	SLC(bt,	LOCAL(neg_cmp_low),
+	 cmp/hi	DBL1L,DBL0L)
+	tst	r1,r0
+	bt	LOCAL(nan)
+	cmp/hi	DBL1H,DBL0H
+	SLI(rts !,)
+	SLI(movt	r0 !,)
+LOCAL(neg_cmp_low):
+	SLI(cmp/hi	DBL1L,DBL0L)
+	rts
+	movt	r0
+	.balign	4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(GLOBAL(gedf2f))
+#endif /* L_gedf2f */
+
+#ifndef DYN_SHIFT /* SH1 / SH2 code */
+#ifdef L_sf_to_df
+	.balign 4
+	.global GLOBAL(extendsfdf2)
+	FUNC(GLOBAL(extendsfdf2))
+GLOBAL(extendsfdf2):
+	ARG_TO_R4
+	mov.l	LOCAL(x7f800000),r3
+	mov	r4,DBLRL
+	tst	r3,r4
+	bt	LOCAL(zero_denorm)
+	mov.l	LOCAL(xe0000000),r2
+	rotr	DBLRL
+	rotr	DBLRL
+	rotr	DBLRL
+	and	r2,DBLRL
+	mov	r4,DBLRH
+	not	r4,r2
+	tst	r3,r2
+	mov.l	LOCAL(x38000000),r2
+	bf	0f
+	add	r2,r2	! infinity / NaN adjustment
+0:	shll	DBLRH
+	shlr2	DBLRH
+	shlr2	DBLRH
+	add	DBLRH,DBLRH
+	rotcr	DBLRH
+	rts
+	add	r2,DBLRH
+LOCAL(zero_denorm):
+	mov.l	r4,@-r15
+	add	r4,r4
+	tst	r4,r4
+	bt	LOCAL(zero)
+	shlr8	r3	/* 0x007f8000 */
+	mov.w	LOCAL(x389),r2
+LOCAL(shift_byte):
+	tst	r3,r4
+	shll8	r4
+	SL(bt,	LOCAL(shift_byte),
+	 add	#-8,r2)
+LOCAL(shift_bit):
+	shll	r4
+	SL(bf,	LOCAL(shift_bit),
+	 add	#-1,r2)
+	mov	#0,DBLRL
+	mov	r4,DBLRH
+	mov.l	@r15+,r4
+	shlr8	DBLRH
+	shlr2	DBLRH
+	shlr	DBLRH
+	rotcr	DBLRL
+	cmp/gt	r4,DBLRH	! get sign
+	rotcr	DBLRH
+	rotcr	DBLRL
+	shll16	r2
+	shll8	r2
+	rts
+	add	r2,DBLRH
+LOCAL(zero):
+	mov.l	@r15+,DBLRH
+	rts
+	mov	#0,DBLRL
+LOCAL(x389):	.word 0x389
+	.balign	4
+LOCAL(x7f800000):
+	.long	0x7f800000
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(xe0000000):
+	.long	0xe0000000
+	ENDFUNC(GLOBAL(extendsfdf2))
+#endif /* L_sf_to_df */
+
+#ifdef L_df_to_sf
+	.balign 4
+	.global GLOBAL(truncdfsf2)
+	FUNC(GLOBAL(truncdfsf2))
+GLOBAL(truncdfsf2):
+	mov.l	LOCAL(x38000000),r3	! exponent adjustment DF -> SF
+	mov	DBL0H,r1
+	mov.l	LOCAL(x70000000),r2	! mask for out-of-range exponent bits
+	mov	DBL0H,r0
+	mov.l	DBL0L,@-r15
+	sub	r3,r1
+	tst	r2,r1
+	shll8	r0			!
+	shll2	r0			! Isolate highpart fraction.
+	shll2	r0			!
+	bf	LOCAL(ill_exp)
+	shll2	r1
+	mov.l	LOCAL(x2fffffff),r2 /* Fraction lsb | lower guard bits.  */
+	shll2	r1
+	mov.l	LOCAL(xff000000),r3
+	shlr8	r0
+	tst	r2,DBL0L /* Check if msb guard bit wants rounding up.  */
+	shlr16	DBL0L
+	shlr8	DBL0L
+	shlr2	DBL0L
+	SL1(bt,	LOCAL(add_frac),
+	 shlr2	DBL0L)
+	add	#1,DBL0L
+LOCAL(add_frac):
+	add	DBL0L,r0
+	mov.l	LOCAL(x01000000),r2
+	and	r3,r1
+	mov.l	@r15+,DBL0L
+	add	r1,r0
+	tst	r3,r0
+	bt	LOCAL(inf_denorm0)
+	cmp/hs	r3,r0
+LOCAL(denorm_noup_sh1):
+	bt	LOCAL(inf)
+	div0s	DBL0H,r2	/* copy orig. sign into T.  */
+RETURN_R0_MAIN
+	rotcr	r0
+RETURN_FR0
+LOCAL(inf_denorm0):	!  We might need to undo previous rounding.
+	mov.l	LOCAL(x2fffffff),r3 /* Old fraction lsb | lower guard bits.  */
+	tst	r1,r1
+	bf	LOCAL(inf)
+	add	#-1,r0
+	tst	r3,DBL0L /* Check if msb guard bit was rounded up.  */
+	mov.l	LOCAL(x5fffffff),r3 /* Fraction lsb | lower guard bits.  */
+	addc	r2,r0
+	shlr	r0
+	tst	r3,DBL0L /* Check if msb guard bit wants rounding up.  */
+#ifdef DELAYED_BRANCHES
+	bt/s	LOCAL(denorm_noup)
+#else
+	bt	LOCAL(denorm_noup_sh1)
+#endif
+	div0s	DBL0H,r2	/* copy orig. sign into T.  */
+	add	#1,r0
+LOCAL(denorm_noup):
+	RETURN_R0
+	rotcr	r0
+LOCAL(ill_exp):
+	div0s	DBL0H,r1
+	mov.l	LOCAL(x7ff80000),r2
+	add	r1,r1
+	bf	LOCAL(inf_nan)
+	mov.w	LOCAL(m32),r3 /* Handle denormal or zero.  */
+	shlr16	r1
+	exts.w	r1,r1
+	shll2	r1
+	add	r1,r1
+	shlr8	r1
+	exts.w	r1,r1
+	add	#-8,r1	/* Go from 9 to 1 guard bit in MSW.  */
+	cmp/gt	r3,r1
+	mov.l	@r15+,r3 /* DBL0L */
+	bf	LOCAL(zero)
+	mov.l	DBL0L, @-r15
+	shll8	DBL0L
+	rotcr	r0	/* Insert leading 1.  */
+	shlr16	r3
+	shll2	r3
+	add	r3,r3
+	shlr8	r3
+	cmp/pl	DBL0L	/* Check lower 23 guard bits if guard bit 23 is 0.  */
+	addc	r3,r0	/* Assemble fraction with compressed guard bits.  */
+	mov.l	@r15+,DBL0L
+	mov	#0,r2
+	neg	r1,r1
+LOCAL(denorm_loop):
+	shlr	r0
+	rotcl	r2
+	dt	r1
+	bf	LOCAL(denorm_loop)
+	tst	#2,r0
+	rotcl	r0
+	tst	r2,r2
+	rotcl	r0
+	xor	#3,r0
+	add	#3,r0	/* Even overflow gives the correct result.  */
+	shlr2	r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(zero):
+	mov	#0,r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(inf_nan):
+	not	DBL0H,r0
+	tst	r2,r0
+	mov.l	@r15+,DBL0L
+	bf	LOCAL(inf)
+	RETURN_R0
+	mov	#-1,r0	/* NAN */
+LOCAL(inf):	/* r2 must be positive here.  */
+	mov.l	LOCAL(xff000000),r0
+	div0s	r2,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(m32):
+	.word	-32
+	.balign	4
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(x70000000):
+	.long	0x70000000
+LOCAL(x2fffffff):
+	.long	0x2fffffff
+LOCAL(x01000000):
+	.long	0x01000000
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(x5fffffff):
+	.long	0x5fffffff
+LOCAL(x7ff80000):
+	.long	0x7ff80000
+	ENDFUNC(GLOBAL(truncdfsf2))
+#endif /*  L_df_to_sf */
+#ifdef L_addsub_df
+#include "IEEE-754/adddf3.S"
+#endif /* _addsub_df */
+
+#ifdef L_mul_df
+#include "IEEE-754/muldf3.S"
+#endif /* L_mul_df */
+
+#ifdef L_df_to_usi
+#include "IEEE-754/fixunsdfsi.S"
+#endif /* L_df_to_usi */
+
+#ifdef L_df_to_si
+#include "IEEE-754/fixdfsi.S"
+#endif /* L_df_to_si */
+
+#ifdef L_usi_to_df
+#include "IEEE-754/floatunssidf.S"
+#endif /* L_usi_to_df */
+
+#ifdef L_si_to_df
+#include "IEEE-754/floatsidf.S"
+#endif /* L_si_df */
+
+#ifdef L_div_df
+#include "IEEE-754/divdf3.S"
+#endif /* L_div_df */
+#endif /* ! DYN_SHIFT */
+
+/* The actual arithmetic uses dynamic shift.  Supporting SH1 / SH2 here would
+   make this code too hard to maintain, so if you want to add SH1 / SH2
+   support, do it in a separate copy.  */
+#ifdef DYN_SHIFT
+#ifdef L_sf_to_df
+	.balign 4
+	.global GLOBAL(extendsfdf2)
+	FUNC(GLOBAL(extendsfdf2))
+GLOBAL(extendsfdf2):
+	ARG_TO_R4
+	mov.l	LOCAL(x7f800000),r2
+	mov	#29,r3
+	mov	r4,DBLRL
+	not	r4,DBLRH
+	tst	r2,r4
+	shld	r3,DBLRL
+	bt	LOCAL(zero_denorm)
+	mov	#-3,r3
+	tst	r2,DBLRH
+	mov	r4,DBLRH
+	mov.l	LOCAL(x38000000),r2
+	bt/s	LOCAL(inf_nan)
+	 shll	DBLRH
+	shld	r3,DBLRH
+	rotcr	DBLRH
+	rts
+	add	r2,DBLRH
+	.balign	4
+LOCAL(inf_nan):
+	shld	r3,DBLRH
+	add	r2,r2
+	rotcr	DBLRH
+	rts
+	add	r2,DBLRH
+LOCAL(zero_denorm):
+	mov.l	r4,@-r15
+	add	r4,r4
+	tst	r4,r4
+	extu.w	r4,r2
+	bt	LOCAL(zero)
+	cmp/eq	r4,r2
+	extu.b	r4,r1
+	mov.l	LOCAL(c__clz_tab),r0
+	bf	LOCAL(three_bytes)
+	nop
+	cmp/eq	r4,r1
+	mov	#22,DBLRH
+	bt	LOCAL(one_byte)
+	shlr8	r2
+	mov	#14,DBLRH
+LOCAL(one_byte):
+#ifdef __pic__
+	add	r0,r2
+	mova  LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r2),r2
+	mov	#21,r3
+	mov.w	LOCAL(x0),DBLRL
+	sub	r2,DBLRH
+LOCAL(norm_shift):
+	shld	DBLRH,r4
+	neg	DBLRH,DBLRH
+	mov.l	@r15+,r2
+	shld	r3,DBLRH
+	mov.l	LOCAL(x6fa00000),r3
+	add	r4,DBLRH
+	mov r2,r4
+	add	r3,DBLRH
+
+	div0s	r3,r4
+	rts
+	rotcr	DBLRH
+LOCAL(three_bytes):
+	mov	r4,r2
+	shlr16	r2
+#ifdef __pic__
+	add	r0,r2
+	mova  LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r2),r2
+	mov	#21,r3
+	mov	#6+10,DBLRH
+	sub	r2,DBLRH
+	mov	r4,DBLRL
+	shld	r3,DBLRL
+	shld	DBLRH,DBLRL
+	bra	LOCAL(norm_shift)
+	add	#-10,DBLRH
+LOCAL(zero):
+	rts	/* DBLRL has already been zeroed above.  */
+	mov.l @r15+,DBLRH
+LOCAL(x0):
+	.word 0
+	.balign	4
+LOCAL(x7f800000):
+	.long	0x7f800000
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(x6fa00000):
+	/* Flip sign back, do exponent adjustment, and remove leading one.  */
+	.long 0x6fa00000 
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+	ENDFUNC(GLOBAL(extendsfdf2))
+#endif /* L_sf_to_df */
+
+#ifdef L_df_to_sf
+	.balign 4
+	.global GLOBAL(truncdfsf2)
+	FUNC(GLOBAL(truncdfsf2))
+GLOBAL(truncdfsf2):
+	mov.l	LOCAL(x38000000),r3
+	mov	DBL0H,r1
+	mov.l	LOCAL(x70000000),r2
+	mov	DBL0H,r0
+	sub	r3,r1
+	mov.l	DBL0L,@-r15
+	tst	r2,r1
+	mov	#12,r3
+	shld	r3,r0			! Isolate highpart fraction.
+	bf	LOCAL(ill_exp)
+	shll2	r1
+	mov.l	LOCAL(x2fffffff),r2 /* Fraction lsb | lower guard bits.  */
+	shll2	r1
+	mov.l	LOCAL(xff000000),r3
+	shlr8	r0
+	tst	r2,DBL0L /* Check if msb guard bit wants rounding up.  */
+	mov	#-28,r2
+	bt/s	LOCAL(add_frac)
+	 shld	r2,DBL0L
+	add	#1,DBL0L
+LOCAL(add_frac):
+	add	DBL0L,r0
+	mov.l	LOCAL(x01000000),r2
+	and	r3,r1
+	mov.l	@r15+,DBL0L
+	add	r1,r0
+	tst	r3,r0
+	bt	LOCAL(inf_denorm0)
+	cmp/hs	r3,r0
+	bt	LOCAL(inf)
+	div0s	DBL0H,r2	/* copy orig. sign into T.  */
+	RETURN_R0_MAIN
+	rotcr	r0
+RETURN_FR0
+LOCAL(inf_denorm0):	! We might need to undo previous rounding.
+	mov.l	LOCAL(x2fffffff),r3 /* Old fraction lsb | lower guard bits.  */
+	tst	r1,r1
+	bf	LOCAL(inf)
+	add	#-1,r0
+	tst	r3,DBL0L /* Check if msb guard bit was rounded up.  */
+	mov.l	LOCAL(x5fffffff),r3 /* Fraction lsb | lower guard bits.  */
+	addc	r2,r0
+	shlr	r0
+	tst	r3,DBL0L /* Check if msb guard bit wants rounding up.  */
+	bt/s	LOCAL(denorm_noup)
+	 div0s	DBL0H,r2	/* copy orig. sign into T.  */
+	add	#1,r0
+LOCAL(denorm_noup):
+	RETURN_R0
+	rotcr	r0
+LOCAL(ill_exp):
+	div0s	DBL0H,r1
+	mov.l	LOCAL(x7ff80000),r2
+	add	r1,r1
+	bf	LOCAL(inf_nan)
+	mov.w	LOCAL(m32),r3 /* Handle denormal or zero.  */
+	mov	#-21,r2
+	shad	r2,r1
+	add	#-8,r1	/* Go from 9 to 1 guard bit in MSW.  */
+	cmp/gt	r3,r1
+	mov.l	@r15+,r3 /* DBL0L */
+	bf	LOCAL(zero)
+	mov.l	DBL0L, @-r15
+	shll8	DBL0L
+	rotcr	r0	/* Insert leading 1.  */
+	shld	r2,r3
+	cmp/pl	DBL0L	/* Check lower 23 guard bits if guard bit 23 is 0.  */
+	addc	r3,r0	/* Assemble fraction with compressed guard bits.  */
+	mov	r0,r2
+	shld	r1,r0
+	mov.l	@r15+,DBL0L
+	add	#32,r1
+	shld	r1,r2
+	tst	#2,r0
+	rotcl	r0
+	tst	r2,r2
+	rotcl	r0
+	xor	#3,r0
+	add	#3,r0	/* Even overflow gives the correct result.  */
+	shlr2	r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(zero):
+	mov	#0,r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(inf_nan):
+	not	DBL0H,r0
+	tst	r2,r0
+	mov.l	@r15+,DBL0L
+	bf	LOCAL(inf)
+	RETURN_R0
+	mov	#-1,r0	/* NAN */
+LOCAL(inf):	/* r2 must be positive here.  */
+	mov.l	LOCAL(xff000000),r0
+	div0s	r2,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(m32):
+	.word	-32
+	.balign	4
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(x70000000):
+	.long	0x70000000
+LOCAL(x2fffffff):
+	.long	0x2fffffff
+LOCAL(x01000000):
+	.long	0x01000000
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(x5fffffff):
+	.long	0x5fffffff
+LOCAL(x7ff80000):
+	.long	0x7ff80000
+	ENDFUNC(GLOBAL(truncdfsf2))
+#endif /* L_df_to_sf */
+
+
+#ifdef L_addsub_df
+#include "IEEE-754/m3/adddf3.S"
+#endif /* _addsub_df */
+
+#ifdef L_mul_df
+#include "IEEE-754/m3/muldf3.S"
+#endif /* L_mul_df */
+
+#ifdef L_df_to_usi
+#include "IEEE-754/m3/fixunsdfsi.S"
+#endif /* L_df_to_usi */
+
+#ifdef L_df_to_si
+#include "IEEE-754/m3/fixdfsi.S"
+#endif /* L_df_to_si */
+
+#ifdef L_usi_to_df
+#include "IEEE-754/m3/floatunssidf.S"
+#endif /* L_usi_to_df */
+
+#ifdef L_si_to_df
+#include "IEEE-754/m3/floatsidf.S"
+#endif /* L_si_to_df */
+
+#ifdef L_div_df
+#include "IEEE-754/m3/divdf3.S"
+#endif /* L_div_df */
+#endif /* DYN_SHIFT */
+
+#endif /* __SH_FPU_DOUBLE__ */
--- gcc/gcc/stmt.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/stmt.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1515,6 +1515,7 @@ warn_if_unused_value (const_tree exp, location_t l
 
     case SAVE_EXPR:
     case NON_LVALUE_EXPR:
+    case NOP_EXPR:
       exp = TREE_OPERAND (exp, 0);
       goto restart;
 
--- gcc/gcc/collect2.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/collect2.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -2660,6 +2660,8 @@ scan_prog_file (const char *prog_name, scanpass wh
           if (found_lto)
             continue;
 
+	  fatal_error ("__gnu_lto_v1 not implemented");
+
           /* Look for the LTO info marker symbol, and add filename to
              the LTO objects list if found.  */
           for (p = buf; (ch = *p) != '\0' && ch != '\n'; p++)
--- gcc/gcc/convert.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/convert.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -583,7 +583,6 @@ convert_to_integer (tree type, tree expr)
       else if (outprec >= inprec)
 	{
 	  enum tree_code code;
-	  tree tem;
 
 	  /* If the precision of the EXPR's type is K bits and the
 	     destination mode has more bits, and the sign is changing,
@@ -601,13 +600,7 @@ convert_to_integer (tree type, tree expr)
 	  else
 	    code = NOP_EXPR;
 
-	  tem = fold_unary (code, type, expr);
-	  if (tem)
-	    return tem;
-
-	  tem = build1 (code, type, expr);
-	  TREE_NO_WARNING (tem) = 1;
-	  return tem;
+	  return fold_build1 (code, type, expr);
 	}
 
       /* If TYPE is an enumeral type or a type with a precision less
--- gcc/gcc/reload1.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/gcc/reload1.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1631,6 +1631,9 @@ calculate_elim_costs_all_insns (void)
 	    {
 	      rtx set = single_set (insn);
 
+	      if (set && GET_CODE (set) == PARALLEL)
+		continue;
+
 	      /* Skip insns that only set an equivalence.  */
 	      if (set && REG_P (SET_DEST (set))
 		  && reg_renumber[REGNO (SET_DEST (set))] < 0
@@ -3648,6 +3651,7 @@ elimination_costs_in_insn (rtx insn)
     {
       gcc_assert (GET_CODE (PATTERN (insn)) == USE
 		  || GET_CODE (PATTERN (insn)) == CLOBBER
+		  || GET_CODE (PATTERN (insn)) == PARALLEL
 		  || GET_CODE (PATTERN (insn)) == ADDR_VEC
 		  || GET_CODE (PATTERN (insn)) == ADDR_DIFF_VEC
 		  || GET_CODE (PATTERN (insn)) == ASM_INPUT
@@ -3898,7 +3902,7 @@ set_initial_elim_offsets (void)
 /* Subroutine of set_initial_label_offsets called via for_each_eh_label.  */
 
 static void
-set_initial_eh_label_offset (rtx label)
+set_initial_eh_label_offset (rtx label, void *ignored ATTRIBUTE_UNUSED)
 {
   set_label_offsets (label, NULL_RTX, 1);
 }
@@ -3924,7 +3928,7 @@ set_initial_label_offsets (void)
     if (XEXP (x, 0))
       set_label_offsets (XEXP (x, 0), NULL_RTX, 1);
 
-  for_each_eh_label (set_initial_eh_label_offset);
+  for_each_eh_label (set_initial_eh_label_offset, NULL);
 }
 
 /* Set all elimination offsets to the known values for the code label given
--- gcc/libstdc++-v3/src/c++98/mt_allocator.cc	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libstdc++-v3/src/c++98/mt_allocator.cc	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -48,6 +48,7 @@ namespace
 	{
 	  __gthread_key_delete(_M_key);
 	  ::operator delete(static_cast<void*>(_M_thread_freelist_array));
+	  _M_thread_freelist = NULL;
 	}
     }
   };
--- gcc/libstdc++-v3/ChangeLog.STM	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libstdc++-v3/ChangeLog.STM	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,46 @@
+2012-04-25  Laurent Alfonsi  <laurent.alfonsi@st.com>
+
+        * testsuite/lib/libstdc++.exp (libstdc++_init): uses 
+        GCC_UNDER_TEST, GXX_UNDER_TEST. 
+        (v3_target_compile, v3_target_compile_as_c): Appends 
+	additional linker flag if exists.
+
+2012-04-17  Antony King  <antony.king@st.com>
+
+        * include/ext/concurrence.h (~__mutex): Check _M_init instead of
+        __gthread_active_p.
+        (~__recursive_mutex): Likewise.
+        (__recursive_mutex::_S_destroy): New template function for default
+        implementation using __gthread_recursive_mutex_t.
+        * include/std/mutex (__destroy_recursive_mutex::_S_destroy): Likewise.
+
+2012-04-13  Laurent Alfonsi  <laurent.alfonsi@st.com>
+
+	PR libstdc++/52604
+	* src/c++98/mt_allocator.cc: (__freelist::~__freelist): Reset pointer.
+
+2011-11-23  Christian Bruel  <christian.bruel@st.com>
+
+	* testsuite/23_containers/vector/bool/modifiers/insert/31370.cc:
+	xfail for sh-superh-elf.
+
+2009-10-06  Antony King  <antony.king@st.com>
+
+	INSbl30052:
+	* configure.host: Enable atomic builtins for sh*-superh-elf.
+	* configure: Regenerate.
+
+2009-02-24  Antony King  <antony.king@st.com>
+	    Christian Bruel  <christian.bruel@st.com>
+
+	INSbl28513:
+	* include/ext/concurrence.h (__scoped_gmutex_lock): Defined.
+	(__mutex:_M_init): Declare and initialize.
+	(__recursive_mutex:_M_init): Idem.
+	(__mutex:lock): Initialize mutex if needed.
+	(__recursive_mutex:lock): Idem.
+	* libsupc++/eh_globals.cc (__cxa_get_globals): Initialize eh_globals.
+	(__eh_globals_init:_M_create): New function
+	(__eh_globals_init): Initialize _M_once.
+	(__cxa_get_globals): Call init_create once.
+
--- gcc/libstdc++-v3/include/ext/concurrence.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libstdc++-v3/include/ext/concurrence.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -140,6 +140,63 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION
   }
 #endif
  
+#if __GTHREADS
+  class __scoped_gmutex_lock
+  {
+  private:
+    __scoped_gmutex_lock(const __scoped_gmutex_lock&);
+    __scoped_gmutex_lock& operator=(const __scoped_gmutex_lock&);
+
+    class __mutex_type
+    {
+    public:
+      __gthread_mutex_t _M_mutex;
+      __gthread_once_t  _M_once;
+
+      __mutex_type() : _M_once(__GTHREAD_ONCE_INIT) { }
+
+      ~__mutex_type() { }
+
+      void
+      _M_create()
+      {
+#if defined __GTHREAD_MUTEX_INIT
+	__gthread_mutex_t __tmp = __GTHREAD_MUTEX_INIT;
+	_M_mutex = __tmp;
+#else
+	__GTHREAD_MUTEX_INIT_FUNCTION(&_M_mutex);
+#endif
+      }
+    };
+
+    static __mutex_type _M_device;
+
+    static void
+    __M_device_create()
+    { _M_device._M_create(); }
+
+  public:
+    explicit __scoped_gmutex_lock()
+    {
+      // Do not need to check __gthread_active_p() as assume already
+      // checked before a sentry is created.
+      __gthread_once(&_M_device._M_once, __M_device_create);
+
+      if (__gthread_mutex_lock(&_M_device._M_mutex) != 0)
+	__throw_concurrence_lock_error();
+    }
+
+    ~__scoped_gmutex_lock() throw()
+    {
+      if (__gthread_mutex_unlock(&_M_device._M_mutex) != 0)
+	__throw_concurrence_unlock_error();
+    }
+  };
+
+  __scoped_gmutex_lock::__mutex_type __attribute__((weak))
+    __scoped_gmutex_lock::_M_device;
+#endif
+
   class __mutex 
   {
   private:
@@ -148,24 +205,31 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION
 #else
     __gthread_mutex_t _M_mutex;
 #endif
+    bool	      _M_init;
 
     __mutex(const __mutex&);
     __mutex& operator=(const __mutex&);
 
   public:
-    __mutex() 
+    __mutex() : _M_init(false)
     { 
 #if __GTHREADS && ! defined __GTHREAD_MUTEX_INIT
       if (__gthread_active_p())
-	__GTHREAD_MUTEX_INIT_FUNCTION(&_M_mutex);
+        {
+          __GTHREAD_MUTEX_INIT_FUNCTION(&_M_mutex);
+          _M_init = true;
+        }
 #endif
     }
 
 #if __GTHREADS && ! defined __GTHREAD_MUTEX_INIT
     ~__mutex() 
     { 
-      if (__gthread_active_p())
-	__gthread_mutex_destroy(&_M_mutex); 
+      if (__builtin_expect(_M_init == true, true))
+        {
+          __gthread_mutex_destroy(&_M_mutex); 
+	  _M_init = false;
+        }
     }
 #endif 
 
@@ -174,12 +238,27 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION
 #if __GTHREADS
       if (__gthread_active_p())
 	{
+	  if (__builtin_expect(_M_init == false, false))
+	    {
+	      __scoped_gmutex_lock sentry;
+	      if (_M_init == false)
+		{
+#if defined __GTHREAD_MUTEX_INIT
+		  __gthread_mutex_t __tmp = __GTHREAD_MUTEX_INIT;
+		  _M_mutex = __tmp;
+#else
+		  __GTHREAD_MUTEX_INIT_FUNCTION(&_M_mutex);
+#endif
+		  _M_init = true;
+		}
+	    }
+
 	  if (__gthread_mutex_lock(&_M_mutex) != 0)
 	    __throw_concurrence_lock_error();
 	}
 #endif
     }
-    
+
     void unlock()
     {
 #if __GTHREADS
@@ -203,24 +282,31 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION
 #else
     __gthread_recursive_mutex_t _M_mutex;
 #endif
+    bool			_M_init;
 
     __recursive_mutex(const __recursive_mutex&);
     __recursive_mutex& operator=(const __recursive_mutex&);
 
   public:
-    __recursive_mutex() 
+    __recursive_mutex()  : _M_init(false)
     { 
 #if __GTHREADS && ! defined __GTHREAD_RECURSIVE_MUTEX_INIT
       if (__gthread_active_p())
-	__GTHREAD_RECURSIVE_MUTEX_INIT_FUNCTION(&_M_mutex);
+        {
+          __GTHREAD_RECURSIVE_MUTEX_INIT_FUNCTION(&_M_mutex);
+          _M_init = true;
+        }
 #endif
     }
 
 #if __GTHREADS && ! defined __GTHREAD_RECURSIVE_MUTEX_INIT
     ~__recursive_mutex()
     {
-      if (__gthread_active_p())
-	_S_destroy(&_M_mutex);
+      if (__builtin_expect(_M_init == true, true))
+        {
+          _S_destroy(&_M_mutex);
+	  _M_init = false;
+        }
     }
 #endif
 
@@ -229,6 +315,22 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION
 #if __GTHREADS
       if (__gthread_active_p())
 	{
+	  if (__builtin_expect(_M_init == false, false))
+	    {
+	      __scoped_gmutex_lock sentry;
+	      if (_M_init == false)
+		{
+#if defined __GTHREAD_RECURSIVE_MUTEX_INIT
+		  __gthread_recursive_mutex_t __tmp =
+		    __GTHREAD_RECURSIVE_MUTEX_INIT;
+		  _M_mutex = __tmp;
+#else
+		  __GTHREAD_RECURSIVE_MUTEX_INIT_FUNCTION(&_M_mutex);
+#endif
+		  _M_init = true;
+		}
+	    }
+
 	  if (__gthread_recursive_mutex_lock(&_M_mutex) != 0)
 	    __throw_concurrence_lock_error();
 	}
@@ -284,6 +386,14 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION
         void>::__type
       _S_destroy(_Rm* __mx)
       { __gthread_mutex_destroy(__mx); }
+
+    // default match.
+    template<typename _Rm>
+      static typename
+      __enable_if<std::__are_same<_Rm, __gthread_recursive_mutex_t>::__value,
+        void>::__type
+      _S_destroy(_Rm* __mx)
+      { __gthread_recursive_mutex_destroy(__mx); }
 #endif
   };
 
--- gcc/libstdc++-v3/libsupc++/eh_globals.cc	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libstdc++-v3/libsupc++/eh_globals.cc	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,6 +1,7 @@
 // -*- C++ -*- Manage the thread-local exception globals.
 // Copyright (C) 2001, 2002, 2003, 2004, 2005, 2006, 2009, 2011
 // Free Software Foundation, Inc.
+// Copyright (c) 2009  STMicroelectronics.
 //
 // This file is part of GCC.
 //
@@ -29,6 +30,7 @@
 #include "cxxabi.h"
 #include "unwind-cxx.h"
 #include "bits/gthr.h"
+#include <ext/concurrence.h>
 
 #if _GLIBCXX_HOSTED
 using std::free;
@@ -92,12 +94,10 @@ struct __eh_globals_init
 {
   __gthread_key_t  	_M_key;
   bool 			_M_init;
+  __gthread_once_t	_M_once;
 
-  __eh_globals_init() : _M_init(false)
-  { 
-    if (__gthread_active_p())
-      _M_init = __gthread_key_create(&_M_key, eh_globals_dtor) == 0; 
-  }
+  __eh_globals_init() : _M_init(false), _M_once(__GTHREAD_ONCE_INIT)
+  { }
 
   ~__eh_globals_init()
   {
@@ -105,14 +105,23 @@ struct __eh_globals_init
       __gthread_key_delete(_M_key);
     _M_init = false;
   }
+
+  inline void
+  _M_create()
+  { _M_init = __gthread_key_create(&_M_key, eh_globals_dtor) == 0; }
 };
 
 static __eh_globals_init init;
 
+static void
+init_create()
+{ init._M_create(); }
+
 extern "C" __cxa_eh_globals*
 __cxxabiv1::__cxa_get_globals_fast() _GLIBCXX_NOTHROW
 {
   __cxa_eh_globals* g;
+
   if (init._M_init)
     g = static_cast<__cxa_eh_globals*>(__gthread_getspecific(init._M_key));
   else
@@ -124,6 +133,11 @@ extern "C" __cxa_eh_globals*
 __cxxabiv1::__cxa_get_globals() _GLIBCXX_NOTHROW
 {
   __cxa_eh_globals* g;
+
+  if (__builtin_expect(init._M_init == false, false)
+      && __gthread_active_p())
+    __gthread_once(&init._M_once, init_create);
+
   if (init._M_init)
     {
       g = static_cast<__cxa_eh_globals*>(__gthread_getspecific(init._M_key));
--- gcc/libstdc++-v3/testsuite/lib/libstdc++.exp	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libstdc++-v3/testsuite/lib/libstdc++.exp	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -95,6 +95,8 @@ proc libstdc++_init { testfile } {
     global tool_timeout
     global DEFAULT_CXXFLAGS
     global STATIC_LIBCXXFLAGS
+    global GXX_UNDER_TEST
+    global GCC_UNDER_TEST
 
     # We set LC_ALL and LANG to C so that we get the same error
     # messages as expected.
@@ -211,11 +213,21 @@ proc libstdc++_init { testfile } {
     set tool_timeout 600
 
     # Default settings.
-    set cxx [transform "g++"]
+    if [info exists GXX_UNDER_TEST] {
+        set cxx $GXX_UNDER_TEST
+    } else {
+    	set cxx [transform "g++"]
+    }
     set cxxflags "-g -O2 -D_GLIBCXX_ASSERT -fmessage-length=0"
     set cxxpchflags ""
     set cxxldflags ""
-    set cc [transform "gcc"]
+
+    if [info exists GCC_UNDER_TEST] {
+        set cc $GCC_UNDER_TEST
+    } else {
+    	set cc [transform "gcc"]
+    }
+    
     # Locate testsuite_hooks.h and other testsuite headers.
     set includes "-I${srcdir}/util"
     # Adapt the defaults for special circumstances.
@@ -420,6 +432,8 @@ proc v3_target_compile { source dest type options
     global cxxldflags
     global includes
     global STATIC_LIBCXXFLAGS
+    global srcdir
+    global additional_linker_flag
 
     if { [target_info needs_status_wrapper] != "" && [info exists gluefile] } {
         lappend options "libs=${gluefile}"
@@ -435,6 +449,9 @@ proc v3_target_compile { source dest type options
 
     # Flag setting based on type argument.
     if { $type == "executable" } {
+        if { [info exists additional_linker_flag] } {
+           lappend cxx_final $additional_linker_flag
+	}
 	# Link the support objects into executables.
 	lappend options "additional_flags=./libtestc++.a $cxxldflags"
     } else {
@@ -462,6 +479,8 @@ proc v3_target_compile_as_c { source dest type opt
     global cc
     global cxxflags
     global STATIC_LIBCXXFLAGS
+    global srcdir
+    global additional_linker_flag
 
     if { [target_info needs_status_wrapper] != "" && [info exists gluefile] } {
         lappend options "libs=${gluefile}"
@@ -475,6 +494,10 @@ proc v3_target_compile_as_c { source dest type opt
     set cc_final [concat $cc_final $STATIC_LIBCXXFLAGS]
     set cc_final [concat $cc_final $cxxflags]
     set cc_final [concat $cc_final $includes]
+    if { [info exists additional_linker_flag] } {
+        lappend cc_final $additional_linker_flag
+    }
+    
     regsub -all {\s[-]nostdinc[+][+]} $cc_final "" cc_final
 
     # This is needed for "C" tests, as this type of test may need the
--- gcc/libstdc++-v3/testsuite/23_containers/vector/bool/modifiers/insert/31370.cc	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libstdc++-v3/testsuite/23_containers/vector/bool/modifiers/insert/31370.cc	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -17,8 +17,8 @@
 
 // 23.2.5 class vector<bool> [lib.vector.bool]
 
-// { dg-skip-if "" { powerpc64-*-freebsd* } { "*" } { "" } }
-// { dg-do run { xfail *-*-darwin8.[0-4].* } }
+{ dg-do run { xfail "*-*-darwin8.[0-4].* sh-superh-elf" } }
+{ dg-skip-if "" { powerpc64-*-freebsd* } { "*" } { "" } }
 
 #include <vector>
 #include <stdexcept>
--- gcc/include/ChangeLog.STM	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/include/ChangeLog.STM	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,3 @@
+2006-03-27  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* libiberty.h: Add support for cygpath.c.
--- gcc/include/libiberty.h	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/include/libiberty.h	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -678,9 +678,21 @@ extern unsigned long libiberty_len;
    (char *) memcpy (libiberty_nptr, libiberty_optr, libiberty_len))
 #endif
 
+#ifdef __MINGW32__
+/* Reassign the pointer PATH without freeing anything.  */
+extern char *cygpath (const char *path);
+#define CYGPATH(path) do {path = cygpath (path);} while(0)
+
+/* Reassign the pointer PATH and free the previous content.  */
+extern void cygpath_replace (char **path);
+#else
+/* If these were properly empty statements then there might be warnings
+   which would kill a -Werror build.  */
+#define CYGPATH(path) do {} while (0)
+#endif
+
 #ifdef __cplusplus
 }
 #endif
 
-
 #endif /* ! defined (LIBIBERTY_H) */
--- gcc/libiberty/configure.ac	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libiberty/configure.ac	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -185,6 +185,7 @@ case "${host}" in
   *-*-freebsd2.2.[[012]])	frag=mh-fbsd21 ;;
   i370-*-opened*)       frag=mh-openedition ;;
   i[[34567]]86-*-windows*)	frag=mh-windows ;;
+  *-*-mingw*)		frag=mh-mingw ;;
 esac
 
 if [[ -n "${frag}" ]]; then
--- gcc/libiberty/wrap_file.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libiberty/wrap_file.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,138 @@
+/*
+  THIS FILE HAS BEEN MODIFIED OR ADDED BY STMicroelectronics, Inc. 1999-2009
+*/
+/*
+ * wrap_fopen.c
+ *
+ * This file redefines the standard library functions 
+ * open, create, fopen, fdopen, freopen, remove, rename, unlink, stat for native WIN32 build.
+ * Its purpose is to preprocess argument strings in order to
+ * convert CYGWIN like paths specifiers into native WIN32 paths
+ * It uses the GNU ld -wrap functionality to replace
+ * at link time calls to fopen into calls to __wrap_fopen.
+ *
+ * This file must be linked with any DLL or EXE object
+ * and the linker command line must have the following  option:
+ * -Wl,-wrap,open,-wrap,creat,-wrap,fopen,-wrap,freopen,-wrap,remove,-wrap,rename,-wrap,unlink,-wrap,stat
+ *
+ */
+
+#ifdef __MINGW32__
+#include <stdio.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <dirent.h>
+#include <unistd.h>
+
+#include "libiberty.h"
+
+/*
+ * Declare real versions of functions.
+ */
+extern int __real_open (const char *pathname, int flags, mode_t mode);
+extern int __real_creat (const char *pathname, mode_t mode);
+extern FILE *__real_fopen (const char *path, const char *mode);
+extern FILE *__real_freopen (const char *path, const char *mode, FILE *stream);
+extern int __real_unlink (const char *pathname);
+extern int __real_remove (const char *pathname);
+extern int __real_stat (const char *file_name, struct stat *buf);
+extern int __real_chdir (const char *path);
+extern int __real_rmdir (const char *pathname); 
+extern DIR *__real_opendir (const char *name);
+extern int __real_access (const char *pathname, int mode);
+
+/*
+ * Following is the implementation of replacement functions.
+ */
+int 
+__wrap_open (const char *pathname, int flags, mode_t mode)
+{
+  int r;
+  char *path = cygpath (pathname);
+  r = __real_open (path, flags, mode);
+  return r;
+}
+
+int 
+__wrap_creat (const char *pathname, mode_t mode)
+{
+  int r;
+  char *path = cygpath (pathname);
+  r = __real_creat (path, mode);
+  return r;
+}
+
+FILE *
+__wrap_fopen (const char *pathname, const char *mode)
+{
+  FILE *f;
+  char *path = cygpath (pathname);
+  f = __real_fopen (path, mode);
+  return f;
+}
+
+FILE *__wrap_freopen (const char *pathname, const char *mode, FILE *stream)
+{
+  FILE *f;
+  char *path = cygpath (pathname);
+  f = __real_freopen (path, mode, stream);
+  return f;
+}
+
+int __wrap_unlink (const char *pathname) 
+{
+  int r;
+  char *path = cygpath (pathname);
+  r = __real_unlink (path);
+  return r;
+}
+
+int __wrap_remove (const char *pathname)
+{
+  int r;
+  char *path = cygpath (pathname);
+  r = __real_remove (path);
+  return r;
+}
+
+int __wrap_stat(const char *pathname, struct stat *buf)
+{
+  int r;
+  char *path = cygpath (pathname);
+  r = __real_stat (path, buf);
+  return r;
+}
+
+int __wrap_chdir(const char *pathname)
+{
+  int r;
+  char *path = cygpath (pathname);
+  r = __real_chdir (path);
+  return r;
+}
+
+int __wrap_rmdir(const char *pathname)
+{
+  int r;
+  char *path = cygpath (pathname);
+  r = __real_rmdir (path);
+  return r;
+}
+
+DIR *__wrap_opendir(const char *pathname)
+{
+  DIR *d;
+  char *path = cygpath (pathname);
+  d = __real_opendir (path);
+  return d;
+}
+
+int __wrap_access(const char *pathname, int mode)
+{
+  int r; 
+  char *path = cygpath (pathname);
+  r = __real_access (path, mode);
+  return r;
+}
+
+#endif /* __MINGW32__ */
--- gcc/libiberty/cygpath.c	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libiberty/cygpath.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,358 @@
+/* Basic Cygwin pathname support for MinGW.
+
+   Copyright (C) 2006 STMicroelectronics
+
+   This file is part of the libiberty library.
+
+   This program is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2 of the License, or
+   (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 51 Franklin Street - Fifth Floor,
+   Boston, MA 02110-1301, USA.
+
+
+   This file implements a limited amount of support for Cygwin paths.
+   It is intended for use by MinGW programs that must interact with Cygwin.
+
+   It is limited to absolute paths only.  I.e. Those beginning with Cygwin
+   mounts, such as /cygdrive/...  See the comment on cygpath() below.  */
+
+#include "libiberty.h"
+#include <string.h>
+#include <ctype.h>
+#include <windows.h>
+
+
+/* These are all the possible settings for the ST_CYGPATH_MODE
+   environment variable.  */
+static enum
+{
+  mode_unset,
+  mode_off,
+  mode_normal,
+  mode_full
+} mode = mode_unset;
+
+
+/* These are the values extracted from the registry.
+   They are extracted the first time cygpath is called.  */
+static const char *cygdrive = NULL;
+static struct mount
+{
+  /* The name of the Cygwin mount point.  E.g. "/usr/bin"  */
+  char *mount;
+
+  /* The actual Windows path that the mount translates to.  */
+  char *actual;
+
+  struct mount *next;
+} *mounts = NULL;
+
+
+/* Read a string from the Windows Registry.
+   KEY should be a valid handle from RegOpenKeyEx().
+   NAME should be the name of the value within the key.
+   The value should be of type REG_SZ.
+   If the value does not exist, is of the wrong typei, or another error
+   occurs, then NULL is returned.
+   Otherwise a malloced string is returned.  */
+static char *
+read_string_from_registry (HKEY key, const char *name)
+{
+  DWORD valuetype = REG_NONE;
+  DWORD valuesize = 0;
+  char *value = NULL;
+
+  if (RegQueryValueEx (key, name, NULL, &valuetype,
+		       NULL, &valuesize) == ERROR_SUCCESS
+      && valuetype == REG_SZ)
+    {
+      value = (char *)xmalloc (valuesize);
+      if (RegQueryValueEx (key, name, NULL, &valuetype, (unsigned char *)value,
+			   &valuesize) != ERROR_SUCCESS)
+	{
+	  free (value);
+	  value = NULL;
+	}
+    }
+
+  return value;
+}
+
+
+/* Fill in the mounts list (mounts is defined statically above).
+   All subkeys (not values) of KEY that contain a REG_SZ value named 'native'
+   are added to the start of the mounts list.  */
+static void
+read_mounts (HKEY key)
+{
+  int mountsize = 15;
+  char *mount = (char *)xmalloc (mountsize);
+  DWORD size = mountsize;
+  int index = 0;
+  int retval = 0;
+
+  /* For each subkey ...  */
+  while ((retval = RegEnumKeyEx (key, index, mount, &size, 0, NULL, 0, NULL))
+	 != ERROR_NO_MORE_ITEMS)
+    {
+      struct mount *newmount;
+      HKEY subkey;
+      char *actual;
+
+      switch (retval) {
+      case ERROR_MORE_DATA:
+	/* The buffer wasn't large enough for this key name.
+	   Unlike RegQueryValueEx, RegEnumKeyEx won't tell us how big it
+	   should be, so just make it bigger and try again.
+	   Note that this code path does NOT increment index.
+       	   Most of the time we will only be dealing with short strings.  */
+	mountsize += 10;
+	mount = (char *)xrealloc (mount, mountsize);
+	break;
+
+      case ERROR_SUCCESS:
+	/* Find the actual windows path.  */
+  	if (RegOpenKeyEx (key, mount, 0, KEY_READ, &subkey) != ERROR_SUCCESS)
+	  {
+	    index++;
+	    break;
+	  }
+	actual = read_string_from_registry (subkey, "native");	
+	RegCloseKey (subkey);
+	if (actual == NULL)
+	  {
+	    index++;
+	    break;
+	  }
+
+	/* Create the new entry in the mount table.  */
+	newmount = (struct mount *)xmalloc (sizeof (struct mount));
+	newmount->mount = xstrdup (mount);
+	newmount->actual = actual;
+	newmount->next = mounts;
+	mounts = newmount;
+	index++;
+	break;
+
+      default:
+	/* Don't infinite loop should any other return value occur.  */
+        index++;
+      }
+
+      /* The last call to RegEnumKeyEx may have clobbered size.
+         Fix it before the next call.  */
+      size = mountsize;
+    }
+
+  free (mount);
+}
+
+
+/* The top level registry reading function.
+   Open the keys, call the above functions to get the right values,
+   and clean up.  */
+static void
+read_registry (void)
+{
+  HKEY hcu_key, hlm_key;
+
+  /* Get key handles for the two places cygwin keeps its registry data.  */
+  if (RegOpenKeyEx (HKEY_CURRENT_USER,
+		    "Software\\Cygnus Solutions\\Cygwin\\mounts v2",
+		    0, KEY_READ, &hcu_key) != ERROR_SUCCESS)
+    hcu_key = NULL;
+
+  if (RegOpenKeyEx (HKEY_LOCAL_MACHINE,
+		    "SOFTWARE\\Cygnus Solutions\\Cygwin\\mounts v2",
+		    0, KEY_READ, &hlm_key) != ERROR_SUCCESS)
+    hlm_key = NULL;
+
+  /* Get the virtual mount point used for windows drives.  */
+  if (hcu_key)
+    cygdrive = read_string_from_registry (hcu_key, "cygdrive prefix");
+  if (hlm_key && cygdrive == NULL)
+    cygdrive = read_string_from_registry (hlm_key, "cygdrive prefix");
+
+  /* Read the other mount points.
+     Read hlm before hcu to ensure hcu settings get used by preference
+     by being closer on the mounts stack.  */
+  if (hlm_key)
+    read_mounts (hlm_key);
+  if (hcu_key)
+    read_mounts (hcu_key);
+
+  if (hlm_key)
+    RegCloseKey (hlm_key);
+  if (hcu_key)
+    RegCloseKey (hcu_key);
+}
+
+
+/* Given a path of unknown variety, return the same path with any
+   Cygwin mount points substituted.
+   This function always returns a malloced string which should be
+   freed when the the caller is finished with it.
+
+   The mapping is affected by the ST_CYGPATH_MODE environment variable.
+   See the fprintf messages below for full information.
+
+   It can replace /cygdrive/<letter>/..... style pathnames, even if the
+   user has used 'mount -c' to an alternative string.
+
+   It can replace (if enabled) other Cygwin mount points, such as
+   the usual '/', '/usr/bin', '/usr/lib', as well as any other user defined
+   mount points.
+
+   It does NOT attempt to convert any pathnames that look like native Windows
+   names - such as those starting with '<letter>:' or double slash (UNC).
+
+   It does NOT handle relative pathnames passing through cygwin mounts
+   (e.g. '../cygdrive/c'), or absolute paths with repeated directory
+   separators or relative elements within the mount name
+   (e.g. '/usr/./bin').
+   
+   It does NOT allow backslash \ directory separators within the actual mount
+   path (e.g. '/usr\bin').  Cygwin does not always allow them there either.  */
+char *
+cygpath (const char *path)
+{
+  char *result = NULL;
+
+  if (path == NULL)
+    return NULL;
+
+  /* If this is the first time this function has been called then read the
+     environment and registry.  */
+  if (mode == mode_unset)
+    {
+      char *env = getenv ("ST_CYGPATH_MODE");
+
+      if (env == NULL || strcmp (env, "normal") == 0)
+    	mode = mode_normal;
+      else if (strcmp (env, "full") == 0)
+	mode = mode_full;
+      else if (strcmp (env, "off") == 0)
+	mode = mode_off;
+
+      if (mode != mode_off)
+	read_registry();
+
+      if (mode == mode_unset)
+	{
+	  /* The variable was set, but not to any known value.
+	     Set up a default and print an informational message
+	     for the user.  */
+	  mode = mode_normal;
+	  fprintf (stderr, "ST_CYGPATH_MODE should be one of:\n");
+	  fprintf (stderr, " off    - Disable all path translation.\n");
+	  fprintf (stderr, " normal - Translate %s only.\n", cygdrive);
+	  fprintf (stderr, " full   - Translate all Cygwin mounts.\n");
+	}
+    }
+
+  /* First, test if this can only be a windows (non-cygwin) path.
+     This includes paths that start with a drive letter or UNC double slash.  */
+  if ((isalpha (path[0]) && path[1] == ':')
+      || ((path[0] == '\\' || path[0] == '/')
+	  && (path[1] == '\\' || path[1] == '/')))
+    result = xstrdup (path);
+
+  /* Second, handle /cygdrive/<letter>/ (or whatever) paths.  */
+  if (!result && cygdrive != NULL && (mode == mode_normal || mode == mode_full))
+    {
+      int length = strlen (cygdrive);
+      /* Note that cygwin does not allow '\\' instead of '/' in cygdrive.  */
+      if (strncmp (cygdrive, path, length) == 0
+	  && (path[length] == '/' || path[length] == '\\'
+	      || path[length] == '\0')
+	  && isalpha (path[length+1]))
+        {
+	  result = (char *)xmalloc (strlen (path) - length+1 + 1);
+	  result[0] = path[length+1];
+	  result[1] = ':';
+	  strcpy (result + 2, path + length + 2);
+	}
+    }
+
+  /* Third, handle other types of cygwin path.  */
+  if (!result && mounts != NULL && mode == mode_full)
+    {
+      int matched = 0;
+      struct mount *foundat = NULL;
+      struct mount *mount = mounts;
+      /* Find the longest matching mount point.
+	 This is important. If we just used the first matching mount point
+	 it would probably always match '/' when '/usr/bin' is right.
+	 Use the first of equal length matches - this allows current-user
+	 mounts to override 'local machine' mounts (can this happen?).
+         It is a match only if the matching part is followed by a directory
+         separator or the end of the path, except for the root mount point.  */
+      while (mount != NULL)
+	{
+	  int length = strlen (mount->mount);
+	  if (strncmp (mount->mount, path, length) == 0
+	      && matched < length
+	      && (length == 1 /* Special case for root mount point '/'.  */
+		  || path[length] == '/' || path[length] == '\\'
+		  || path[length] == '\0'))
+	    {
+	      matched = length;
+	      foundat = mount;
+	    }
+	  mount = mount->next;
+	}
+      if (matched)
+	{
+	  /* There was a match so do the substitution.
+	     If matched is 1 then it can only be the root mount point, in
+	     which case we do not want to remove the matched part as it is the 
+	     directory separator.  */
+	  if (matched == 1)
+	    matched = 0;
+	  result = (char *)xmalloc (strlen (foundat->actual) + strlen (path) + 1
+			    - matched);
+	  strcpy (result, foundat->actual);
+	  strcat (result, path + matched);
+	}
+    }
+
+  if (result)
+    {
+      /* Ensure that the return is never just a drive letter.
+	 This is not a valid directory on Windows, but code often
+	 trims trailing slashes.  */
+      int length = strlen(result);
+      if (result[length-1] == ':')
+	{
+	  result = (char *)xrealloc (result, length+2);
+	  result[length] = '/';
+	  result[length+1] = '\0';
+	}
+      return result;
+    }
+
+  /* If we get here then it must have been some other kind of path.  */
+  return xstrdup (path);
+}
+
+
+/* This is just to make inserting the conversion more convenient.
+   The CYGPATH_REPLACE is conditionally compiled so it is harder to
+   add clean up code to go with it without this.  */
+void
+cygpath_replace (char **path)
+{
+  char *result = cygpath (*path);
+  free (*path);
+  *path = result;
+}
--- gcc/libiberty/Makefile.in	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libiberty/Makefile.in	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -125,7 +125,7 @@ COMPILE.c = $(CC) -c @DEFS@ $(CFLAGS) $(CPPFLAGS)
 CFILES = alloca.c argv.c asprintf.c atexit.c				\
 	basename.c bcmp.c bcopy.c bsearch.c bzero.c			\
 	calloc.c choose-temp.c clock.c concat.c cp-demangle.c		\
-	 cp-demint.c cplus-dem.c crc32.c				\
+	 cp-demint.c cplus-dem.c crc32.c cygpath.c			\
 	dyn-string.c							\
 	fdmatch.c ffs.c fibheap.c filename_cmp.c floatformat.c		\
 	fnmatch.c fopen_unlocked.c					\
@@ -155,7 +155,7 @@ CFILES = alloca.c argv.c asprintf.c atexit.c				\
 	timeval-utils.c tmpnam.c					\
 	unlink-if-ordinary.c						\
 	vasprintf.c vfork.c vfprintf.c vprintf.c vsnprintf.c vsprintf.c	\
-	waitpid.c							\
+	waitpid.c wrap_file.c						\
 	xatexit.c xexit.c xmalloc.c xmemdup.c xstrdup.c xstrerror.c	\
 	 xstrndup.c
 
@@ -197,7 +197,7 @@ CONFIGURED_OFILES = ./asprintf.$(objext) ./atexit.
 	./basename.$(objext) ./bcmp.$(objext) ./bcopy.$(objext)		\
 	./bsearch.$(objext) ./bzero.$(objext)				\
 	./calloc.$(objext) ./clock.$(objext) ./copysign.$(objext)	\
-	./_doprnt.$(objext)						\
+	./cygpath.$(object) ./_doprnt.$(objext)				\
 	 ./ffs.$(objext)						\
 	./getcwd.$(objext) ./getpagesize.$(objext)			\
 	 ./gettimeofday.$(objext)					\
@@ -220,7 +220,7 @@ CONFIGURED_OFILES = ./asprintf.$(objext) ./atexit.
 	./tmpnam.$(objext)						\
 	./vasprintf.$(objext) ./vfork.$(objext) ./vfprintf.$(objext)	\
 	 ./vprintf.$(objext) ./vsnprintf.$(objext) ./vsprintf.$(objext)	\
-	./waitpid.$(objext)
+	./waitpid.$(objext) ./wrap_file.$(object)
 
 # These files are installed if the library has been configured to do so.
 INSTALLED_HEADERS =                                                     \
@@ -628,6 +628,13 @@ $(CONFIGURED_OFILES): stamp-picdir
 	else true; fi
 	$(COMPILE.c) $(srcdir)/dyn-string.c $(OUTPUT_OPTION)
 
+./cygpath.$(objext): $(srcdir)/cygpath.c $(INCDIR)/ansidecl.h \
+	 $(INCDIR)/libiberty.h
+	if [ x"$(PICFLAG)" != x ]; then \
+	  $(COMPILE.c) $(PICFLAG) $(srcdir)/cygpath.c -o pic/$@; \
+	else true; fi
+	$(COMPILE.c) $(srcdir)/cygpath.c $(OUTPUT_OPTION)
+
 ./fdmatch.$(objext): $(srcdir)/fdmatch.c config.h $(INCDIR)/ansidecl.h \
 	$(INCDIR)/libiberty.h
 	if [ x"$(PICFLAG)" != x ]; then \
@@ -1206,6 +1213,13 @@ $(CONFIGURED_OFILES): stamp-picdir
 	else true; fi
 	$(COMPILE.c) $(srcdir)/waitpid.c $(OUTPUT_OPTION)
 
+./wrap_file.$(objext): $(srcdir)/wrap_file.c config.h $(INCDIR)/ansidecl.h \
+	$(INCDIR)/libiberty.h
+	if [ x"$(PICFLAG)" != x ]; then \
+	  $(COMPILE.c) $(PICFLAG) $(srcdir)/wrap_file.c -o pic/$@; \
+	else true; fi
+	$(COMPILE.c) $(srcdir)/wrap_file.c $(OUTPUT_OPTION)
+
 ./xatexit.$(objext): $(srcdir)/xatexit.c config.h $(INCDIR)/ansidecl.h \
 	$(INCDIR)/libiberty.h
 	if [ x"$(PICFLAG)" != x ]; then \
--- gcc/libiberty/configure	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libiberty/configure	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -4842,6 +4842,7 @@ case "${host}" in
   *-*-freebsd2.2.[012])	frag=mh-fbsd21 ;;
   i370-*-opened*)       frag=mh-openedition ;;
   i[34567]86-*-windows*)	frag=mh-windows ;;
+  *-*-mingw*)		frag=mh-mingw ;;
 esac
 
 if [ -n "${frag}" ]; then
--- gcc/libiberty/ChangeLog.STM	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libiberty/ChangeLog.STM	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,25 @@
+2010-04-12  Christian Bruel  <christian.bruel@st.com>
+
+	* wrap_file.c: Fix prototypes.
+	Remove rename wrapper.
+	* cygpath.c: Shut-up warnings.
+
+2009-12-07  Yvan Roux  <yvan.roux@st.com>
+
+	* wrap_file.c: New file.
+	* Makefile.in: Add wrap_file.[co].
+	* config/mh-mingw: Add wrap_file.o.
+
+2006-05-15  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* cygpath.c (cygpath): Convert pathnames consisting only of a
+	drive specifier to a valid directory (e.g 'c:' -> 'c:/').
+
+2006-03-27  Andrew Stubbs  <andrew.stubbs@st.com>
+
+libiberty/
+	* cygpath.c: New file.
+	* config/mh-mingw: New file.
+	* configure.ac: Add mh-mingw makefile fragment when host is MinGW.
+	* configure: Regenerate.
+	* Makefile.in: Add cygpath.[co] .
--- gcc/lto-plugin/ChangeLog.STM	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/lto-plugin/ChangeLog.STM	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,3 @@
+2012-10-11  Laurent Alfonsi  <laurent.alfonsi@st.com>
+
+	* lto-plugin.c (claim_file_handler): Detect no symbol case.
--- gcc/lto-plugin/lto-plugin.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/lto-plugin/lto-plugin.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -910,6 +910,9 @@ claim_file_handler (const struct ld_plugin_input_f
 
   status = add_symbols (file->handle, lto_file.symtab.nsyms,
 			lto_file.symtab.syms);
+  if (status == LDPS_NO_SYMS)
+    goto cleanup;
+    
   check (status == LDPS_OK, LDPL_FATAL, "could not add symbols");
 
   *claimed = 1;
--- gcc/config/mt-ospace	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/config/mt-ospace	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -1,3 +1,3 @@
 # Build libraries optimizing for space, not speed.
- CFLAGS_FOR_TARGET = -g -Os
- CXXFLAGS_FOR_TARGET = -g -Os
+CFLAGS_FOR_TARGET += -g -Os 
+CXXFLAGS_FOR_TARGET += -g -Os 
--- gcc/config/mh-mingw	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/config/mh-mingw	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -6,3 +6,9 @@ CFLAGS += -D__USE_MINGW_ACCESS
 # as GCC turns out to need that much more to pass all the limits-* tests.
 LDFLAGS += -Wl,--stack,12582912
 BOOT_LDFLAGS += -Wl,--stack,12582912
+
+# Activation of CYGPATH feature: Support for cygwin pathes in mingwin32 shell
+#   through syscall wrapping at linker level
+LDFLAGS += -Wl,--wrap,open,--wrap,creat,--wrap,fopen,--wrap,freopen,--wrap,remove,--wrap,unlink,--wrap,stat,--wrap,chdir,--wrap,rmdir,--wrap,opendir,--wrap,access
+
+
--- gcc/libcpp/ChangeLog.STM	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/libcpp/ChangeLog.STM	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,12 @@
+2009-06-10  Antony King  <antony.king@st.com>
+
+	* mkdeps.c (deps_write): Use ISALPHA instead of isalpha.
+	(deps_phony_targets): Likewise.
+
+2007-08-14  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* mkdeps.c (deps_write): Convert paths to Cygwin format on MinGW,
+	if GCC_CYGWIN_DEPS environment variable is set.
+	(deps_phony_targets): Likewise.
+
+
--- gcc/libcpp/mkdeps.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/libcpp/mkdeps.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -321,6 +321,17 @@ deps_write (const struct deps *d, FILE *fp, unsign
 	      column++;
 	    }
 	}
+#ifdef __MINGW32__
+      if (getenv ("GCC_CYGWIN_DEPS") != NULL
+	  && ISALPHA (d->targetv[i][0])
+	  && d->targetv[i][1] == ':')
+	{
+	  fputs ("/cygdrive/", fp);
+	  fputc (d->targetv[i][0], fp);
+	  fputs (d->targetv[i]+2, fp);
+	}
+      else
+#endif
       fputs (d->targetv[i], fp);
     }
 
@@ -341,6 +352,17 @@ deps_write (const struct deps *d, FILE *fp, unsign
 	  putc (' ', fp);
 	  column++;
 	}
+#ifdef __MINGW32__
+      if (getenv ("GCC_CYGWIN_DEPS") != NULL
+	  && ISALPHA (d->depv[i][0])
+	  && d->depv[i][1] == ':')
+	{
+	  fputs ("/cygdrive/", fp);
+	  fputc (d->depv[i][0], fp);
+	  fputs (d->depv[i]+2, fp);
+	}
+      else
+#endif
       fputs (d->depv[i], fp);
     }
   putc ('\n', fp);
@@ -354,6 +376,17 @@ deps_phony_targets (const struct deps *d, FILE *fp
   for (i = 1; i < d->ndeps; i++)
     {
       putc ('\n', fp);
+#ifdef __MINGW32__
+      if (getenv ("GCC_CYGWIN_DEPS") != NULL
+	  && ISALPHA (d->depv[i][0])
+	  && d->depv[i][1] == ':')
+	{
+	  fputs ("/cygdrive/", fp);
+	  fputc (d->depv[i][0], fp);
+	  fputs (d->depv[i]+2, fp);
+	}
+      else
+#endif
       fputs (d->depv[i], fp);
       putc (':', fp);
       putc ('\n', fp);
--- gcc/fixincludes/ChangeLog.STM	(.../vendor/tags/4.7.2)	(revision 0)
+++ gcc/fixincludes/ChangeLog.STM	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -0,0 +1,20 @@
+2008-28-08  Antony King  <antony.king@st.com>
+
+	* check.tpl: Avoid premature termination of script on fgrep -v
+	failure.
+
+2008-27-08  Christian Bruel  <christian.bruel@st.com>
+
+	* fixincl.c (test_test). Dont quote test.
+
+2007-10-02  Antony King  <antony.king@st.com>
+
+	* fixincl.c (cygpath_open): New function.
+	(load_file): Replace open() with cygpath_open().
+	(create_file): Likewise.
+	(process): Likewise.
+	(initialize): Add calls to CYGPATH() and CYGPATH_FREE().
+	(test_test): Add missing quotes.
+	(fix_with_system): Likewise.
+	(fix_with_system): Force use of Unix shell.
+	(main): Redirect stdin to nul: on Windows.
--- gcc/fixincludes/check.tpl	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/fixincludes/check.tpl	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -140,9 +140,9 @@ echo $exitok`
 
 cd $TESTBASE
 
-find * -type f -print | \
+( find * -type f -print | \
 fgrep -v 'CVS/' | \
-fgrep -v '.svn/' > ${TESTDIR}/LIST
+fgrep -v '.svn/' > ${TESTDIR}/LIST ; true )
 
 exitok=`
 exec < ${TESTDIR}/LIST
--- gcc/fixincludes/fixincl.c	(.../vendor/tags/4.7.2)	(revision 3649)
+++ gcc/fixincludes/fixincl.c	(.../branches/4.7_devs/gcc)	(revision 3649)
@@ -101,6 +101,20 @@ void process (void);
 
 #include "fixincl.x"
 
+static int
+cygpath_open (const char *file, int oflag, mode_t mode)
+{
+  int fd;
+
+#ifdef _O_BINARY
+  oflag |= _O_BINARY;
+#endif
+
+  fd = open (file, oflag, mode);
+
+  return fd;
+}
+
 /* * * * * * * * * * * * * * * * * * *
  *
  *  MAIN ROUTINE
@@ -122,7 +136,11 @@ main (int argc, char** argv)
       and err open so that the proper input file does not get closed
       by accident  */
 
+#if defined(__MSDOS__) || defined(_WIN32)
+  freopen ("nul:", "r", stdin);
+#else
   freopen ("/dev/null", "r", stdin);
+#endif
 
   if (file_name_buf == (char *) NULL)
     {
@@ -211,6 +229,8 @@ do_version (void)
 void
 initialize ( int argc, char** argv )
 {
+  char *arg;
+
   xmalloc_set_program_name (argv[0]);
 
   switch (argc)
@@ -221,7 +241,8 @@ initialize ( int argc, char** argv )
     case 2:
       if (strcmp (argv[1], "-v") == 0)
         do_version ();
-      if (freopen (argv[1], "r", stdin) == (FILE*)NULL)
+      arg = argv[1];
+      if (freopen (arg, "r", stdin) == (FILE*)NULL)
         {
           fprintf (stderr, "Error %d (%s) reopening %s as stdin\n",
                    errno, xstrerror (errno), argv[1] );
@@ -325,7 +346,7 @@ load_file ( const char* fname )
       the file size is not a multiple of the page size.  If it is a multiple,
       then this adjustment sometimes fails anyway.  */
   data_map_size = stbf.st_size+1;
-  data_map_fd   = open (fname, O_RDONLY);
+  data_map_fd   = cygpath_open (fname, O_RDONLY, 0);
   ttl_data_size += data_map_size-1;
 
   if (data_map_fd < 0)
@@ -476,7 +497,7 @@ create_file (void)
 
   sprintf (fname, "%s/%s", pz_dest_dir, pz_curr_file + find_base_len);
 
-  fd = open (fname, O_WRONLY | O_CREAT | O_TRUNC, S_IRALL);
+  fd = cygpath_open (fname, O_WRONLY | O_CREAT | O_TRUNC, S_IRALL);
 
   /*  We may need to create the directories needed... */
   if ((fd < 0) && (errno == ENOENT))
@@ -501,7 +522,7 @@ create_file (void)
         }
 
       /*  Now, lets try the open again... */
-      fd = open (fname, O_WRONLY | O_CREAT | O_TRUNC, S_IRALL);
+      fd = cygpath_open (fname, O_WRONLY | O_CREAT | O_TRUNC, S_IRALL);
     }
   if (fd < 0)
     {
@@ -539,7 +560,7 @@ static int
 test_test (tTestDesc* p_test, char* pz_test_file)
 {
   tSCC cmd_fmt[] =
-"file=%s\n\
+"file='%s'\n\
 if ( test %s ) > /dev/null 2>&1\n\
 then echo TRUE\n\
 else echo FALSE\n\
@@ -852,7 +873,7 @@ fix_with_system (tFixDesc* p_fixd,
 #else
       /* Don't use positional formatting arguments because some lame-o
          implementations cannot cope  :-(.  */
-      tSCC   z_cmd_fmt[] = " %s > %sX ; rm -f %s; mv -f %sX %s";
+      tSCC   z_cmd_fmt[] = " '%s' > '%sX'; rm -f '%s'; mv -f '%sX' '%s'";
 #endif
       tCC**  ppArgs = p_fixd->patch_args;
 
@@ -935,7 +956,34 @@ fix_with_system (tFixDesc* p_fixd,
                pz_temp_file, pz_temp_file, pz_temp_file);
 #endif
     }
+#if 1
+  {
+    char *cmd;
+    char *fname = make_temp_file( 0 );
+    FILE *pf = fopen( fname, "w" );
+    if (pf == NULL)
+      {
+	fprintf (stderr, "Error %d (%s) creating %s\n",
+		 errno, xstrerror (errno), fname);
+	exit (EXIT_FAILURE);
+      }
+    fwrite( pz_cmd, 1, strlen( pz_cmd ), pf );
+    fclose( pf );
+    asprintf( &cmd, "sh %s", fname );
+    if (cmd == NULL)
+      {
+	fprintf (stderr, "Error %d (%s)\n",
+		 errno, xstrerror (errno));
+	exit (EXIT_FAILURE);
+      }
+    system( cmd );
+    free( (void*)cmd );
+    unlink( fname );
+    free( (void*)fname );
+  }
+#else
   system( pz_cmd );
+#endif
   free( (void*)pz_cmd );
 }
 
@@ -1283,7 +1331,7 @@ process (void)
 
       if (read_fd == -1)
         {
-          read_fd = open (pz_curr_file, O_RDONLY);
+          read_fd = cygpath_open (pz_curr_file, O_RDONLY, 0);
           if (read_fd < 0)
             {
               fprintf (stderr, "Error %d (%s) opening %s\n", errno,
@@ -1337,7 +1385,7 @@ process (void)
       pz_file_source = pz_temp_file;
     }
 
-  read_fd = open (pz_temp_file, O_RDONLY);
+  read_fd = cygpath_open (pz_temp_file, O_RDONLY, 0);
   if (read_fd < 0)
     {
       if (errno != ENOENT)
